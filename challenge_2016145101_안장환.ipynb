{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "challenge_2016145101_안장환.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y-PWfzpReXrg",
        "w81APM0rpasp",
        "vGTaJQqjpe9Z",
        "wqd3l-ixN7qs",
        "Cp72Zn3C2p_n"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "vDXsKayAN_LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejfEjuVvN-gs",
        "outputId": "0bc4b311-4888-4cde-f082-aa47a0fe7f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 13 17:21:06 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P0    38W / 250W |  14533MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UYhKaIZpgLm",
        "outputId": "15794973-eaee-4608-dd7a-97d6e6809a08"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AkmTaZIVR5s",
        "outputId": "e9107867-fea7-476f-89bf-3e69a572e615"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.7/dist-packages (2.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import SimpleITK as sitk\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import scipy.io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms, datasets\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "zEC6ylxpprOU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data check"
      ],
      "metadata": {
        "id": "Y-PWfzpReXrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## view_input_dcm_data"
      ],
      "metadata": {
        "id": "w81APM0rpasp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/train/img/002.dcm'\n",
        "images = sitk.ReadImage(filename)\n",
        "\n",
        "print(\"Width: \",images.GetWidth())\n",
        "print(\"Heigh: \",images.GetHeight())\n",
        "print(\"Depth: \",images.GetDepth())\n",
        "print(\"Dimension: \",images.GetDimension())\n",
        "print(\"Pixel ID:\",images.GetPixelIDValue())\n",
        "print(\"Pixel ID Type: \",images.GetPixelIDTypeAsString())\n",
        "\n",
        "images_array=sitk.GetArrayFromImage(images).astype('float32')\n",
        "img=np.squeeze(images_array)\n",
        "copy_img=img.copy()\n",
        "min=np.min(copy_img)\n",
        "max=np.max(copy_img)\n",
        "\n",
        "\n",
        "#0~1(실수)로 normalize하고 0~255(정수)로 바꿔줌\n",
        "copy_img1=copy_img-np.min(copy_img)\n",
        "copy_img=copy_img1/np.max(copy_img1)\n",
        "# copy_img*=2**8-1\n",
        "# copy_img=copy_img.astype(np.uint8)\n",
        "\n",
        "copy_img=np.expand_dims(copy_img,axis=-1)\n",
        "copy_img=cv2.cvtColor(copy_img,cv2.COLOR_GRAY2RGB)\n",
        "print(\"check\")\n",
        "\n",
        "# plt.imshow(copy_img, cmap=plt.cm.bone)\n",
        "#cv2_imshow(copy_img)\n",
        "#cv2.waitKey(0)\n",
        "#cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "plt.imshow(copy_img.squeeze(), cmap='gray')\n",
        "\n",
        "print(copy_img)\n",
        "print(copy_img.shape)\n",
        "#datatype = np.ndarray\n",
        "print(\"==================\")\n",
        "print(\"images_array\")\n",
        "print(type(images_array[0][0][0]))\n",
        "print(images_array.shape)\n",
        "print(images_array)\n",
        "print(np.amin(copy_img))\n",
        "print(np.amax(copy_img))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOLtpeMSpeF9",
        "outputId": "c5d8ae1c-90b4-4ab5-f32c-8e8165c82ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Width:  2588\n",
            "Heigh:  3232\n",
            "Depth:  1\n",
            "Dimension:  3\n",
            "Pixel ID: 3\n",
            "Pixel ID Type:  16-bit unsigned integer\n",
            "check\n",
            "[[[0.06028708 0.06028708 0.06028708]\n",
            "  [0.06124402 0.06124402 0.06124402]\n",
            "  [0.05845295 0.05845295 0.05845295]\n",
            "  ...\n",
            "  [0.11331739 0.11331739 0.11331739]\n",
            "  [0.12025518 0.12025518 0.12025518]\n",
            "  [0.12185008 0.12185008 0.12185008]]\n",
            "\n",
            " [[0.06371611 0.06371611 0.06371611]\n",
            "  [0.06204147 0.06204147 0.06204147]\n",
            "  [0.05813397 0.05813397 0.05813397]\n",
            "  ...\n",
            "  [0.11307815 0.11307815 0.11307815]\n",
            "  [0.12001595 0.12001595 0.12001595]\n",
            "  [0.12623605 0.12623605 0.12623605]]\n",
            "\n",
            " [[0.06371611 0.06371611 0.06371611]\n",
            "  [0.06052632 0.06052632 0.06052632]\n",
            "  [0.0585327  0.0585327  0.0585327 ]\n",
            "  ...\n",
            "  [0.11347687 0.11347687 0.11347687]\n",
            "  [0.11738437 0.11738437 0.11738437]\n",
            "  [0.12519936 0.12519936 0.12519936]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.14824562 0.14824562 0.14824562]\n",
            "  [0.14338118 0.14338118 0.14338118]\n",
            "  [0.13883573 0.13883573 0.13883573]\n",
            "  ...\n",
            "  [0.13269538 0.13269538 0.13269538]\n",
            "  [0.1299043  0.1299043  0.1299043 ]\n",
            "  [0.13030303 0.13030303 0.13030303]]\n",
            "\n",
            " [[0.13716109 0.13716109 0.13716109]\n",
            "  [0.13700159 0.13700159 0.13700159]\n",
            "  [0.13030303 0.13030303 0.13030303]\n",
            "  ...\n",
            "  [0.13437001 0.13437001 0.13437001]\n",
            "  [0.13413078 0.13413078 0.13413078]\n",
            "  [0.12998405 0.12998405 0.12998405]]\n",
            "\n",
            " [[0.12767145 0.12767145 0.12767145]\n",
            "  [0.13046253 0.13046253 0.13046253]\n",
            "  [0.12663476 0.12663476 0.12663476]\n",
            "  ...\n",
            "  [0.13524722 0.13524722 0.13524722]\n",
            "  [0.13787879 0.13787879 0.13787879]\n",
            "  [0.13556619 0.13556619 0.13556619]]]\n",
            "(3232, 2588, 3)\n",
            "==================\n",
            "images_array\n",
            "<class 'numpy.float32'>\n",
            "(1, 3232, 2588)\n",
            "[[[3067. 3079. 3044. ... 3732. 3819. 3839.]\n",
            "  [3110. 3089. 3040. ... 3729. 3816. 3894.]\n",
            "  [3110. 3070. 3045. ... 3734. 3783. 3881.]\n",
            "  ...\n",
            "  [4170. 4109. 4052. ... 3975. 3940. 3945.]\n",
            "  [4031. 4029. 3945. ... 3996. 3993. 3941.]\n",
            "  [3912. 3947. 3899. ... 4007. 4040. 4011.]]]\n",
            "0.0\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANwAAAD8CAYAAAAc9sq3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO29a6xt13Ue9s39fp4HTfqClCnJdSgUSu0oCiGrTlCnMSwzQgAmsOvKBmLWMaz8kNwE6A/TbQEHcQI7RWKjgV2jCkRECmKrQh4wbahVadW1URh2JMu2IlJUdEWLECk+dHnvPXe/n7M/9v7m/tY4c+29zjmX565zzhrAxt577fWYa+35zTHGN8Yc03nvUUghhZyPlO51Awop5CpJAbhCCjlHKQBXSCHnKAXgCinkHKUAXCGFnKMUgCukkHOUcwecc+4x59yXnXPXnXNPnvf1CynkXoo7zzicc64M4D8B+H4ALwH4LIAf8d4/d26NKKSQeyjnreHeA+C69/4F7/0UwCcAPH7ObSikkHsmlXO+3lsAfF2+vwTgu3UH59wHAXxw/fUvlcvlxAlKpRKolZ1z4bP3Ht77sC2muZ1zZ2p82vHOufDaJbZdekzseN6L3rP9DYg/F/09dnypVDp2rH3XY5bL5bH7OK2FdNb/4m7LtvvY9r/rZ353zmE2m93w3j9gjzlvwO0U7/1HAHwEACqVit/f3wcAlMtllEql0EnYqRaLBUqlEubzORaLBZxzmE6nWC6X8N4n9i+Xy3DOYblchu3rax57eN770MH0WGDTQZfLJRqNBmq1Wmgft7MtlUolbPfeYz6fH+vMpVIJlcrqr6jX6xiPx1gul1gul5hMJlgsFuF+qtUq6vU6qtUqZrMZ+v1+OM57j3K5jGq1iul0iuFwiPl8juVyifl8nrheuVxGrVZDrVZDvV7HfD4P+3J/trnZbGI8Hoff1x0qAWoL6F2Ayjvg9H7sgMrPpVIp8Tz5qlareOmll16MXee8AfcygIfl+7ett6UKQUNNx5vmn0wgEBTz+TwBLAVJuVwOYCPg+D2mObgfH6rut1wuUavV0Gw2Ua1Ww7HssASQDgqVSgWVSiUxWOg+BLj3HpPJJIBX/3zeU6VSQa1Ww2w2AwBUq1VUKhUMBoMA+MVikWoNsGPwueoAo+2q1WphMOO12aZt2vcqiB2ogd3P4LwB91kAjzjnvh0roH0AwI/uOoiA4h+tn2u1WtjGUVxNJUqpVEpoLDta2RGOAKPmsiZjtVpFrVYLHZ3tUVADSZPQgr3ZbIaODwCDwQDT6RTOuaDBdGBgO6ldWq0WWq0W5vM5ms0mhsMhJpMJ6vV60IyxkZvn5yCgmleBz4GLFgOFgLzqkuYCbJNzBZz3fu6c+zCATwMoA3jKe/9s2v7UWgogAmu5XCZGZ+7Ph8Df2MHVHFKtx9+oxWJaU9oPYAU2C0AFbblcTviTuq8FQalUwmw2g3MuYTpSsyjYVWN77zGdTsM5l8sl+v1+AORisUClUgkaULU2BwneP7Upz0MNXalUwnPjPbL9VrtdVolpsSzHpMm5+3De+08B+FTW/bcREuyE/MwOoh2evo3tIKr9CCA9jzU7Lahpjlkfkb4bwWvfrUmm5wMQjvfeB82p51Rgs831ej3RxtlsFgYkgkQHIgKO51KTkQNOpVI5Zpaq2bnrP7sKchow5o40iUlMg1k/iOYaOxL3ARB8qEajkTBL+a7gq1QqqFarCXDYttDUVN9NRa9vgWZ9KSV2eE7eCwkUYKPdS6VSgpABgFqthslkEgBGk1MHI7USarVaON9isTimxRTo+qxiJqq95yxyGQBpwaYuwzbJPeDS2CGaPlYsk0ltx98ABFMN2IBZSQSrlYCNKcgOy/Pxd+2w1ozkdWLMKDXRcrkMDCRZSoJfGVfuT/9PtV+MKQUQzEMOPNTo6g+yPaoV1SJQDcr263GXTezAyG36+2kk94DTTsoRhB2NnU39LH4mIFXzARvmjQ+sUqmEY0jrqkbgNehnaXvUn+I1YyYwOzD3VfOTIFGA1Gq1AA5qo+l0GszMer0ejolpUw4yysBaCpuAIuCUheVz08EmFoPTZ5D2/aJIjDizEhtcTurD5hpwShYAx2/Ydn76I9yX+6g5Rzrdxk6oZSj0a8rlcgJsqpWsFuD1YsygalTVVNQ8ah7bMAXbzTaqFuVAAqx8USVa9Bnye6PRCMSTZU55LxqDS7MkrA961WXbIKSSa8ABG2paO914PE6Yl0DSobeUPrDRZDSpLNOoHUtHeGUwG41G0AgU1WoEjwKCwm2qhXi8mi4EnY0lWgKHfiY1Ic9J5lI1Gs9dqVTQaDQwGo0S2l/BacEWG1RiJtZVkRiwYu5CmuQacNqR2aHUjCKAlGVjB1KtpaM3j1U/kPuoJlXfitvm83lg+Ei9Wz+OD5/A40t9Sb2u3qdqr1gnZ+dXX02FprNqYjU9m81myCJRP5eihJIlSLRdbM9pAHcRQZqmvWI+rd1uJdeAo7DzatoRvyudDWy0m2WLlBBYLpchPqVMJrB5WIyDcdR3zoVgcqPRQKfTwXw+x2w2CyanZSPt9ZWxpDZScOv1VbNZskcZR6Xy6etpIJ4DQKPRQLVaRa/XS4QX2E7VbGQtY6axHQjuBpFwkUV9v9ggaCXXgNM/1/od2sHYqayJqCDju3Yaso00wwAEDcER3lL5i8UCo9EIk8kEtVotMICz2Sz4j1YTa3t4L8q2aihDR0m2zwa81VQmUJXer1arIZjtnAskEQcOgl2JIUoa9X9ScuAySBqRkkYU2cEoJrkGnIqaXTria0aJPggSEhZowKpTtVqtQDioWUlS4ejo6Fhmivpgi8UC0+k0XF8pfEvja/stna5/qmomAlKBw7ZSo9JMHAwGoT28vjVlNc/Uno/XtGBL8914P1dV9PnpAMntF9qktAmzGmdS5pB+h42nATi2PzUHMzLm8zkajUbIwh+NRok2WNApIzqfz4OG0mvP5/OgfQEEranmI31PAIEEYQ6lplXxM58HBwden2YtX5oLCawyUXissq6WKNrWUWId67KBbhdY0iQGwDTJPeDsDVhzTU0zZmeoicZzKP3PzkrQ8LzT6RSj0Qij0SihZQhYTYcCklpVY1Xs2O12G977MLVFwUoTkOcheBqNRjiHms7qf3q/yqMkicNBQ30vtptmr/p2luq3TGlMs1nz3koW8F02gKbJhdVw9o+3fgs7ls7t4uwBe9NkO3mcpi0x87/X62E4HIbEX06/4flms1nwfdRUVRDSzCTRU6vV0O12MZlMgulH1pUAABBYz+FwiGazGcxbXmcymWA2mwVtRQBPp9PgP6r5CiCEAZRs0cx/DiRp7NpV9NtOKnYQ2eXH5RpwqmFUq9DEYmdvtVoJDUbzkiQBj+N51LQjEJVqVy00Go2CucjJphqG0LYCmyx+alVq0Wq1ina7jclkEg0s81yLxSKkdjFmSH+TMUBeX302JXf4jJi4zecYS0beBbYsRMBVlpg5uU2T5xpwFHYiG6QFNhMvgWS+I4Gk5if9JctklkqrKTL03ZR6Z2dmZ6U2JOs3mUxCGzXuRmBQI7bbbdTrdTQaDYzH40Q4QzNMlG3U9tMv9N5jNBqFeXPW9CPLymwaTe62dL9qutg7P8fMzKseDjitXAjAadaIjtYU7UCLxSJBp/N364PQFyRA7chvz01tNJlMAvVeq9VC9sloNErE8vRay+USvV4vAI0+p5ImPI6A1QA2z69sowUpsEkE4LHK6Grow95j2va7qdkuIyhVu2WV3APOajRlydhp+V07nLJ1wMb/Uy3Ec1QqFQyHw2gAXWuBsMNTO81ms6Bh77vvvgAMmnpsM5lBNUU1MK2moGbHaOCb7VBTUs+v6V22HIV9FnyO+jxjWq2QbHKSZ5V7wFmKm6aa+mPj8TgQJ/xd41kav2PHJJGg51WtRDOTZqQVBc54PE74W3t7ewCQGAwmk0nIAbWZKbyenV2uMUZe04ZFeKzux+fGe6Dm1+dp73nb9xgBFft8lUQHyphpnya5BxyQZNM0DKBxOM2NpLag8HeNbamPpmCgUBvZFDFNTuZ+PG46nQafTpOueQ86d039SQbp9Y8iyG0aF9vAY3WKD89hp95o7C72bPXdbr9qYsGjyQrbBpesg1DuAWcDzvycVtxHtQa/s7PpJExlBgkSEiGxoK4+eA2oq4ahJlGNCiCxv74INp2Px/1pRhJ4atLyPmIZJTxvLPnYiprm/K7vdt+08+ySy6YFtS9mDXhTcg84vTkFmE4cteXgACQ6PP0uBpbpA/F8PJ4moMbYbFa9dh7t5GyrbbN2Zs5X4zUBBIaVoOFvehwD+mpeK2liAU3NpgOHakjdN02zZelAlw1Iu+Q0JImV3AOOoiO8zf7QjkMWD0hOs1ksFoFhpK9HDaH+mGpCW0eS7VBtx2Niv6vfRE3G1K1qtZr4nfPzNFamwCTgqI0VaNRSMbCltTs2SKQRKIVsJAY6a1Ftk9wDTjuN7Ujqs7Hj67uCgcdoCTmdMKqBZjX1FEwKAvUjgeN1L/nZbldyRDVauVwOoQXur1WbOVCQHdW28RgFqpJFMac+TcOpWFBeNY2WRU46SOUecJbJ022k5GkOqv+l/pbWb1QtorOmaR5agPLhKbBt27RD8rMyokByvplzLpRHZxhgsVgE7aSpYSr030j+EFAaRNf224RkC7TYb/y8S7KC76qANOt9XgjAAcfZI3ZMjWcpecF9NKVJtRhNT05/4XZNGiaJotdVVtRW/7Lmm/qWduYCM0WoxZjlYgsiMZCvgw4HCpuwrH5mGhmyTavt0nhXRWxfu5uSa8BZIFn2z2ouTU5WBo9SLpdRr9cT9jbLifPcaoZqHM0CRk1Rgl9BavMsrflHsHIGuRJC3vswQ1uPU2YS2IQuFGyq5fjdPk/7jO1+um8BwOMS8+PUetkmZwKcc+5rAHoAFgDm3vtHnXP3Afg/ALwdwNcA/LD3/pZbteR/BfB+AEMA/533/vM7zh+9MXY6goWdQtOkePxkMgkdVwv3KHNpg982LUr9NX2w3FdZTTVPSerY9mtHJinD0nh6z+qbEaQayFbtq5pUBwJrHlsAWdN5238R+1zIydLh7oaG+6+99zfk+5MAPuO9/wW3WlL4SQA/DeCvA3hk/fpuAL8KszbcNrE3pYnMtlKWzjVjh1dzkyxhr9cLmoZJxpYIAZIhATsIEPCaXK3ajbMNVMvxOryncrkcBobxeBzawnAA74mLffC6MUKJ96gzCJTttMfsIlCyjNppcpmBue25nDdp8jiAv7r+/DEA/y9WgHscwMf9qjV/4Jw7cM496L1/ZdvJ1FTSzwSVmm9qginBoJWGaUayQ+q6bDQLNfdQ/TItT2A1i/pR3vuwRhsZRa1tSXaUvpmtOKYsKVnVyWSSmEFufVvL3lIU3LvIEitnAdtVEh2Qd8lZAecB/N/OOQ/gf/erxRSvCYheBXBt/Tm2+ulbACQA52QFVE2Psn4LTSxrgsVG+ul0GkZ4Hq/1Pegf1et11Ot1dDqd0HHVfNNgs/URtTNrTIx+GiemcgY2J5lyfhywKYXAQYJhAh4fYxLtn201Hu8v+udt8fPSfJKrCEBrdseeQVZ/96yA+yve+5edc98K4Bnn3POmEX4NxsziZQXUarXq1dSzforNl4z5P2oixmYPaMecz+e4c+cOBoNBCDlwhVAWFwIQ5rnpuW2QW7NW9B7G43EAI7UY42t6DyzJRy0eeU6J+475ZrpfTLPpe9r59VwnkYsOzCxMpR2Q3nTAee9fXr+/7pz79wDeA+A1morOuQcBvL7e/VSrn2oysPpJ6hMBSQAB6aMQgGPZKeqD8fycTsOkZoKPIYVutxumAZHSV9BQo1nKnpqZ2xqNBhqNRhggarUahsMhxuNxYqENO3iw3TpgqLbjvrFYnP1sO0qadiskLid5VtvX1tkizrm2c67LzwDeB+CLAJ4G8MR6tycA/Mb689MAfsyt5L0Ajnb5bwoKBYP+vr5+WOBCt+n+2vHszAMLNl5PNQ6B1e/3MRqNQmGgRqOBb/3Wb8X+/v6xjH87Y4DnpRZkLI9+6Hg8DrVLmIambVFzNXZf+lyshos9VzvobCNP+EwLSZcsWvEsGu4agH+//hMqAH7Ne/9/Oec+C+CTzrmfAPAigB9e7/8prEIC17EKC/x4lotYP0nNSDuyaLaIjvZW62kn0wpfdsqPag7dj4Do9/twblUb8uDgANeuXcNoNEKv1wuEzHA4PGb+6bV4Ps5do2mr+2sbVNvp/ek9W18yJrHt20zTQuJiSbRdz+vUgPPevwDgL0S2vwHg+yLbPYAPneI6x7SaBZR2QGZhULQGpPpwWkDIuc0KpNwv1qHV3FRN0Ov10O/3sb+/j3a7jU6ng+l0GrQgcDzVS01LzZfUQHpM66jvymOoLZkQbYscpWmtmGa8G3IVQZrVl8t1pgnpcSCZU2kJEnZOHqPvlhIHNv4ep/gQfAxi09QjQ6gZ/GreqUZZLpeBcHFC9Wt1Z5sfSfKE6V1sm01ZsywZ70+TtDUovs1n22U2Fv7b6WXbc6XkGnBvfetb8fM///N4/vnn8clPfhKvvvpqosNop1CWUEERKw5EbaMZIuz4zrlE0LzZbAYtSuDF1qBjzA9AoPCV/dQ260BBjcpwgV2wI2Ya2vu3NUxsGMACzQLXmkX6W+xzIXFJY4xVcg24o6Mj/PEf/zF+8Ad/EN/zPd+Dn/zJn8RgMIiSAtQKFO6jaVbA8STiWq2WCCgDyeRn+lrl8mqZX62IRXBriQRquvF4HAoTUQhMO6uBmo9sJUMCeqzeq92mmpPmqJ3xHZMY43lWs/IyATMGnl2+2oXWcLdv38Yv//Iv4/d///fx0Y9+FN/7vd+L3/zN3wSQXN9NF8EAjms5imoOYKOJdPa1AknzJ6nZtGOz0jNBomsVcF/VgqwVSaCzg2vJBO99WP1GQZn2R6f9wfQL7bHb/LbCnMwuseeUZbDKNeCA1U0899xzuHnzJt72trclzEPr1wEbsOl+1Cy6LWam2aWFFYz6gHmsxsp4XU2SJolBEPF4xvRoxvKcmnFCILP9SuJom9O+W7PRbos957tFmlxG2WYqbjPFreQecADwnd/5nTg8PMTXv77KDFOQxRKMgWTcjvvxwWgirz4cBautzqVmqWo+go8pXNxO34/A4rH0Gyk8P305BSXbaGunWLGdQe83jUDRbbFzF/7bbkmzBi4s4B544AF86EMfwuOPP45XXnkFv/u7v5sADrC5aS1Bx05qY3i6v4KP22yMD0hmbVjTjOdXU5Tgoy8FbKoj8/t4PA5VmBW4uq/3HvV6HcPhMGpW2vvh9l309LZR+qzm5FUDpj73XQMbJdeA63Q6+KEf+iG8+uqr+Omf/mncvHnzWHwqLUME2F1j0Jqlek7uz3Pac/AYDSPQ9BsMBonFNDjNxnt/jKDR5bd4HV0/nHE/JXLUtOWAoKK+LL8rYNM+F3JyifWnbZJrwH3961/H7du38aUvfQnPP/98gv62Jp6unmM7oHZmazLq+azpCSRHK16H++qabQQgSzZw3h2wCcZzxvlgMAjnJMPJfbnuG8Gm1b3YVhWrfWPPRwEXA5/N07xqmuosctJnlWvAzWYzvPDCC3j3u9+dqOloM0bY8dP8o5ippVpNQwf04yhWo1lt5P0mFUvr/6vvqCulct6d0va8ppYyp8nJHFEFur0Pe78cUNKAZrWbHaBUCvCly2lM8FMnL5+X/Omf/ikefvhhPPTQQwDiGe5pMwWsprKjPk1ApeQVeEqK8KWVkpndoWL9R55L/btGo4Fms4lmsxnAp7mUOhjYGd76W4zYsGbNrg5hn1khZ5cL68M55/D5z38e9Xod3/Vd34UXX3zxWCcj5c+OGTOvKGkjuzXLrA/E68ReMe1hFxtR81VzHzWGqCldto02fqgmrA4IbKvNAz3pMz+NFIDNJrnXcF/96lcxHA7x6KOPRoFkWUngOLHAl+ZFqk+nJqfuqyXoFIQEAksoaL1LBQ8rLWs2iuZN8jqlUilMTLWzyy2DGQtz8HnoKyYFKO6exCytLJJrDQcAb7zxBp599lk8/PDDx0wpmngxp19NL4r+zg6rGkHBpQFsZRfV5+O5Ca7JZJIotW7rqtTr9QAm/rZcLo/NfdNQhQIuDWxW9Fi9b/2uGjmNLCkAmi722WQFYO4Bt1wu8VM/9VNoNpsAklnyNjhsk375riajAoodW+l17XyWHeRvNiFas/U5S5ymY6fTCWuDDwYDjEYjVCoVdLvdcBwJIZ2yY//ANIJkm8RMSutXFrJdTmOWb5PcA845h9deey0AzWovZQS1jgmw8YMUOEpK8DvFTgXi9YHkrGv1nazW1GtNp1MMBgMsFgu0221UKhW02+0A2OFwCAChArSudaAr5PB6qpVjYrV5DFBKEN0tuSqacBf4srCWuQccxdLy2gFZZ9ICScFC8d4nKjTzXf0qC8oYUcPj9Fhr8jrnEszj3t5eMIHn83lYEYdVxWwKmIKcA4cWjgUQihnFcj75PcbE6v3Ye4x9L2QjpwkHUHIPOO34jMNp/X1+14wP68upT0OA0EdjaTqdHWCZQe3wJEkUlApsOw0I2Kyy2u/30W63E+ylMo5ss25XMOvqrVo5Oha+0PAG7yHmAxdydjkJAHMPODXRdFoLOylTpmKjdpr5yCA0sNIQjUYjoQUsWUFAM4vfuVVGSa/XS6yeGiMnCBr6eOPxOHEPSprwuja1jM+B2lzbZ0MdKpaxtGRKIacTO3BZ2fZ8cw84dkBLu+s2u7Y1cJwmt/VDGLCeTCahHLnNMOG+1ncsl8totVq4dm1V45aFg7giThpDyHQvJXvIYqo2slpZBx2bKwps4oXbTEZ9noWcTWJgSyO3rOQacJZVrNfroS6kdjb9TNqex/N3zVeMAcISLTGfRh/qnTt34NyqYhc1pBYM0gHAhhi896Fo0Ww2Q7PZTPhgaWbfNu2kg0nMjEy7l0Lujlwak1JHbKXltR6JZQpVaLbp0sG6j2oMvYad8Mk2KMC99xgMBuj3+4nzaaVk5zYJyBws1GTk741GI4QTrC+mcTclj+x9Wl/SDizWj7sboLvq5MpJCZTcAw5Igk47EjsZZ1Dr7xYwaUSHah5912OtmaZEh52xrUAgI0qTt9FoAEiu5MPzdjodlEqrVXw0yG79MJ43BpjY4FT4bfmS3APOAiRmHsVqP1qmUueXWfPQajmrVXS7TgNijZLFYoHxeIzRaBTaoiwjzWGWTdCCQ1xXoNFohHbb2d/6rmSLamyrefX5FCblmyPWWsoiOwHnnHsKwN8A8Lr3/r9YbzvxoovOuScA/M/r0/4j7/3HsjZSO5KCg52GwWLG4xiTs5WvFFRcKccuGRXrwApiDRWweJCNndnaJPTxqBG5PwcBzhSgNiTzqtW9LMj0nk5q1sSeRyGnk5M++yzw/JcAHjPbuOjiIwA+s/4OJBdd/CBWiy4SoD+L1QKM7wHws865wywNtKZRzLyysSrvNxW16Luplmi1WmE978lkkqjpT/qeMS5WUKYGGw6HGA6H6Pf7GA6HIVhNLUbQcOEP51zQhASRFmy1mSlqhsZms9vnEvuc9irk7kvsfzhTWMB7/3vOubebzSdadHG97zPe+5vrhj2DFYh//SQ3E2MOgY2G03loyg7aOWs60ZT7WkpeX/a66sPp9arVKtrtdkIbMUlZ16Vj2zgYEEhafsGWzotpsjR/UyUry1nI+chpfbiTLrqYtn2nWD/GEh86+hNANOs0hKCdjSUMLPMHJMGU1nnVj+J3EiLNZjPE+dRcJfj0GmRbCUYuzqjLacV8Ttvms7CPpzVHCzmdnJk08f7kiy5uEycroFpQEUzWt1INRaaP2s3mUVriwa5LYPMilfmzTKV+ZxkF1ZpcI67RaCSC3TbhmqYrNdy2CaQnBZTex2mOL2S72AFrl/l+2gmor61NRbhsiy5mXozRe/8R7/2j3vtH08Cm2siO+Oz4dhIncDxLIwZo7m8D6vq7TgXSeCCwMm+Hw2GC2lcNyJVVuaKqpnZp6MPOgVOJ3YNuj0mhxd4cOelzPS3gTrro4qcBvM85d7gmS9633rZTdjn//K5Jy1wVxzl3jDShEDSxsEAseK6i/qHdbjNWVLtwXly9Xg8DCAcFOzjYUdNqdgu0tOdTyJsrai1ksRyyhAV+HSvS437n3EtYsY2/gBMsuui9v+mc+zkAn13v9w9JoGQRNfVUlKK3HZ1itZOeM/auYmNetoMDmwrJmttps2C4zjfTufibxg9JrNiEZNs+m6USI1VU46rW1msX8ubIrmebhaX8kZSfTrToovf+KQBP7bqeFe1w1nzUzsROq6LrvqmPp+RK7Hp6zRgjaeuhMM6mmoe+m3PJOXEkSVgPhZpZfc6Y38hjCWpu471ZIuc8gHXVtWna/Z8pLHAvxYJKR/m0gDa1HROd7UhvK2DpOWwgnSBSMGn5cgqJGudciP8xDkfiRFPACBqdDhS7Z20L28d20M+z/myhvc5P0hjebQNRrgFHYUdSvynmVzFovFwuQ7Uszh/jPhTbyfUcCk7vfSK0QNAp4EnQzGazULOkVquh3W6jVCqh2WyGuiVsA1O6OJ9O26fnjhFF6juqKW0XYrRmprUSCnCeXdJckTS5EICjxEBib7jVaiW0kZpfwHFSgh3PPiRr2lHUX7TFeLQCMzNUWDOz0+kksks4g0EBp5o77U/T6/GzsrBZnfgCbHdHThr7zD3grFlpf6Ows04mEzSbzTB7QDujMoFqNlq/zV7XsoN6/Zh5y3PqOuGc92b9Ry17rhrWJiPH/lDra9pnEiNf0uQ0AfAiaJ6UGMNs5UIATjuOmkgaZGbSLzs3kASZLo6o/o8lT9JAZr9bTWRHOj2Gwe3JZBJ8O2BTMmI2m0VLplsT2IJONZ1KWrC8MCPvvsQsrQtrUtqb0JdWMAYQYluc5qJsJJOBrU8DJAGlU2p4DrZD97f1KnV/C0AlbjhnTv1QTvVhmECvaetf6rPQZ6LbdRAq5O5I1ueZRdvnGnClUilQ7go2O6+NxISlzYFkBoqyjfxNQaILdVi2MKa91BRU8GsIQolPPzEAACAASURBVAkWneajKWW8T4YJdKHG2EqsfI91hJh2sxk5hdx9uRQ+HBlC1ugHEIqnqraiOTmbzQINz5gWA8o6t0ynv1hGkkIw8sXrU1gQiMwjJ6HaMAZBrmvX2WJH3vvgczLEEEuitqCJFR1KEztg6PnSnn0BzpPLLuDlGnAAErEqUu1ayoAAYuk5rqtGgmI+n6PZbIale8vlciibroFxfWmHpL+nc+X0oS4WizArgMeoqUftyxkANveS2hdYhQomk0kig0bp/hijGqP6t/lsu/zCQk4naay5ldwDjp0UQCjEww45m80SGSaccKozvVWTEVAsaaCarlqtYjQahQwQpdzZKdnxrfnICapc/ZQajYsvakoXNR7jcBoIV7PXkkNKGGmbTmIqpu1TaLO7KxeWNAE2nUH9KwCJYPRyuQyZHgQhj1XaXTM+xuNx4jv3B5KsHzu4DXRb4kLLOrAKF+uXtNttLJdLDIfDYxqU90AWdTKZBJNSQUaxjKRKGmiymJCF3D250GEBYENSsANqehVvjhke3JekBLWLzbPUOJklZayfpbmTBAA1K2l+ajUCn+fsdrvw3mMymSRqlXBg4LWsNqWZaf+8mFmZ5tvZY+znQu6uZHmuuQecmoHUIhS1mxUg1Wo1AEFZxBiFrh1dOyp9Lr0OQcVcSa6/TaFmGw6HYdaAFiaq1WqYTqch/Yz+GWNw0+kUk8kkDBha2k9FNbtu05BFAazzEesz75LcAw7YBIjtwvLsdErlK9jYadMW+tDULPpxSmIAG5CRCdWqYN77Y0sdE2xsp12jm4syAgjFihif07lxBBCQzJbhwKBhC0vyFNrs/CWrD5xrwNmb0OwOq+lUg2kAW4v22PQn9b2obXgsActSdxpPIzDoOzKYzUwXDgIkXzghVsFL05MJ1qqhdVVXivp7bL9d/TXm3xWgOx+5FBrOBrltChZDApZUWCwWoa6JbiMo+ZlgI5OoHZ7Xp1a16WIEiuY8snAR65MAm7CGMpME3Wg0ClOJuEYchT5nLB0tjUyJaTZrRsd8v0JOJ3bQv/CAI8j0RvR7rCIXR3qSEvSblOZXcqHVagVKnlqQANFr8NpAEgT0H3ntxWKB0WgUvpNQoamqGlUzUXhtvRcNlqsGBnBMmytRQ9kFqLuh/a56SCEr0Ci5BhyAY4ACVn+yrmKqN6zkB0FH01FLGDQaDTQajYTfpmBWzakaRk1Tq+Vs3KxSqYTpQvT3NFeS56B21YA+ZxnYOYBp/hrPpVrMhgNiz9Jqw6sMntOKNfe3DWKnLSJ0bmKzLixDpwyRZvLTt2IwW2tCsnKWkg6aCaJgZsfnNRR0CgpWciZACCCtW8J9+Icw8E5ihxkpel+6bnksTqjPxpqV1joo5O5JDFhZkhByreEUXOxAqiHUx2OnIzjYUbmABrDq9PV6HZ1OJ8EwMj5mzUktCERzkZ1a28R9eQ0F4ng8DtqOYCYLCSCQLaPRCN57NJvNUHZdzUcbE7RJzSox/80+18J/O5tYtly3b5NcA45iU50sWaD+C8FAE5GAqdVqODg4CEFnbrMmnBby0bCDFgKyk0N5PudcKKdATUqfrlKpoNPpBG2qgCUjSpOyUqlgPB4n1iBQv9OaLnYSrIrtFCcBW2Finlx2Pa9cm5QEkI7wlmkDjldBVpKBkz6bzSb29vZCkrOdpqMaVEuTc3/up+cGNtS8zhqoVquo1+sJjUltZ2NsBArN3Gq1ilarhU6ng1arFRYC0XtXLcztbF9M1OzWbYXcfbnwGk4JCiBe8IcdSv0q/l6tVtHtdtHtdjEej7G3t5fI+qdm4TkVEHpOYLPYhvqQSmoQnGQXZ7NZQsPq+ampNMVLtS21JY9nYjTbSU1pa1nGJM2ZL0zLuyMnGcxyDziKTfoFkonFfBFs1DLtdjtUPGawuVqthrlr9M1YYUtLHSjlr4Fp1XIEk53jRpnNZmi1WgCQAB3vgyEJjeXpfRD8GuTmcfQJCVo9pgBSPiX3gCPjRy2gGSXWWeV+XDZqb28P1WoVs9ksTJ+ZTqdhPW3t3Hoe+nG6vJRNcOb17SBAckTBwtAD/ToSKZZw0TZo8Fy1qfqPaq4qEcO2xUBXmJL3Vnb6cM65p5xzrzvnvijb/oFz7mXn3J+sX++X337GOXfdOfdl59wPyPbH1tuuO+eetNfZJhpbYyeKaRxS63t7e9jf34dzLnRcmnckOEhs8Fhu5wwAUvqWIVQQqW9FLdRsNtFutwP7yMUfgU0gm8BhorLNiNGBwGpDeZ5he4xMsn6vbo99PosU2jQp255HFg33LwH8MoCPm+2/5L3/p7rBOfdOAB8A8OcBPATgt51z71j//CsAvh+rteE+65x72nv/3K6LaxDbXCvBzDWbzRDHYtKyagOu10YtV6vVQqIxAEwmE+zv7wfTTYkUXsfG6FTzVKtVNJvNUH+S1bhUMzPATa2kS1wBm+peuqoqBwp+tlo2jcG1A1Qhb75ksR5OuwJqmjwO4BPe+wmAP3POXcdqiWEAuO69f2HdsE+s990JOMsI2s5GBpJJxuyY4/H4WLKyzhpg5r9W0VJChNn7nPemidA8H6/Pa3PG+WQyCVN1gM3cNpqIHBCULOF1rT8KbExO+pRWs/M3JUd0QLKaLsb0FnJ22cUWA2cLC3zYOfeFtcnJ9brPvAKqc+6DzrnPOec+F6u/r0HndruNw8PDQIyUSqVQ8VgXOdQiQiQZ5vP5McqdGoS/a2oWQWCXwtI8SZqJNBUPDg4Sc+ZYaEjPzxeQnMKj2SxAfJa59z7BUuq79Tn1eyF3R9KAte0ZnxZwvwrgOwC8C8ArAP7ZKc9zTLwsyKhkAEduJgAfHh6GDs1alKwtQuAoocCKXsCq49+6dQuj0SgBRs4yABBmEHAZYGovmnA6wZUaUQFH0FsmldfT4LyCDtgkPeucPoLPkjS0AFQzxkImthNY07SQk0sWjWblVCyl9/41fnbO/QsAv7X+um2l00wroFphp6FPQz+JNUMYo9I6INrJefx8Pke/3w+dmzOvW61WqG9izVWl7HVmwHK5mY1AU5OVuzjNxjmH8XgcyBnVRApy1bA6JYft1iwUZWuVBdV2p+VZ8vMusTGlQnbLSZ7ZqQDnnHvQr1Y2BYC/BYAM5tMAfs0594tYkSaPAPgPAByAR5xz344V0D4A4Ed3XUdHb8bUtOQcgAS9r/6edkbVEIzPMX3q4OAgfNZOzI7NNCxqPwKPk1L1WjwHsMlA4UKMnGTK+wKQ0HoKVHnOIbVMgUNCxvqo5j8K7zr4WM1XxOzOV067Aupfdc69C4AH8DUAfxcAvPfPOuc+iRUZMgfwIe/9Yn2eD2O1zHAZwFPe+2ezNJA1JtWX0jW8VSNpZr5OOLWdVrNLnHNBy5ER5DHj8TgxM5ugpw+nmSF812k+Gmqg78hrUnMBm3ABAUvgqcazaWLMMqFY8FlyJ2Y+KtjSRumso3ehGVey6xm4PI9ulUrFX7t2LWSBMEbGzA5mW+ifrSDQWBuwiZexbN1sNgtJxf1+P2SfUCMRqMwUqdfrABCApAwjM/4Zc6NGpKk5n89DLiWARNkGgmg2m2EymST8NS4CohpS43ZqQvK4mObTIL4lUawvYjtNViBdZsBZs1zDRhqf5eB58+bNP/LeP2rPk+tMk1Jps7aAkgrUOgACra8Eiyb28jM7tK5iQ3NxOp2i1Wolch8ZsOY+fKeW0o7abDYTPp0mJFO72NxNXUuOmSgEJwcCXtcytPys2/mnqynN33j/qtmBol7l3RKr3bcNPLkGnAZyeVMEG00uvTl2Oi1RRw2oWRv0CVlTZD6fh9nfzNjntdjBlQXkdchkqsmp2oUdnPtzLhyJE40vAkhMKVosFmF/anjeG5+Hkiv6oqiPpqAsAHbvJPeA06ko1C40K9Xf4n6a1aGzBrRcOrUJNc14PA6LOOrEUYKbHZcaa7FYhLhfrVYLbKRqFQW8aj4FAdvC0gsEmPqjbDOJEg4c2i5ecxcbSdDFzKMChOcjuZ4PByChEdRGVnAReKwdaX2jWHoWTTmGF2hOLperkuTs8OonUYvpdBxqRTXpOFBoapf6fECSRbVEEEMWrLFJMJGhVd+RogyrFav9dvlapwVfAdrdkmsNR01hS5grgwcgaAWdnqIdmppDtZt2Wvpvmm7FUAOvrxNN2+12WDNgOp0eK3NHcDUaDZTL5UCE8Hy8t1iwmm3mgKBalD4t09bSyA0lVWLMZBrgzspUXnXJ8oxyr+Fo7qh2A5AIC3AfraBFzcaOTZNNtV+tVkOj0cDh4WHIVrHC8uNkPTmLvF6vY29vL2g81Wwa1Lbgpd+n7JYOIgoSHSTYbmpVG2fTd54XSM4St5qxyDY5m1iAZck8ybWGA44XfLUmkw3ichtwnDYHNmSDdlKuIUeGUDslzVWmjTHU0Ov18NBDDwX2U8GiJpwGtHXGAP0yNZcJLJ6Dx3Ffrh3Hz0qg2GfAEIH6awqutNhcIdnlNJo/94ADVgCaTCYhw4RBcGoUy8SpuUnGD9h0Ml1aSs04AsASMIyHLZdLjEajsLjjYDBAuVxGt9tFtVpFv98P1bfIWGrSNP061kqhaaiZJ7ZWCjNVgA1oms1m4ljVgJQYUcNnmaWjnNaMvIrm50nuN/eAU7ZO6zayI9n5YBr70pkGOnOcnV9NVF6LnV+3q3bQLJabN29iuVyi2+2GWBy1IH039SHVV6PmUyZV/Txd747ZNmyjJmxr2IJtAzYhkjQ/zwJDNeFVA8xZRS2MXZJ7wPEmCDQt/U1SxdLdduRXPxBAotw4tRq1qMareC62g+dlitdoNEK/38fR0RG+7du+LfiE3vtAlHCBD+3Imqpl71V9TyVxGKxnNgtnMvB8PN6a2KrlgGQ2SSy0sO1/KIC4XfSZpknuAce5YdRKWnCHfp1NWLYB5Zi/ogBWTaE+npYeV2m1WsGknM1mGAwGobgsJ7aqthoMBolMGD2vmq4MdvM3mxvK9g8Gg5D/qZXEKAquWI6lSlo87yzgusrg3KXlcg04aihd6klfSgAwxUvNSL5rhgk1me6jx+mqqKrl2BYA2N/fDyahMqEEKwPZzWYzTDTVOXDqc2k7+eL1uQ8ZUmpA5ly2Wq3EwEANrv6pjriq8bIA4ioD5zRy4U1KJUOsuaRmJZlFdkx2SpqcChgFEVOvmGNpk4O5H9sCAN1uN2xnG2ii8p0pYyy/ACCQKcBGi1Jzqdkbo/i5vwbSx+NxKP9HcoVtTyNK7LO1WjGLaVlIumQZnHINOAWbVrLSSZlahkGztilqXjKIrawfO6cGpdW004Ku5XIZe3t7YRa4soYEHAPUi8UCg8EgzBgg8ChK+rC9qrHV1CXoWOKP985FSujT2Tou1o+jqNmdFkuKhRoKjXdcTvp8cg04YFMYlaYXzUeO0KppaHryN/Xd7DQKNbuAZGVnTTpuNpvhQXa73dDxWbcEQGANY2lcw+EwxM8484HHTSaTY9OLVLtoErbeL/1P5pYqCTQajRKDjh6rksWXO4tcJnBuexYWbLvuO9eA006mHYjahTmMFCVJ1Jwi6cKEZwUXHxK30/+i1qzVajg8PMRgMECj0Qh5lnywBBvjatTG+icRpGyz96v16ehPsi0UamDVsGpK673akAhrt6hPyGeiLK69pra30GjZRJ9NbFCLSa5Tu5T6J3B0pVJ2MgChApeNmVmwqS+nGo/7MUGZ+/H6+/v7iRnbfNVqNXQ6HcxmsxAz47GaSsb7ARASlbl0lpqbvB+alfyspp76m/zOP1ufg95jjIWMdZhYp7GDRyErOc1zybWGAzadzY7QOs+NHUWTnJUooY+j51MzVFk8nkPXjAMQ6k1S29KM29vbCySNFvphx9ZULrbdLuzR6XQwHo9DFTHV6BpoV7Bohg1nc2vJBn0uGodTsO3ScLp/IdtlG0GlkmvAKSB0xLcdSUMF6p/xGDU9ld5Xc5UdmHmKnH9Wq9XQarXQ7/cT13ZuNcuAAXA7R02n0fD6bI+GK8g8Mt7ImJ3eu/6JOolViw6R2GEFaLsoie0IMQ1nNfG2/yULCC8zWE97X7kHnPUnqIFUk1higMDjfDIAiYwSvrT8HLVCuVwOeZDtdjuR0aGgrVQq2N/fD+3Q2J1zLmgcdmQtYgQk/SglhlqtFhaLBYbDYcJvtSlcwGZakpI06ldqFbEYexkDoWp7Pnf7XxRy+ueRe8Axq0JHfSCp5VQLqi9DYsIylKT9lS5nR1a/Tfen8Dzdbhf1ej1U+1LyRWNrJGLUH6W/R+0LbMIXrLUCIATXNdDP+OJyuTy2VoGCkUH3WDm9NGBZX0/3LcB2XOwz2WVOAjkHHIDQYUjP88/XDq1AIUHC2d9aDo9iO5YyeeoPlkqlUMlLU7VYjJaxMG0Tz6k+FEFHsCndr/fE6xE0XNOO57RLEGu7bNk8pphp+7ZpNn0WWcIDVx2Eu8ilNMk14NgJtUNpkJig0AwS1oBUkoTHqaajD6VpXTxWCRddLYdpWgcHB1gulxgMBsdqjGhAW+/DEhnKplIzkXhRn1T315kFbJNeF0AANu9N08pUw8W0mbbXakC93knksgLzUpqU7KjsbPTHCESNn2ldEa2WTEBaml0z8VXD8FgCQLctl6upOM65kNWhMTnN5rCkhM7Lo3AwAJIzHMg6smyeakFLovA8+p2z0pnmRtDp9a3Gp8TOr9t3bbsqErvvLM8i14ADNhpLa0c654L/wtofjHlZUfZSM/GpbagJgU0dSec2M7+Xy2Xwm5rNJhqNRsg00RLryj5q6hjBYBf2AJJZIEr9E7xaf1MHDy3hoFqT5ifPR+ZTGU8NJ8Q0nAX1VQXUaSSLSZllBdSHnXO/45x7zjn3rHPu76233+ece8Y595X1++F6u3PO/XO3Wun0C865d8u5nljv/xXn3BNZb4L0OVOkFBj8TlDZVC/us75+0IRaT1IrOmssS83KSqWCZrMZEoephXSWAIXbFazAZjliAlJNO4LMJk7r8arRebwSRQSgzhTQeJ7NwIk5/du+sw2FxMWa6jHJkmkyB/A/eO/fCeC9AD7kViudPgngM977RwB8Zv0dAP46Vot4PALgg1gtbQXn3H1YrUvw3Vgt0vizbrOu3M4bofnFoj3e+7AYIxN6gc0EVfpeSoIAyYRlDR0QXEp00JRjnIxz28gy2kJG1Cyc+U2ChPsrRa/t0aC4DXFoYjaFhIiGBHjvCihq/9jz1He77STmUhYAXjaQ7hqYtt3vTsB571/x3n9+/bkH4EtYLab4OICPrXf7GIC/uf78OICP+5X8AYAD59yDAH4AwDPe+5ve+1sAngHwWIbrH+sYSjjU6/VE5Sx9ALxxDSUAm7QpaoRyebV+gE6TsSyjhgB0TpsuymHbS22j26ktdRDQ7cy1VFZUtRVBxlLsGui3vheP1+enWo7XTgPTWUB2FeWumJQqzrm3A/iLAP4QwDW/WbLqVQDX1p/PtAqqMyug6siu7B87OzWfzmWzaWDqw9j9LNHCl2ZxaP0Q1X5q0hGAXApLCxmxvQAShI7OViCTquygZsnotXkunSXAZ6MA0jbGBq5tQNul8Qo5LjHLwUpm0sQ51wHwbwH8fe/9HT2p99475+7KsOe9/wiAjwBAvV737Hg0/whCLRZEpk8ByJGdLCLNLvV/tENaE3HdlkA8cJaA5ktabcvzayiD4CJ4eD0NPRB8alra6l16Td635o7q9VU7MndT8yn1GKsVed+FbJfTEkqZNJxzrooV2P619/7frTe/tjYVsX5/fb09bRXUbaujpl03sf41b5LA0vUDgM38MPWPFBjaqVUb8Nzqj5FAYfBcr8POrOfQILZfB6n5mxZ+1eWNCXLWJuFKrrossXMuaE0CifuwmpdOLbIsrBItMc0V03pphEphYq4k7X7vFkvpAHwUwJe8978oPz0NgEzjEwB+Q7b/mFvJewEcrU3PTwN4n3PucE2WvG+9bacoKUGmT0vPERjObVa8sZM2NePETlglxa/xunK5HCaMaqFW+nMxf5CaVMFGDTwej9Hr9TAYDNDr9UJhWZ1Mq9qG7dSFIglUXoezC0iMKDsKbCaw6qBiQwBZTMbY7ycF2VUC5VlNyr8M4G8D+I/OuT9Zb/sfAfwCgE86534CwIsAfnj926cAvB/AdQBDAD8OAN77m865nwPw2fV+/9B7fzPLDahJqZ1H8wltWQQSIqygBSSn+tBs43k0NUzr+lNr2tnhrLasvp1qVu83k05Z44QaTjU191UAa3uo0Xh/LNmgGlJ9UACJdcl5v5YsUbFg1+28H36/SsDZJsalyuzr7gSc9/7/A5B2pu+L7O8BfCjlXE8BeGpnq9aipITG2zQZV0GkeZWq3bivspYKNGoFBQO1Jel8XTJYRVlAJVz40kwSvnN/nWoDJAcEtp3gHg6Hod26PJbeHwkWJXz0fiyhos9Z3+3v9j+J/XZan+YySZbB6EJkmmjqlsakFFgAEgCiOcUEXk0s1oC1BqE5u0BNUIKY51NtauNgPDeBygGB52eb1dcjUGxHJpi43DDBXK1WQ9qX7kug0udUjaYzIXhfev008qTQaOmiWp8SG8ys5Bpw7LA2X5KjO2/Q1qIENlkZOhMaSJYg4L6aBE1gMY9SY2LAxuSzGoQAnkwmISNG70FDGnp/9Bc1FDGdThPLX6n/pWYj749mMFdypclJIeCUVbXtsP6jBWIam3nVtRoQN73TJNeAo9gAs5IB9JO0lojtPASfzf63v+lx+k5/S0kaBY8upEiw8XeeR6tE64ChYQHOX9OQALUpkFzvTcsFsl1kR1WT8RloW/S5qMSeW2zf02q+ywTQGMObRXINOHZIIBnQJpHA0VqzN5S11OpXQLID813JC+5D9pAg1pAEX0qAUHRtAs0QAZAof65gJqtKsFlNaE1SNRMBBKBqTJLt43k1/sftPNc2QiQrUK+aWHBl0WyU3AOOf7DNAonto36S+mw6v4zMIoEKIBAi6lvRrLSdl/tbU5LAUd/OBtkVTGQZ1cSzpqemhfF3NZ31HrQ9ShixrRrusPmaWTXfLrPyMmmwbRK7T+t/p0muAadAo1bibG5dOFE7lPoqNgis2lJByH0IKq3ybMvecR9qMJ3nRgCodrNmB0mV2MwAZSj1T+W+ej8Ep20/r8Pj9LyacqZZLcpsFsTJbjnLoJJ7wHnvQ9aHju6ad8iYmSbr2lGcc8XYsWq1WsJs1KAxO6mO3LowooJcNaACU9vH2QLKONp8TAWQtoEgohAcHBBs7miM7ud9awjDspR67K7wQBY27irJSQCYa8ABmzlk2zIogE0cSoPkNKcINmUhVSupJtUaIsqCqn+k4NfkZ/UnqR15Ts6h02Mse6j3Yn09mwupRBELydpsEqtlY+ZfDKBpvpxe+yrKtvu2Pnaa5LryMpBcENGmLukab5zXxrlxmh/JfdW80niYJRFiU31iU20s2cLjqTlpPrJupWU3KTZPU2cSWO2jz0M1Hdugvqs1SxXg+trm9GftSIVckjgcqW6ac6zxzw7JRTQAJPwnq/mATdAZ2FD5lUolUSlLCQ6rUfQ3NR3L5XIiPMF9OBFV22PPY8GuFL9th93G49VMZjaMZsXoNdX81N+2ESMqMYLlqpAluyTLs8i9hgOQ6IQ0yZglT5DpWmnKGtK0JOj4nYt20Ax0LrlwozKAup6BgkHBpdoOQPDZ2MlVs8QATd9M1ydgW7ZpI30u6r+qNtZZEGqe2jbYjhLTbmcF1lU1Rym5B5ySCox9MQtDS+F1u91jE1KVtaxUKglzMxbo1jQuACHOphM9bR5nrH2cZqNsogJDzVCb+6kMqgUq75XXZ7vpd2qiNX/TfE4Fn/qEer5t/8O275SrCqisg1KuAacdE9iM9pqVsVwusbe3F8rXeb+aNFqr1YI2UwqeWovgI5hIPug0GCVGNKeT5qL6iAp2Tr2JkRaaImYBxfNrKpomawPJKTVKuGgWjNVaVrPG/EhLyhRyMsk60OQacEDyzy+VSqHgj5YBv3btWmKV0kqlEmqU2NIFWpmZgGLhWJ6PYNTYFK9NsDGVi3E5/q70v842V0ZV74dgiJl5BL9qO30mMcAxB1T31ZcNeQBJ3yMNbNt+KyQp257ThSBNgCQLSB9luVzibW97G+bzeVjdhsexk1PzdLvdoJ2oNWieDgaD4G9Ru6mfpkBR7aIdVX0+DchralWav6Tn0HIMltCwo6iavzpTQtvKtsdmDKQ965P8L1c5THAayTXggGSn0VIHJE7q9Tp6vV6g0gkwriBTr9fDMb1eL2SSUCPVajW02+2wuulwOEyQDPysphqvASABRg1mK41vfSWbua9+pNYpoSiJYn9T8kQBx2fELB3uxxhhbG4fj+N5LZDSgM/jLqMGPMlgcuHDAhQGj9W/abfbAIBbt26F0ABNxU6ng1qthn6/j9dffz0sw6sZKez0BGq73UatVsPBwQH6/T5Go1Gic3E/MqHULvpwdeFHYANG7mMzQpzbFDmKaXK+bK4ncNyXs8ykimp2O3jZvNQsoYEsoYPLCL4ssuu+cw047XQ0E8vlMjqdTtjOhembzSb29/fRaDQwGAzw4osvYjweo9VqhZctX7dcrha9H4/HGAwG6Pf76HQ6aDQaqFQq6PV6Ib42mUyC70gm0taXJOiBpObZRrdrSEC1uWVQFQz6fPR8MRAoAUOySdlTG4I4iRTm5EqyMrhAzgFHkKlmaLVaCRKDhMfe3h7q9Tq++c1vBq13cHAQfBY18ZhHqVN5AKDf7+Po6CisetpqtYIJCiBRQ4QxPs2B1HPZys8UZSqVMdT0NU1U1m0xzaGg05Q3DXMoO8n7TnveWf6Ts4LssmpA7WNpkmvAqWbz3qPVaoU/ikvr7u/vo91uo1Qq4fr161gsFol1t9UsY+fnCG/Zw2q1itlsFkxK9QE5XYckSLlcThTzARDCEVwfPBbvs6SGmptKTqg3TgAAHVBJREFUCvG+YySI9fsoOj2I59OZDZY5tVku26TQZnGJDRwXNg6nMTf6IVwmyvvVajaHh6vlCV566SXMZjO02+2QagUg0Pd8L5VKiekx9H10DblKpYLRaISjo6NQwYskTKPRALCJoxFkHBz4HcAxsKkpSVGgAckisPq7Mp/cTomxlwRbbB0EAs6SOaqNT9qRrrKcZCDKNeCATcfiutvA6o+/77778OCDDwIAbt++De99MCE5eussah7LZGIyluyADFiTkq/X65hOp+j3++GcNBsZUFffSefaafk8ivXBYvE3LVykYomMGLup19IEZs280RCHHQS0nbHPdtuu/a6qNtx137kGHDvy3t5eItuk1Wrh/vvvR61Ww507dzAajdDtdgFs/nwycqoZnHPH2D4AobKxVkhm8BxAqAXJfag9dD02JU5YHt3ei52jp9pGfTbVQFo+ghJL76I1oAMDAc19tC1pJmJsW1ZQXkWQnfSec+3D8c9stVqhM+3t7eGtb30rKpUKXnnlFYxGo+Cz0d+iSabTXJQtpAYCNovYE2DUis45NJvNEBBvNpsYjUaJcuI0b1mJmRpFF0JUwPN3S5yov8ZBQmn8arUaqkPr75bBBTasKMkiarU0siX2vK2pmdapCr8uKVlM7rMsyPgPnHMvO+f+ZP16vxzzM261IOOXnXM/INsfW2+77px7MnY9KzTfWq0WHnzwQXzHd3wHqtUqbty4gcFggGq1GsgNXeeNZiF/4wPR37X+CJe94ppz1KiMu02nUzSbTbRarWDesqPr+m88v1ZV1j9EfTA173gstSuAhObTjBGrtex1SPXzXc1AmzWTRbKyl4XsToHLouG4IOPnnXNdAH/knHtm/dsvee//qbngOwF8AMCfB/AQgN92zr1j/fOvAPh+rJaq+qxz7mnv/XNpFyaBQP+s2Wzi4OAg1OZfLBaBNFGzS81KJVu0aJCal5bGJb3PgDnjVZrArLEsakSSJRwkRqNRwk+zJqBqqXK5nMj/rFQqiXW6uZ9el8Lz8fwkgRRgVuOeVApNd3ckS6nzVwC8sv7cc85xQcY0eRzAJ7z3EwB/5py7jtWKpwBw3Xv/AgA45z6x3jcVcBSuq93tdgMZ4r1PrIaqpiRnf2uuofc+AILmIYGgdU0IRB6jMxRo8mnZBwXAdDpFu90OGSdKyxMQ+l1NSE3KZls544CzD3RgYLtjwlCFanTur9p0mw+3C0RnBZkOQBdRYuRRludxIh/OOfd2bBZk/MsAPuyc+zEAn8NKC97CCox/IIfpwot2Qcbv3nE9vOUtb4H3HsPhEA899FDopGr2kShoNpsAEEqW66RQEhsMDdRqteCTDYfDBHiU0aOWUBNNTUGdcV0qlcLscRIx9PnsTHT1w2jS6hJV/H0wGBxjJK1vqBpPA+mxdfL4LNI6+0mBdFW0267BZxdrSznLgoy/CuDnAPj1+z8D8Heynm/LdT6I1drgoZjr7du3wxoBAEJsjCYewUdWkSBgGhY7Iat/lUolNBqNEOhmqXCrKTUZWs9LE1BZQXY8DgisiqzbaYrKvSZAwwFETUPVajbMYM1Dspq6NJfdz9aFsaLAjvw3id8UbGoiW9P5ssmue9oGzkyAc5EFGb33r8nv/wLAb62/blt4ceeCjF5WQD08PPT9fh/er7JMGEOjaViv19Fut4P/xBABOzW1DgGm9SI1a6Tb7YZO2u/3E0ygBYxWeNbOqbE/gk0ns1qmktt5PDWhmorq4ynJwt90H4rmS/I726u+XZo5GTM57b5px18FbRfTZDro7JKdgHOrsxxbkNE596DfrPH9twB8cf35aQC/5pz7RaxIk0cA/AcADsAjzrlvxwpoHwDwo9uu7b0PScKa8c7Z3O12G957HB0dYTweh+PUz6rVamEuHPebTqdwbrUwfa1Ww3Q6DQwlfT8FrpISBB6vo0SGaiZN67LaTwPpCga2hQCrVqvBRLVmZBoYCEw1Wdkmm62y/h+jsbpC0kUHpF2spJWzLMj4I865d2FlUn4NwN9dN+ZZ59wnsSJD5gA+5L1fAIBz7sNYrXpaBvCU9/7ZXTe2WKyW1WXH2N/fR7PZxGw2w9HRUch7rFQqaDabwQSkudlut1Gv1wMBASBM5+EyVs45HB0doV6vo9lsYjgcBhNWNR1JDZI1Skwwo4Oai8CKUf/rZ5Hwu8iYMuaobbOg0M92dLWdwPpvMROQop3opH7cSWcaXATZxsqmbd8FwLMsyPipLcf8YwD/OLL9U9uOiwmrcZEIcc5hMpng5ZdfDgQIfbjlcol+vw/nVkFrHnv79u0E+0gTlD4TyyowYVkZTqXmVVvZAkXr+0u03Wb4891m/7PDakYLr69Mq14j5h+pP8jfdP6ftm8bQ5mVqYxd+yqJff52AItJrjNNgA3bRlOOfhoLCO3v74d4Gf0wBrrJ/KlWIHFBDcJ4F2udsHOXSqUwO4HXAlYPlXUyqQmVOVSShn+ITtXhu4JNwdPr9UJ9FV1MJEbpWz8rxpbpfDe99i6JESKFbCQGtiyS+1zKfr8fTLvxeIw7d+6ERQ+VXWSHJzHCMAFBNRqNwjlZ6IcLHy6XyzCRlWBREFLrKKlAYsRW2WI7yBjGQgvAhtCJmWJaBoL76sumcfG6VgNbQCpgrdlJiZlDWUbuqySxASirH5drwAFImH662D3zHplBws6sHUpZQp2MSV+NnZcmFzv5wcFBAKMSMOx4Gnejr2aTpXluXUjSaiWd/aASm/1tQwYxUfaU92V9tG3so+4XA1kaM3nV5CyDT64BR1+M8TWWgGu320H7KPNH0GiHoynKfdmZNVDOsnbs5J1OJ0xE9d6HLBBqLXYyhif0+goU7sM/iPuwfWQOdc3x2Eu1omomvU81PXWWup21YLWa1ZAnkULzbSSr+Z1rH44ZG6yoRbBRY3GKzHg8RqfTCdvpn43H45BrSZaSmkpnbtP047X6/T6Wy2Wo4NXpdABsVjFlR9dAtrKWNlTA/dLMP0tmWBOP17QEipIk+pua1labWRNTt6XJLk14WjkNyO+17LIwdkmuAbdcLnHjxo1QteratWu4ceMGgGSsjf7d4eEhhsNhQmPQDOX5mNZFX47xLhIV1BCNRgOz2SwQIzQb1X8i4aLVuyz9T+ApGC1RYv0qvQawIXq20fhqzmpJhRgzacMMMckSLjgLq3nRRS2EGFmVJrkGHGn5+XweJphyFCfJoelP/X4f999/P7z3gVghWJiRoqQHsNFavA5LKHCyKQmWvb29oPW0YynYrP9m07KU5ODsA/UFuV2rPusfq4Frqx3ttdKKBcU0aIxYsS/+vk0uK9i2WSAnldz7cMDKXDs4OACwqVFJqp4dYj6fYzgc4qWXXsKdO3cSc9a89+h2u4G15ARRZpeQkKCG0xLmwMoc5WcGze1qORT1mTRobuNgChgFnDKl6heq/6fPBtgMDmRT1Uy2GhdIr4ESe/ZZ5bKBLcv9xJ7RrueWaw3nnAuVkdvtNm7dupXIGNGS3ZrRMR6PcfPmTSyXS7z1rW/F/fffj8lkEtK/nHOhZgmAEOgmOWI7p3Mu+ImWnFEwKOvIiakEFLerxlJT0uZmahjB0vn6fHS1VQKMJI4+nzSGMQvxsctUvEqmJHD8v9PtuwCXaw3HpONv+ZZvwXg8DonFzKXUjkg/jPmRzjncuXMHzz//PGazWZjJTVDo0sI6j05Tw1hKnQ/YsqTqL2nsjW2nn0fwaMaKzbVULaOEigVpjGzR8Ad90Mlkcmx2QZopGZMsIYCsfstlkdj9xiyOC8tSUhOVSiW8/vrrIS5GIAKb5YY1wZnkCLDK3PjGN76Bd7zjHSEZmABjqICagmGCSqWCdruN2WwWSjRoW7TgKrUwgKDROPmUmtOugqqaS++VYgkVAFGTlNsVtDbnUs9jiY+smimrD3cVZNfAov9zTHKt4dghWB9SA9vAyp/qdDoBKNRW7NDACgTf+MY3ACBoRp6LWSyMWynFz5oofCfY6NspiUHA8/yk5QlEtptA1Xuz/pMlUWyamO6jfppqRE1P4/su0yfGOO7y62JAvAqajmIHoCwDUq41nPermd4kAtgRGS8jlQ9sOrBO4+G2GzduoN/vo9vtBhOQZifXemu1Wmg0GiF7RUGj89SUqOEac/TndIUdBb5OCKVvx++8T9V4ynKqOWmPUTbTzqGL7a/PSX+z29VctRqykJXY55jFfwNyruGWyyV6vV7wlWg2WlaRC23wxU6uc9du3LgRfLS9vb3QydU81WWstNOqDwZszAbVMjobXAFAbQggAER9PiV7+K5z1zQmp6Yyn4nmcXI/Skwrqg9I0Wun+SIWuIUkJfY8Y5JrDceAtpID2jk4kVTX+iYZwdIJ7ES3b98OdD5jeFymGNgwnho0p1bl7ADVdM65RKyLnZjxNYJJwamzsdku1Zg6aChwdLYAxTmXmDNHbZ+mpWKdwO6r100zl64CI3mS+4sRKds0Xa4Bt1gsMBwOMRgMQjUsYEMI6Dw0jvi2vAH9KPpqh4eHwaQjpc5Ozw7MDjifzwPxob9ppyO4gU0YQ809akyarqo1NbitmscmNatG5Hl5DgCJgUAl5rfFzm1Zzyyjdczk1PNddrHEVtaBKNeAA1Yah5WrWFKBfhw7iZIafAgMaNOUYofsdrt44403woKOTM0iqNQXJI1P05UPVEvYaUdVFpMak1OIWEFLtwFJ+18Bo+YjQWKn8/DavG/1BfkcrAbmZx6v56JsMx9PQxRcFskykOzydXMPOAAhSZmxL/0OIJS74xoEJEW896Eyc7lcxnA4xN7eHvb29oLGI+VPkDDnkWDjyjsKSNYe4fQbaiA7a4FaUTWbZo8ASU2h1L9mifD3bWaiNUl1m5VtHcJ2mLTjr4JpqRIjr2Im5C7yJPeAI41PE4+kCQkODSyPRqPQwXXuHE3RxWIR1pLTuXTUnHfu3MFsNgvlGfr9PiaTCYCNdqHJyjUH2Db6adRwWgPTaiKeT/1ACyTLDsb+8BgwLFDTxP5uNWBMk2bReJdNYvcXG9AUaNvicLkGHE07fVcgUXPoPtVqNVRVZsUrgnAwGAQwDgaDQIawpiUfLjXiYrEIZRoGgwGcc2g0GsF009V21MciGFXjUTSrH0DQpgQegWiLF6lpaAkSfV4x7WO3WdMyJruIlqzbr5rs0vy5DgsAm3Sr6XQaCvwwRsZOyYx+HYlLpdWscJZHv337Nm7fvo1erxeqbvHcwCZ7n74hsCE2GHbQiay60ijBTh8uttywAkf9K4JSC85yfwVcTDva7TzOvsfMUe0YGmiPncvKVQdd2oAVC7lYyTXgeAMs5jMejxMlC7jghgIQ2CzW0Wg00Gq1cPv2bdy6dQtHR0eYzWYh35KJ0XaRRmolnp+aiy+d28brKcnC+Xs661pjg6q9dIqOxhgtIGIhA8tuAnG/LWb2ULZ1jiwkwVXz5YB0Py2L5ZBrwKmw88/nc3Q6nUB0UFsBSPhUXOJqPB6HOWyTyST4eZyuQwaUGg7YrB+nMS6Cqt1uB4aSQNH4IIBEWIKgoqivR+2omk6Lw1JisTFNgrbai+82/5JiNa+du6fnsfvb32LfL7JkHTxOO1DlGnAcPQmoarWKwWCA5XIZyh5wO1O1qGU4700X6pjP5yFzhT4dM1jIajK9S4PjNGU1zKA+lIKV5ddZi4XAYF4m91eNpGlrvG/+xu+8rgbTKQr+tI5gHf0YMWP3jZ1jF7guE/isbBtoLDucJrkGHIBEB2Phn16vh8VigYODg2AeEpT03Q4PDwOrqCuScp/BYIDRaITxeJwoBsSOTCB2u90wUZX+HoHEXEotXa7ZLCxbTg3Gz5bcsJqLwKa5yf1iVbi2mZfcDsQJlRjzuM1EtKTLVRPLSsbM97RQDCXL2gINAL8HoL7e/99473/WrdYI+ASAbwHwRwD+tvd+6pyrA/g4gL8E4A0A/633/mvrc/0MgJ8AsADw33vvP73r+pox4txm/tmdO3fQ7Xaxt7cXkpIZVwOQ2GbDA5PJJIQGvF+VY2DpvGaziX6/nyi9oKYmtSlNWc2JpM/GIq4asKZfp/VJ1FTUxTf4pylQKdxXiRfzfyU6Q4zJ1M8WxLGBwP6+7f2yirUQdBsl7ZmrZNFwEwB/zXv/FwC8C8Bjzrn3AvgnWK2A+ucA3MIKSFi/31pv/6X1fnDJlVEfA/C/OeeSmbdGSNfTHGOaFbP4+/0+bt68iTfeeAOj0Qj1eh2dTgd7e3uJTknfjxqnUqng8PAwrA0OrECxt7cX4nvscErEcJ4bwamzvvmgZ7NZIHeUSOE19I/T8nYxM88ykDoVx0qWDq9A0hd/03NZkG07/1UA27YQSYwZTpOdgPMr6a+/VtcvD+CvAfg36+0fA/A3158fX3/H+vfvc6sWhJVRvfd/BkBXRo2Kcw7dbjdoMs59q9VqwWysVCro9Xp47bXX8Oqrr4a15Fj7hP4YgBA+oPlHDTUajTCZTHB0dBRIFF2aiv6fljNQTcPzUKsxB1PBxDYxfqeVxOzUGs6bszG8mKaxJg4/x3yKmEm5rXrXLtZtF9Auuj+3a5CJ3d+ue87kwznnym61cs7rAJ4B8FUAt733LJqhq5y+BeuVTte/H2FldobtkWP0Wh90zn3OOfe58XiMN954A7du3UK/3w8pVrwxahuaj/1+H2+88QaGwyHa7XbQjtQ2BIN28HK5HFLD+FKGkfswPkcQsy1M7dIAOAFDjWTBo2C12kNrmPBP1eMJYnleUbMx5svFUsq2/OcnAsxFB9cusYNY7DlmeQaZAOe9X3jv34XVIorvAfCfn7jFGcV7/xHv/aPe+0e1nv9kMgmpVzp5lAAol1cr4EwmE7z66qtBy7RarWAKahk7zQJhMJsxN4JmuVyGxGid5T0cDkMszq6c470POZmk29lOXlvjcdQwtiyexurSnHMlgbRDKJBjncQC3YJrm3m55X878X99UUEa8+cou57DiVhK7/1tAL8D4L8EcOCcI+miq5mGFVDXv+9jRZ5sWxk1VdR3oobRHMV1uwIYKpUKhsMher0eSqVS0HQatGYHJbNJcoSmJrP7dRKoFg0i6cIZB2zDcDgM1ZoJUL0GtR3vS+NxNF3Xzy2wpEAyP5J/sNX0FkAKtrRMEt039ptKrCNdVMCcVXaZ32fy4ZxzDzjnDtafmwC+H8CXsALeD613ewLAb6w/P73+jvXv/49fteZpAB9wztXXDCdXRk0V1TTsdAxecy01O4WFGoUzvBkT0xoj6p8Bm0JENCc1Lkayhqlb1Gxc2oogoR/IgrLK/vFaylAquIDj7GBsFLUaK00s2NL2t1px27nTTNfLRJic5V6sb5wmWZKXHwTwMbdiFEsAPum9/y3n3HMAPuGc+0cA/hirZYmxfv9XzrnrAG5ixUzCb1kZNU1YYoE3xHlkmt9IwoKaiEHpmzdv4tatW3j44YfR6XQwGAwS5Q5izKBqMWDjT6n/xTXGGVBnSXS2T4kPJVmUbGEhWy3Yqm3RGRF6X/zM52El9ofHQKLHx/xIldiAkHbdyyinIUa2/Z5lBdQvAPiLke0vIMIyeu/HAP6blHNFV0bdJvP5PCwpTE1DzTeZTBLFhLTUwmw2w8svv4wHHngA+/v7GI1G8N4HgFBDUWsxw0SLwZL215V3RqNRuM76nhJ1LUejUUKzsDoYU8xUY2tKGDWizmQnwFRb2s5ta5hY386ykGmmZ+zckf8vuj0NiJdFYn5azO/NYn3kOtPEuU2qlp0zBiQJCvWHCIhvfvOb6PV6gWV0zuHg4CB81gwR+m18aNSa1EAEOhOpK5VKABFLodN3U5aRgOX5aZoCm8JAFF5T/1DVROqP6R9rWUu9BwWY1WTbTCD7W5Z9L5NPF3tOlnyinOS+cw04YJOl4b0PtUN01PZ+VdqcQW12ymazidlshqOjI3jvE+EB51wIkLfb7ZAaxropOteOGocgYcYIsNFoXF+Ober1eomAOUHCAL5W2yLAbLk/ioKQv/O7mqKqKfXZUNKApwNDmlx23y2LZPGbs+yb6wmowAZw9IeYZGw7IrWT+kTOrcqdcy5dp9NBu90OGq1cLieyS0ic0KykH0ayhWwm/TqCl4DlOZm7qX5mq9XCaDQ6Rv+rxuJ9WVZS2U2NIfJ4qxkBJDQcJQZACxz1E/l9Fxj1updRstybNS/TJPeAYydk56efZStV0efidmrFmzdvot/vB+2lbCVFO6cWl9WQxHQ6xe3btzEYDMJxOkuAILF+JhOfSayMRqOQU+ncas4e/yD17WzRWf6ubWbwXX1OBR7BaM8Tu3d93tqek2hCe97LLGlm5a4BKveA08wN1QL1ej2wflqCjpqAQexer4fbt28Hk5AdSo9lR2I6mAKJ7CLNVhtX0ykz9Xod4/E4+J6MFzIBmsAgmAgwno8g4Xcg+ccqgPjdahj+HqOp0xjJLFos9v0y+m4qWRlKa11skwsBOP1MIHBmALWcrXDM7aPRCP1+H/fdd1/IKqHpx+x+zTIBklOCyHhqYaBGo5Hw0YCNBiBhQ3BZE5cA1j+HJiOvqcdryENNTDvTwE6CtcBS3y82Om/z02Is5mXz4bYxsNvEEim7nkuuSRN2rBg17r0Pk0SbzWZi4imwmVfG8gkMCxAAwOYhaU6kVghTALAsHjNNdIqNaif6awBCqEFNX94DQWc1HPexeZPKPup2fSYKqDRWcZtpaffb9v2qyLb7joFN+0xMcg04AMf8CzUBF4tFIDk6nU5i1VPuDyCATuNs3m8C0HxInLCqLCD3V+AQPMpcApvJr91uN4CRvhtnLWjbdaaAmoRWU6tGs2LDCHrfuo3bY2ya+mdXkYHcJjY0o++xz7s0Yu4BR9FRXzP+p9NpIDI4MVSzOkh4sJNrESJOmQGQyOogWUIKn0SIahul+4HNDHGuQTccDoPJSpMxxjQqoaHaT8swKBi1MJFq6JhfxmvY37YF0mPHb/tPLqNksQDSfr/wgGOnJCGiJhg7Dou60s9iQJsJycxxZL1KWyyImo/z1AgM7k/gap4lyRULUk6MBZKZIgSRajU1CxV8GhrgfmyT3rs+H9Vy1sfTfdKecRZgxfa5rKADtpMmOiBZLbfVDM3zA3PO9QB8+V63I0dyP4Ab97oROZG8P4u3ee8fsBvzzlJ+2Xv/6L1uRF7EOfe54nms5KI+i9yblIUUcpmkAFwhhZyj5B1wH7nXDciZFM9jIxfyWeSaNCmkkMsmeddwhRRyqaQAXCGFnKPkFnDOucecc192zl13zj15r9vzZohz7inn3OvOuS/Ktvucc884576yfj9cb3fOuX++fh5fcM69W455Yr3/V5xzT8SulXdxzj3snPsd59xzzrlnnXN/b739cj2PWB7dvX4BKGNVbPY/A1AD8KcA3nmv2/Um3Od/BeDdAL4o2/4XAE+uPz8J4J+sP78fwP8JwAF4L4A/XG+/D8AL6/fD9efDe31vp3gWDwJ49/pzF8B/AvDOy/Y88qrh3gPguvf+Be/9FKtFQx6/x2266+K9/z2sKpupaKl4W0L+434lf4BVXdAHAfwAgGe89ze997ewqoz92Jvf+rsr3vtXvPefX3/uYVWK8S24ZM8jr4DLVBb9kso17/0r68+vAri2/pz2TC7ds3LOvR2rSnF/iEv2PPIKuEIA+JWNdKXiNs65DoB/C+Dve+/v6G+X4XnkFXCnKot+SeS1tWmE9fvr6+1pz+TSPCvnXBUrsP1r7/2/W2++VM8jr4D7LIBHnHPf7pyrYVW9+el73KbzEi0Vb0vI/9ianXsvgKO1qfVpAO9zzh2uGbz3rbddKHGrOS4fBfAl7/0vyk+X63nca9ZmC2v1fqyYqq8C+J/udXvepHv8dQCvAJhh5Wv8BFZLe30GwFcA/DaA+9b7OgC/sn4e/xHAo3Kev4PVenvXAfz4vb6vUz6Lv4KVufgFAH+yfr3/sj2PIrWrkELOUfJqUhZSyKWUAnCFFHKOUgCukELOUQrAFVLIOUoBuEIKOUcpAFdIIecoBeAKKeQc5f8H1TuS2WXkfRIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## view_label_mat_data"
      ],
      "metadata": {
        "id": "vGTaJQqjpe9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mat_file_name =  \"/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/train/label/002.mat\"\n",
        "mat_file = scipy.io.loadmat(mat_file_name)\n",
        "\n",
        "print(type(mat_file))\n",
        "\n",
        "for i in mat_file:\n",
        "    print(i)\n",
        "\n",
        "mat_file_value = mat_file['label_separated']\n",
        "print(\"size :\",mat_file_value.shape)\n",
        "print(\"datatype:\", type(mat_file_value[0][0][0]))\n",
        "\n",
        "print(mat_file_value[:,:,6])\n",
        "plt.imshow(mat_file_value[:,:,6],cmap='gray')#plt.cm.bone\n",
        "mat_file_value[:,:,6] = mat_file_value[:,:,6]*(-1)\n",
        "#datatype = np.ndarray\n",
        "print(\"==================\")\n",
        "print(mat_file_value[:,:,6])\n",
        "print(\"mat_file_value\")\n",
        "print(np.amin(mat_file_value[:,:,6]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx5Les7Aphc9",
        "outputId": "a236b326-6949-4f73-83e5-ccec40236483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "__header__\n",
            "__version__\n",
            "__globals__\n",
            "label_separated\n",
            "size : (3232, 2588, 7)\n",
            "datatype: <class 'numpy.uint8'>\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " ...\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "==================\n",
            "[[255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " ...\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]\n",
            " [255 255 255 ... 255 255 255]]\n",
            "mat_file_value\n",
            "0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet Network"
      ],
      "metadata": {
        "id": "2jJv_N7wRhSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/datasets'\n",
        "ckpt_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/checkpoint'\n",
        "log_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/log'\n",
        "result_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/result'\n",
        "board_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/leaderboard'\n",
        "test_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/test'"
      ],
      "metadata": {
        "id": "DGCUTmJ7SF2M"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "## U-Net 구조 설계\n",
        "class UNET(nn.Module):\n",
        "\n",
        "    # U-Net에서 활용할 기본 block 들을 정의합니다. \n",
        "    def __init__(\n",
        "            self, in_channels=1, out_channels=6, features=[64, 128, 256, 512],\n",
        "    ):\n",
        "        super(UNET, self).__init__()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Contractive Path에 필요한 모듈을 self.downs 리스트에 보관합니다. DoubleConv blocks.\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Expansive Path에 필요한 모듈을 self.ups 리스트에 보관합니다. Transpose2d Convolution 및 DoubleConv blocks.\n",
        "        for feature in reversed(features): #[512, 256, 128, 64]\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2,\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "            \n",
        "        #self.up : [(ConvTransposed2d, DoubleConv) *4]\n",
        "\n",
        "\n",
        "        # U-Net의 가장 아래 부분.\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "\n",
        "        # 가장 마지막 Convolution layer. 1x1 Conv. 로 Output channel 을 맞추는 역할. \n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    # __init__ 에서 정의한 요소들로 실제 U-Net을 설계.\n",
        "    def forward(self, x):\n",
        "\n",
        "        # skip-connection 시킬 feature map 들을 보관하고 있을 리스트.\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs: #self.downs: (DoubleConv *4)\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]  #순서 뒤집어줌\n",
        "\n",
        "        ## self.ups: [(ConvTranspose2D, DoubleConv) x 4]\n",
        "        for idx in range(0, len(self.ups), 2): ## idx: 0, 2, 4, 6\n",
        "\n",
        "            # ConvTransposed2d\n",
        "            x = self.ups[idx](x) \n",
        "            skip_connection = skip_connections[idx//2] ## idx//2: 0, 1, 2, 3\n",
        "\n",
        "            # Resize the upsampled feature map to the size of the skip-connected feature map.\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            \n",
        "            # DoubleConv\n",
        "            x = self.ups[idx+1](concat_skip) ## idx+1: 1, 3, 5, 7\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "f9edEHIg0Kbv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 기본 제목 텍스트\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "        \n",
        "        #Convolution, Batch norm, ReLU set\n",
        "        def CBR2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, bias = True):\n",
        "            #convolution layer\n",
        "            layers = []\n",
        "            layers += [nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = kernel_size, stride = stride, padding = padding, bias = bias)]\n",
        "            #batch norm layer\n",
        "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
        "            #ReLU layer\n",
        "            layers += [nn.ReLU()]\n",
        "\n",
        "            cbr = nn.Sequential(*layers)\n",
        "            \n",
        "            return cbr\n",
        "        \n",
        "        #contracting path\n",
        "        self.enc1_1 = CBR2d(in_channels = 1, out_channels = c_a[0] )\n",
        "        self.enc1_2 = CBR2d(in_channels = c_a[0], out_channels = c_a[0])\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "        self.enc2_1 = CBR2d(in_channels = c_a[0], out_channels = c_a[1])\n",
        "        self.enc2_2 = CBR2d(in_channels = c_a[1], out_channels = c_a[1])\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "        self.enc3_1 = CBR2d(in_channels = c_a[1] , out_channels = c_a[2])\n",
        "        self.enc3_2 = CBR2d(in_channels = c_a[2], out_channels = c_a[2])\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "        self.enc4_1 = CBR2d(in_channels = c_a[2], out_channels = c_a[3])\n",
        "        self.enc4_2 = CBR2d(in_channels = c_a[3], out_channels = c_a[3])\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "        self.enc5_1 = CBR2d(in_channels = c_a[3], out_channels = c_a[4])\n",
        "\n",
        "        #expansive path\n",
        "        self.dec5_1 = CBR2d(in_channels= c_a[4] , out_channels = c_a[3])\n",
        "\n",
        "        self.unpool4 = nn.ConvTranspose2d(in_channels= c_a[3], out_channels = c_a[3] , kernel_size=2 , stride= 2, padding =0 , bias = True)\n",
        "\n",
        "        self.dec4_2 = CBR2d(in_channels= c_a[3]*2 , out_channels = c_a[3])\n",
        "        self.dec4_1 = CBR2d(in_channels= c_a[3] , out_channels = c_a[2])\n",
        "\n",
        "        self.unpool3 = nn.ConvTranspose2d(in_channels = c_a[2] , out_channels = c_a[2] , kernel_size=2 , stride=2 , padding =0 , bias = True)\n",
        "\n",
        "        self.dec3_2 = CBR2d(in_channels= c_a[2]*2 , out_channels = c_a[2])\n",
        "        self.dec3_1 = CBR2d(in_channels= c_a[2], out_channels = c_a[1])\n",
        "\n",
        "        self.unpool2 = nn.ConvTranspose2d(in_channels= c_a[1], out_channels=c_a[1] , kernel_size=2 , stride=2 , padding =0 , bias = True)\n",
        "\n",
        "        self.dec2_2 = CBR2d(in_channels=2*c_a[1] , out_channels = c_a[1])\n",
        "        self.dec2_1 = CBR2d(in_channels= c_a[1], out_channels=c_a[0])\n",
        "\n",
        "        self.unpool1 = nn.ConvTranspose2d(in_channels= c_a[0], out_channels=c_a[0] , kernel_size=2 , stride=2 , padding = 0, bias = True)\n",
        "\n",
        "        self.dec1_2 = CBR2d(in_channels= 2*c_a[0], out_channels = c_a[0])\n",
        "        self.dec1_1 = CBR2d(in_channels= c_a[0], out_channels= c_a[0])\n",
        "\n",
        "        self.fc = nn.Conv2d(in_channels= c_a[0], out_channels= 6, kernel_size=1 , stride=1 , padding =0 , bias = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        enc1_1 = self.enc1_1(x)\n",
        "        enc1_2 = self.enc1_2(enc1_1)\n",
        "        pool1 = self.pool1(enc1_2)\n",
        "\n",
        "        enc2_1 = self.enc2_1(pool1)\n",
        "        enc2_2 = self.enc2_2(enc2_1)\n",
        "        pool2 = self.pool2(enc2_2)\n",
        "        \n",
        "        enc3_1 = self.enc3_1(pool2)\n",
        "        enc3_2 = self.enc3_2(enc3_1)\n",
        "        pool3 = self.pool3(enc3_2)\n",
        "        \n",
        "        enc4_1 = self.enc4_1(pool3)\n",
        "        enc4_2 = self.enc4_2(enc4_1)\n",
        "        pool4 = self.pool4(enc4_2)\n",
        "\n",
        "        enc5_1 = self.enc5_1(pool4)\n",
        "\n",
        "        dec5_1 = self.dec5_1(enc5_1)\n",
        "\n",
        "        unpool4 = self.unpool4(dec5_1)\n",
        "        \n",
        "        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n",
        "        \n",
        "        dec4_2 = self.dec4_2(cat4)\n",
        "        dec4_1 = self.dec4_1(dec4_2)\n",
        "        \n",
        "        unpool3 = self.unpool3(dec4_1)\n",
        "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
        "        dec3_2 = self.dec3_2(cat3)\n",
        "        dec3_1 = self.dec3_1(dec3_2)\n",
        "\n",
        "        unpool2 = self.unpool2(dec3_1)\n",
        "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
        "        dec2_2 = self.dec2_2(cat2)\n",
        "        dec2_1 = self.dec2_1(dec2_2)\n",
        "        \n",
        "        unpool1 = self.unpool1(dec2_1)\n",
        "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
        "        dec1_2 = self.dec1_2(cat1)\n",
        "        dec1_1 = self.dec1_1(dec1_2)\n",
        "        \n",
        "        x = self.fc(dec1_1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "fpQPDaFOSki0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "wpRQ1E8ijtrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, transform = None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        if self.data_dir == '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/test':\n",
        "            lst_input = os.listdir(os.path.join(self.data_dir,'img'))\n",
        "            lst_label = os.listdir(os.path.join(self.data_dir, 'img'))\n",
        "\n",
        "            lst_input.sort()\n",
        "            lst_label.sort()\n",
        "\n",
        "            self.lst_input = lst_input\n",
        "            self.lst_label = lst_label\n",
        "        else:\n",
        "            lst_input = os.listdir(os.path.join(self.data_dir,'img'))\n",
        "            lst_label = os.listdir(os.path.join(self.data_dir, 'label'))\n",
        "\n",
        "            lst_input.sort()\n",
        "            lst_label.sort()\n",
        "\n",
        "            self.lst_input = lst_input\n",
        "            self.lst_label = lst_label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lst_label)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        #testset 일 경우\n",
        "        if self.data_dir == '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/test':\n",
        "            #label은 없어야하지만 dataset을 구성해주기 위해 input을 복사해주어 structure만 만들어줌.\n",
        "            \n",
        "            #absolute path of img and label\n",
        "            input_abs_path = os.path.join(*[self.data_dir, 'img', self.lst_input[index]])\n",
        "            label_abs_path = os.path.join(*[self.data_dir, 'img', self.lst_input[index]])\n",
        "            \n",
        "           #np.ndarray of img\n",
        "            images = sitk.ReadImage(input_abs_path)\n",
        "            input_np =sitk.GetArrayFromImage(images).astype('float32')\n",
        "            input_np_copy = input_np.copy()\n",
        "\n",
        "            #input array를 0~1 float 로 바꿔줌\n",
        "            input_np_copy1 = input_np_copy - np.min(input_np_copy)\n",
        "            input_np_copy = input_np_copy1 / np.max(input_np_copy1)\n",
        "\n",
        "            #false np.darray of label            \n",
        "            label_np =sitk.GetArrayFromImage(images).astype('float32')\n",
        "\n",
        "            \n",
        "            data = {'input' : input_np_copy, 'label' : label_np}\n",
        "\n",
        "            if self.transform:\n",
        "                data = self.transform(data)\n",
        "\n",
        "            return data \n",
        "\n",
        "        #train, validation set 일 경우\n",
        "        else:\n",
        "        \n",
        "            #absolute path of img and label\n",
        "            input_abs_path = os.path.join(*[self.data_dir, 'img', self.lst_input[index]])\n",
        "            label_abs_path = os.path.join(*[self.data_dir, 'label', self.lst_label[index]])\n",
        "            \n",
        "            #np.ndarray of img and label\n",
        "            images = sitk.ReadImage(input_abs_path)\n",
        "            input_np =sitk.GetArrayFromImage(images).astype('float32')\n",
        "            input_np_copy = input_np.copy()\n",
        "\n",
        "            #input array를 0~1 float 로 바꿔줌\n",
        "            input_np_copy1 = input_np_copy - np.min(input_np_copy)\n",
        "            input_np_copy = input_np_copy1 / np.max(input_np_copy1)\n",
        "\n",
        "            #label numpy array\n",
        "            label_mat_file = scipy.io.loadmat(label_abs_path)\n",
        "            label_np = label_mat_file['label_separated']\n",
        "\n",
        "            label = label_np.transpose(2,0,1)[:6,:,:].copy()\n",
        "            # label[6] = -label[6]+1\n",
        "\n",
        "            #data['input'] = 0~1 float32 , data['label'] = 0~1 float32\n",
        "            data = {'input' : input_np_copy, 'label' : label.astype('float32')}\n",
        "\n",
        "            if self.transform:\n",
        "                data = self.transform(data)\n",
        "\n",
        "            return data        "
      ],
      "metadata": {
        "id": "lPn8czK0jwpL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset example"
      ],
      "metadata": {
        "id": "wqd3l-ixN7qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = Dataset(data_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/train')\n",
        "\n",
        "# print(example.lst_input)\n",
        "# for name in example.lst_input:\n",
        "#     print(name[:-3])\n",
        "#     break\n",
        "# print(example.lst_input.pop(0)[:-3])\n",
        "\n",
        "data = example.__getitem__(1)\n",
        "\n",
        "input = data['input']\n",
        "label = data['label']\n",
        "\n",
        "# input = cv2.resize(input.transpose(1,2,0), (2048,2048)) \n",
        "# label = cv2.resize(label.transpose(1,2,0), (2048,2048)) \n",
        "\n",
        "# print(input.shape)\n",
        "# print(input[0])\n",
        "# print(label.shape)\n",
        "# print(label[6])\n",
        "# print(type(np.amax(label[6])))\n",
        "# print(type(np.amax(input[0])))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(input.transpose(2,0,1).shape)\n",
        "# print(input.transpose(1,2,0).shape)\n",
        "# plt.imshow(input.transpose(2,0,1), cmap='gray')\n",
        "\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.imshow(input.squeeze(), cmap='gray')\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.imshow(label[0], cmap='gray')\n",
        "plt.subplot(133)\n",
        "plt.imshow(label[1], cmap='gray')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "AcAc07eDvGCO",
        "outputId": "8203f061-1678-44c6-9e32-a03f40ba155c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe68cedbe50>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAACeCAYAAADe3trJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dX4wk13Xev9vd1VVd3T3Ts8tdihZpUnEER7QBijJNrZE4CBhblhjZ0kNsWLBkwhJAA7IAG85DqORBjswH2QFiwYZCSIaMyJRhh4gMUBAUUwzDOH6xTJmxKVGCxDUpebWiuLN/Znv6T1VXd988dH13TtX2zOzs9szcnjk/YDA91dVVd+bufvfUOeeeY6y1UBRFUY4HlcMegKIoinJwqOgriqIcI1T0FUVRjhEq+oqiKMcIFX1FUZRjhIq+oijKMeLARd8Y83ZjzDeNMWeNMY8c9P2V/UHn9Wii83r0MAeZp2+MqQL4FoCfBvBdAM8BeI+19usHNghl4ei8Hk10Xo8mB23p3w/grLX2ZWvtCMCfAXjXAY9BWTw6r0cTndcjyEGL/usBnBM/fzc/piw3Oq9HE53XI0jtsAdQxhjzMICH8x9/rFqtwhhTOIcuKWstrLXXvF+6njtfnmeMcV/zkNeV50ynU1hrUalUYK3FdDp171cqFUwmk2vGValU3Gu+J+9TPrbd77Foyvcs/32ArbFnWXbRWnvqRu9Vntcbvc5u3HvvvahUirbMt771LWxubu7XLZedm5pX4ODmVtkb1tq5onHQon8ewB3i59vzYw5r7acAfAoAarWa7XQ6CIIAlUoF0+kU1WoVo9EI4/EYk8nEiW6tVoO1FtVqFZPJxP3Hp7BVKhXUajUnZs1m0/2cJAm4uPC7FO3xeIzpdIrRaIQsyzCdTlGr1bCysoLhcOjOqVQq2NzcdGMDgDiO0Wg0UK1WkaYpRqMRptMpoihCv9+HtdYd226hmffzIpCLpzHGCXylUkGlUkG1WkW9XkcYhjh//vx3drjUnubVGLMvgaR2u43nn3/+muOTyQSf/vSn8YlPfALr6+sYj8cAgCiKcO7cuWvOP2bc1LwCBzO3yuI4aPfOcwDeaIx5gzGmDuAXAXx+pw9QhKfTqRNWilOtVkO9XkcURZhOp4XjwJbQh2GIKIpQrVZRq9UQRRGCIHCLSb1ed5+Rgh+GoVsc6vW6uzat/OFwiDiOEccxkiRBmqYFi75SqSCKIoRhWFio6vU6kiTBdDp1i4Mv3ODisud53Q++9KUvzT1erVbx8MMP47nnnsOLL77ovt7//vcf8AiXDi/mVVksB2rpW2vHxpgPAXgKQBXAH1lrX9zufGl9UtSzLHOCSmuU5/JJwFrrxLler6NSqWA8HheEWIp8rVZz1juP0bqv1+sAZpZ8GIZIksQ9IXAMXBAGgwGCIHDXqlarCIIAANwTQr1ed08oAAruIPl7HxZld5m1dteFaa/zuh+85z3vwZkzZ3Y8p16vu/kEgEcffXS/h7XU+DCvyuI5cJ++tfaLAL64188FQYA0TZ0AUVQpUhRbLhLGGERR5ISZTwpBEDjBli4VLh4A3D2m06lzxVDsrbXOPRDHMQA4cU+SBFmWuScDjiVNU+d6Msa46/P79cQm9hs5hnLM4XrGdaPzugjiOMbjjz++p8889thj3j1l+chhzquyP3gXyN0OadHTqpeCb4xBHMfOVUOrfTweYzweu0WATwP8Dy+DflLsqtUqqtWqE3jePwxDVKtVZ6nzKSIIAnecTw+tVgtJkhRiBvTf84mF990pqLzfi0FZ6DmmZeEv/uIvCov29fChD31on0ajKH7jtehT0Cm+dOdQMOmXB+CEPooi55qZTCaYTCZO5Cn8FPJ596PY87MyI4dPDcCWzx6AuweFnaJfXjB4HWlhlq18X6x9+bPvC8D6+joGg4Fz3e0Gn+AU5Tjife0dineapqhUKs5VQgHma1rZ4/HYLRIMxlK4sixzVrf06/Na0jXEIC+FnUFhAG4BIXTdNBoNF0+IoghpmjqXE/34jC1IKx84XLHfiXlPAb7x8z//87jnnntw33334aGHHsJXv/rVHc//3Oc+d0AjUxT/ONAyDHulXq/b06dPF7J3KKrAzM9P37lMowRmbp1Go4F+v4/RaHRN7nalUkEQBO5aUqD5N+FCQ6udrpwsy9z9a7Uaer0ehsNhIfgbRVEhHZNuJoo/v2j172Tp7+eCMC9tUwbJGZ947bXX/tZae98i7rnfaX1RFOHnfu7n8Mgjj+Dee++95v1Wq4V+v7+fQ1gmFjavwOLmVj4pKzfGdnn6Xlv60r1A4WMAl/nvzKQJwxD1eh1pmqLf76PRaGA4HLoALLN5gC0/vhTwOI7RarXcfZltM51OneuAC04QBLDWIkkSJ/hJkrini2az6a7PDVwyS6ds5fvKsoyzTJIkeOKJJ3DmzBm8733vw9e+9jX33uXLl1XwPafVauGv/uqv8Pjjj+OXfumXrjHYlJvD67+mzJmnVR+GIVZWVtBut53o0+JmgNQYg3q9jn6/X7AWoii6Jl1zOp0iSRJ3LvPyAbj3sizDaDSCtdYtDisrK87dJH33jUbDuX+4cMgsHTLPV37QVv71sAw+/e0YjUb47Gc/ix//8R/HBz/4QWxsbOCtb33rYQ9L2YV3vOMdOHPmDN773vfis5/9LJ588kn81E/91J6D9cp8vBZ9oBhcpaulUqkgSZKCGEk3Cd0q9LVzMeCGqHq9jlar5d6bTCYYj8fodrvo9/sFFwyvyc1Vg8EAvV4Po9HIbcZiUDkMw2vuz2tIK18e5/V9o+z2WWaSJMFjjz2GH/3RH8XZs2cPezjKLpQzq975znfiqaeewjPPPIN3v/vdaLfbhzSyo4H3Pv1Tp065DTW0oBuNhsvaKde4oZhGUYRut3vNBi7m2vPpgJk6jAn0+32kaYosy9xCwg1VFParV68CmImJ3DjG7J0sywoBWy4s8jtTTnfL3Nlvwd2tFAOfsC5cuLA0Pn1lT3jn00+SBGEYzn3PWotvf/vb+MIXvoD19XX80A/9EB5++GGMRqObve2RYzufvveif8sttxRKH1QqFWdRy+wabqACtoQsTVPn9mEuP5HZP7LmDgOvg8HAuZV4TaZ8XrlyBdZal0nEBYRPDNKHD+Aawed78knisHL0y357/h1U9I8NXol+EAR7FnANzM9nKQO5QLEUg8yKkTtqa7UasixzOdp0sVDIZJXLcq69TKUcj8fOn8+aPdVqFWEYOvEH4Hb6crHgWBgUljn80qIvu3s41p1+94OinIJ60PdXFAD4zd/8zT1/RgV/b3i9OQvYCrbycY9pk4S7YelOoStnNBq519KNw2tw5yxLOcjyDqzRIgur0YUznU7RbrcxmUzcEwgXDF5bMi9bx6enK5mHz7Hy916WzVnK0aDZbOK3fuu39vSZV155ZX8Gc4TxXvSBopAymCstaKZLUqhpcdNqB7ZcLBL61mW2Dq/HzVmyjINM+wzD0KV8UvC5QAFwVn1ZPOcJv88Wtc9jU44Wf/mXf3ldO6olDzzwwD6N5ujitXtHCqMUZIosA7v089P/PBgMMJlMnH9e1sHnBilp2cv699bOCqr1+30Mh0OEYehcOSzqxjFIlw6AgoUsffpla7kcwPUFWX6BqJWvHBQ/9mN777/y7W9/e/EDOeJ4belLEaLA04Km6MpSyfTtMVefFS5pncv4gPTxMzuHu25lgHUymSAMQ9d5qVqtotvtYjgcFnbv8lzW65FCz/f2Yt37tiAoim988pOfPOwhLCVeiz6RxdMo/nKXLd0ssi7PdDpFq9VCGIbOiqffn8Isc/SHwyGq1SoGgwHiOHbuG2b+1Go1l6tPHz8XD4o5j/GpBCi6eeTPPhRYm8e8JxC19pWD4MMf/jDe/OY34/7778ctt9yCOI533JD1wQ9+8ABHd3TwWvSn0yn6/b7z41PQZdVKnkchl0HaJElc3R0ZoCWDwcBZ93wqMMa43bmtVqtQtXM4HLrYwDyXDSmXXJjn5vFN7LdDBV85KD72sY8BmLW9PHnyJO688063M/fuu+8uxPaef/55rZR6g3idp1+tVm2r1Spk0gCzOjkUc37JypdcFMquHL7H3q+tVgvD4RC9Xg/GmELzbJZsaDQa6Ha76PV6boFgfEDW8pEppFwkyhuzZJbQYW7ImoeMM8hUVMZJ1tfXNU//aOJVnv48oijCT/7kT+KjH/0o7r//flQqFXQ6HbdJUpnPdnn6Xlv6QDGlUDZQoeUvRZ05+7LnLd03AFx6ZpqmSNMUGxsbOHnypCvOxkXCWuviAnQP8elBBo95T2ArlbTsuy9bI7vtwPUNTdlUDpskSfD000/j2WefxQMPPIC77rpLBf8m8Fr0b7vtNvzqr/4qPv7xjxeOM4WS1r9scgJsCTBdOUy/LNfF4Uav4XDosnBkn11a7CzqJi14ueAwQ6hcZ6e843aZHkc1g0fxjfF4jC996UuHPYylx+uUzQsXLuDVV1/FO9/5zsLOWRkslSUUaDmPx2MXdOXXaDRyVnoYhlhbW0Or1Sp8RjY0Z74wN3w1m81r7sv35aYmYHsLXgZ/fWXZYg6KouwNr0V/Mpng+eefxw//8A+7Y+XdohQn+v1lK0IuELTax+Ox61m7sbGBjY0Nl4vfbrdda0amccp6OszXl+UYiHTXzBP+vexsPUyxvdExK4qyPHjt3rn99tvxB3/wB/jZn/1ZAFuuFwAuXbMsksYY58OX6Zn8vDyPn2WZB6aCcmGQ9XhY26fX67lr8/u8ujXl7J2ya2cZLOllGKOiKHvDa9G/dOkSnn32Wdf9ihkzcncssJWlU7b0Zf48sNXAnGmf0vVTrVYLVryMCTBdU9almbf7tlxBU5ZQXgbXDlF/vqIcXbx27wwGA7z3ve8tWMoskSytfGmxy/r287pWMb1StjKUNXxY1jWKIufiYTxgMBgAgFs4KOKyqqfcSCYFf1nFU2ZPKYqy/Hht6bN+vUzRlDn4FG4iLXDZ+SrLMtfkm9elcPM9WZCN51WrVdcukRb8eDx22T3S2pd7BcrZOuXU0p1+X59QsVeUo4fXog8Af/iHf1gQH7Y4LIsuAGfh0+VC0ZUtEwnLK4xGI4xGI9RqNayuriIIAgyHQ7TbbZcBRPFnnj4XjHm9b4kU/rLf3zdxn4cKvqIcTbwX/UcffRS1Ws25dLIsc1k13EAl0zjL2TPS718O6FKMmdUDAJ1Ox1nyo9EI/X7/GsGeV/KhnOood+IuMz5WA1UU5cbxXvQpynLjFUWW1jTfYzA2iqLCEwFdQvw5TVPnipGNVZIkwebmJtrtNtI0dS4iune4H0B27Jrnv5cB3GURzJ1892r1K8rRwXvRp6DSspblFWiRV6tVZ/VLgSZ0xzDPvtVqYTKZYDgcOrcNF4/hcOjqzcjFhYuNLKYmm5yXg7XlCpzLIv4SzdNXlKPHrtk7xpg/MsZcMMZ8TRw7YYx52hjzUv59LT9ujDG/b4w5a4x5wRjzFvGZh/LzXzLGPHQ9g6PgcOMVxb1c92ZeoFQ2IR+NRkiSBP1+H5cvX8arr76KS5cuYTAYFHbSyraKPNZoNAquofJ9ygXKZCCX718PB70osBy1jHNwIeN7HFf+d75jUfOq+MUi/88q/nM9KZv/DcDbS8ceAfCMtfaNAJ7JfwaAdwB4Y/71MIDHgNkiAeAjAN4K4H4AH+FCsRvS755lGUaj0TUim9/DiTMraZZTO4Gtdotsei43UEVRhGaz6frxGmNcuQaeWxb1Mjci+IeBfGoism+w7EWcLwARFjivijesYsH/ZxW/2dW9Y639v8aYu0qH3wXgX+WvPwPg/wD49/nxP7Yz0/CvjTEdY8xt+blPW2svA4Ax5mnMFpI/3e3+0sVCy5659I1Go9BBi1206vV6IW9exgGY3cNUTT49xHGMOI4Lfnhm6LCmvuzeBaCQoinHOudv6N0CIEtVk3L9IMY78r/3pUXOq+INHSz4/6ziNzfq07/VWvtq/vr7AG7NX78ewDlx3nfzY9sd35GyxSwzYyjy9Xrd5evX6/VrKl7yfPk5mcrZbDadC4f9dZnOSRHnjmDuFZA7bOelY857EvBR+Ochxy7dZwBG4rSbmlfFKwIs8P+s4j83Hci11lqzwMYJxpiHMXvMLLhnpJuGwjsajQrVMNM0LXTGklDo6bIA4Grly1RO2V7RWoter4fBYFDo2iPHVk7ZJMseAF30AiXnVTla6NwuFzdahuG1/BEQ+fcL+fHzAO4Q592eH9vu+DVYaz9lrb3PWnsfrU7m6ZeLmDE/Px9H4bvEiN2xvB7dG+Px2LVNHI/HCIIAYRg6lw777nKx2ClQO8/iX2akmwuAXE1val73ZbDKjZJhgf9n922UysK4UdH/PABG8x8C8KQ4/st5Fs8ZAFdzN9BTAN5mjFnLg0Fvy4/tihRXBhkp9tK9Qstblmkg5R29Uvgnkwnq9fo1ZRpkyQV5L167HCiWTyK71dX3Fbmwyt3NeSG6k4ucV8UbNrDg/7OK3+zq3jHG/ClmQZ1bjDHfxSyi/zEATxhjPgDgOwB+IT/9iwAeBHAWwADArwCAtfayMea3ATyXn/dRBoh2ufe2fnNuvqIfXj4RkHJQln57eZxWfbPZRJqmrppnt9t1i4vMcpH1eeQThOx9uwyuHVm3iEFtuqvosqKrLP/9UyxoXhWvuArgZejcHhu8boxeq9Vsq9VyzU2ALVfN2toajJkVZNvY2HAWKd0yQNF/T6GngFOsu90uADjhbzabrixDt9t1qYtyh60MBjOrp1xRU+4TKJdV3s4FddjIfwsy/bVer+PKlSvaGP1o4n1jdOXGsEehMTpdKEyzbLVaLs1QlkiW2T3lJwQZfJUpmNydO5lMXJesKIpcTR6eJ+8DbAWVt3vCWEZ8NgQURbk5vBZ9CjwAV165Uqmg2Wy6ujdBEKBWqzmrntk48zZvyetOJhOXgslFgs3PuTOX7g3W4SkLPV+Xjy2Li2cnyiUlFEU5Gngv+lEUIQiCgsXd6XTQ7Xad4LPRCZuecHEAitU15S7T0WjkFgW2YWTNHf5Ma79SqczN4rneujrLLprLvoApirKF16IPbAlOs9m8xoqnK6bRaCBNU+fT50IxHA5d6QbZ0YrXlcXc6vU6Wq2WE3NuzmKtHwaO+Xlp+ZefIqR75yhY/YqiHB28F31ZR4eVL3u9nuthSyFmsDfLMpeZQsu8vDvXmFmP3UajgSiKXMZOp9NBv98vlFumeLOhStl3z+9MJQX29hSgKIpykHgv+vTry/RNVoCM4xhhGCJNUyRJ4qx2mTEjUyxrtZp7EuC1K5UKNjc3XYC30Wg4VxKrc9KnL8cku3ZR2Of5wZfVytfFSlGOJt6LvrSmKfxBEBSscXkOA74M7tJPT988xWw4HAKAK6XMHbnVahVRFLmibnQNyUVHFlorN2BXd46iKD7jtejLkgcUWGbcADNBZsljAC4jJ4oi53Lhk0K5YTqt9Fqt5hqpMFtHNl2p1+suiMuNXXQZlf32fLKQmTyHwc0+ZZR3QSuKcnTwWvQJBbxc3oDWuKyiGYahc/nM20k7nU4LVr10z8Rx7O4HwD0lyIYjcgGRIs9xyRr9exHMReX2zwsq3yj6xKIoRw+vRV9a+HKzVTlAyxLJPDdJEgRB4BaL0WjkgrVSpGUmTq1Ww+bmptsDQAFnxlCSJEiSZEdB38nCL/v+D8qCvhnh32kHsaIoy8mNFlw7EOaJFV00wJa/fXV1Fa973evcTlpSdgmxvy176DJASwufrRVlPR120up0Ouh0OoUevfPaIwLFgmvyvWW0nJdxzIqibI/Xog8URYeWOt0ztVoNjUYDKysrmE6n7jswK6TGc8vBWqZWMuWTu3vr9TrCMHRPEiytPJlMkCSJu6fc1CXLPpd94ctqIZeD54qiHB28du8AKKRGUoQYVF1ZWXEplhR5bqhicDYIAmRZVnC9sLQDMBP+ZrOJJEnQaDTchi+mhVL8syxDv993n+M4WPtHPoGUSzQcFIsWaBV8RTl6eC/6k8kEaZq6n4MgQKfTQRzHmE6nSNMUxhiEYYh+v4+VlRWsr68jSRKsra05C10GbZm1E8cxOp0OkiRxufvM/KlUKq5UAy1+jqe8F0B+zfPrH7aL5Eb9+oc9bkVRFo/Xoi/979ZaxHGMkydPolqtusAqg7EUdmb0sKgaa/IAKOTwc6evtbM2i6dPn8bly5fdzlvW6mfgNwxD5++XAeEsy66JGcj35aauZUEDuIpydPHep0/habfbrob+YDBwvnYKLWvq9/t9NJtNVKtVDIfDgutF1s+hMCdJgtFo5Fw3o9EIo9EIWZY5wWYdnlqtxi5SLjtIbs6aV3FzmTksN5WiKPuH15Y+yyqwkUq/37+mwTmzcLhLltk4V69edcHZ4XDoRL/RaDifPhcGXiMIAgwGA/deuU5/EATuSQKAy+EnMqOHcYS9uFZ8DJyq4CvK0cJr0a9UKoUsnPF4XOjoRKFnGiV985ubm2i329jY2HD1dOr1eqG0MhcE+SQgK3Dy3CiKnDuJrqZ5/ny5QxeAe6oAlrc0wzJnICmKMh+v3Tuyrg5r5TD3vlqtIgxDdy7TLNnukE1RWq2W8+3LhaPVagHY6nXLCp7MsafbhwFc7s6N4xitVsvFBeQGsvJu2GUXzGVcqBRF2RmvRV+WOqAws9gaFwQGZvk+SzEMBgPnvpH59iynLK1/WRaZ96ULKE1TpGlaeMpg8TZ+Zl6OPoBrNorxXPl9GVj2xUtRlC28Fn1g5iap1WoFS9wY40SYm6W4IFSrVcRxjEaj4TJ4WF8fALIsQ7vddsLNDJw4jp2Is8Y+UNwQxhRObgQDisIu6+7IYO5eAru+1cpZpsVJUZTd8dqnD1zbiQqAE17W0+ExCbNrGMBl8xUAaLVauOOOOwDMFgE+TbAxC614pm6Wg8dcaJjLX662KYO/HCvgZ6B2J5ZprIqiXB/eW/rMg2epBLp6uJmK1j5LMtDqp3uGvn3m0jMV8+zZs6jX685PPxqNEAQBptMpsixzbiEuBsDM6mdqZ5IkiOPY3UvGGmSBOFmf57BQa11RFOK16MuGKfTjA1siJi1vwgbndAHRtcPvURRhMpmg1+vh4sWLbgFpNBru2sy5p2uJPn+WZhiNRhgMBojj2Ll0dgrklouu7acIL/LaulgoytHDe9Hnzllgy63DNobcXDUYDDAejzGZTJBlGQaDAXq9XiFvnjGA06dPO/Fmume32wUw2wDGpwKKtCyoJnfw1ut19Ho9xHF8TdE1Lk7lKpxlF9ROZZgVRVH2A699+nTHyEAsa+k0m023eQuAc9VwAxUze+iTr9VqrpSCTMPkwtDv992+AGAWzOV9ae2naVoQdLp+2u222zjGhYrjB4qZOwfh5inX7lcURSFeW/oAXFCUpRHY71a2QWRJZJZIkBu2KNKnTp1Cu912fvpWq+VcPXQTjcdj9Ho9l5YZBIHz01P4eV1Zspk7fTkmadWXC7JtV5Rt0SzqHhrMVZSjhfeiD6AQEJV17ynCjUbDBXNZLqFWq6HZbKLRaCAMQ9coHZjV2j916hSSJHGuGmbkyLaM1WoV7Xa74N7hcWCr8xZLONDNIzt8lV085aCuWuKKohwk3rt3KOy0sGWWTLVadTn1fJ/lD5iGCWz1vuVnm80mKpUKsixzZZIZ/KVrJMsyFyNguWXW8JHuE7mpa3V1FRcvXgSAQpkGuoJ4r3llG+b97odhZS9bWqmiKHtjV0vfGHOHMeZZY8zXjTEvGmN+PT9+whjztDHmpfz7Wn7cGGN+3xhz1hjzgjHmLeJaD+Xnv2SMeWi3e0thpL+dtXAo+CyJQPdLGIaFICwzc2TRtFOnTrk6/OVUS8YRZP0duWmLTwWy126lUnG7dtmIpRy43c7NAxyOtc+MJjZ8l3WHWLaCsRERm1jIvCp+ofN6vLge984YwL+z1t4N4AyAXzPG3A3gEQDPWGvfCOCZ/GcAeAeAN+ZfDwN4DJgtEgA+AuCtAO4H8BEuFNvBrBvunmV6ZRzHbgGQzdEpxrSs6XIB4NxBp0+fRr/fLzRmkfeTzdK5L4Cll7kI8TiwFdBlQxcuDNu5dMrfDxOmt8pFjJY++w2LqqKrWNC8Kl6h83rM2FX0rbWvWmufz19vAvgGgNcDeBeAz+SnfQbAu/PX7wLwx3bGXwPoGGNuA/AzAJ621l621l4B8DSAt+90b2mx07JnQxMGUfkEEASBK8MQRZETX7pUxuMx4jjGZDJxG68Y1JXBWgaDeX++X6lUMBgM3KIg/fccizwu3VHbLQCH6c+ft49Aij4Atz8h/7mDBc2r4hU6r8eMPQVyjTF3AbgXwJcB3GqtfTV/6/sAbs1fvx7AOfGx7+bHtju+8wArFfR6PZcdw9r6MkDKjBkATri5MPA9BnY3NzddTj9hXR3u1i3HCDgOnsv78AmE7hxrLZIk2XY37nZCC8xfAA5qUSiLvfwuxhBggfOqeIPO6zHjukXfGNMC8DkAv2Gt7cr37EwZFqJQxpiHjTFfMcZ8RQozxZ2VNFmWge8BM0Gmpc7SyaytE0URkiRxIsaFQ27eolXPuj38YlaQtPqZ98/3+J2vy+K+nfAfNvTty4WTLHKccl5v+mKKV+jcLhfXJfrGmAAzwf8Ta+2f54dfyx8DkX+/kB8/D+AO8fHb82PbHS9grf2UtfY+a+19soKm3FxFsZe1clgBE9jqpsXNVVLYx+Ox82HzfZOXe5BllmU6Zq/XczEAueGLmT3SNQLABZPp/pknnj64eBi05UImj5etfwAZFjSvi/49lJvipuYV0LldNq4ne8cA+DSAb1hr/4t46/MAGNF/CMCT4vgvmxlnAFzN3UBPAXibMWYtDwi9LT+28wCFVU1R5k5a2dyc/ncALtXSGOMKqqVpCmst6vW6y+KRXbMYiO12u+h2uy6Hn4sGRTBNU9RqNbf4sG4/nxSyLHOZQDLzaJ7gH2YWDwWfT1BybBwHF8icDSxwXhVv0Hk9ZlxPnv4/B/A+AF81xvxdfuw/APgYgCeMMR8A8B0Av5C/90UADwI4C2AA4FcAwFp72Rjz2wCey8/7qLX28k43pqXMnakx7DMAABOqSURBVLMAnB9dZuvk13clGOr1ekHQZAMUWrbM5+duXwZ7ZVYOM3M4DvrsWbKh0Wi4pwUuRFx8pJDydym7enh8J7Hfr7x5ac3L35nj4aLJdFcAVwG8jAXMq+IVOq/HDOPzjtA4ju2b3vQmrK2tuRLG9OVTVFkrhy4aCn3Z+h8MBs6lQ0FP0xT9ft+1YqQFD8DFDZirLzto8Rr1et2JI5Hxhm63654imAJKC5sLTnmz1zyBPyjfv3TpyA1w9Xodly9f/ttFPb4bY/z9R3f8WNi8Ajq3PmGtnSscXu/IBeDElQFHulcoTPRFM12T2TV033AXLV0uzN/nhqThcFhwvwRB4O5Bd4705/N+rPApyzvwyWM3q77s1vEdHwLOiqIsBq9FXwZfpaBTeNlIhefSMqWbB4BriM5zZGYNXT18QjDGuLLLw+HwGteH7KLFY/V6HQCca4i1/2khl3e1yvHK7/vlxrkZOKZlWZwURdkdrwuuMfBKAZduB1r2soyxXBz4VABsBXalMDO1MwgC5zpixyymdnKR4CYvKX6ytDKFXy4IPKfsuilb//N+Z1/wbRFSFOXm8Vr0ibSeZd0doJhvLwOTDLrKIKzMxZcWPhcINl6RTwO8HtM4Zb9cWaphXleveRU2iU+5+mV8HJOiKIvBa9E3xsjaL6hWq64OPnP0mTnDhSBN00IxNPbYlccowNzolWWZa5pO8eZCQ5ePbOQi4fikq0lW3pyT715YBHyx7OU4tnutKMry47Xoc3crAJdhY4zBD/zADwCAq6tPAQe28vr5Pkss8JjMogmCAOPx2GXZAChY7qz5I2MBMoBMS340GrknBj5RUNDlYlOm/ATgq8Cq5a8oRwfvA7kUwlqths3NTdxxxx0YDAYwZqsSZBAEaLVaToDpjgFmKZ3D4dB1t2K7RV7TGIPBYOBq6Ui3C1M/ga2grey0xeNcVGThNaAYCJVZPfPEfSer/zCCvOU4hKIoRwOvRR/YSoMcjUauC1a320WtVnP1ddI0xWAwKLha4jh2mTPM16efXXa+YpCWSOGWAi8DufOEkFa/fNLgIiObuiyLgKqLR1GOJl6LPjcyWWtdt6sLFy5gdXUVJ0+exOXLl9Hr9VxFTbp4Njc3XQvDlZUVVzuHFTBpmVcqFbTbbQAoZAHJ+wO4JkgrM3u4kMhNVtydyywhWbt/XjB33u/tw+Lgc7BZUZQbw2ufPrC1w5WlkTudDjqdDr7zne+4dEla2MzG4blJkuDixYsF0er3+66MMjtd0TUjhZ8+/nIQeLuuWPTj000zHo8LVT3LTwe+CPt2+Dw2RVFuHO9Fny6b8XiMtbU1rK6u4ty5c2g0GgDgCqWxuxV32qZp6lwuFy9eRBiGrlon6+0DW/X3uQO3XAMf2HrikHn4sr4ON22x/AIXFfmkIq8577VvIis3lKl7R1GODt6L/q23znqzdDod3Hrrrfj+97+PlZWVQkkEYCaeFH+6evi1srKCNE1Rr9ddxg79+dPpFI1Gw/nb6a6hYMtUTWutS80sB20nk4krBsfPcFG43ro6vgi/j2NSFGUxeO3TZyrmiRMncOedd+LcuXOI49j1xmVqJUsnxHHssm5qtRrSNEUYhkiSBL1eD6urqxiPx4Vdt3Eco9lsotfrFSx3it10OnWxApZNLpdjkBuyZEE3WWvneq1lH90+vo1HUZQbx2vRn06nCMMQa2trbjMWM3YAuDr7ANxOWublS7eK9MczBkBxHo1GiOMYwEzUy6UT6vU6oigq+Oil6EufPfP15cLArB1JOXVz3uJw2OIvx6iiryhHB69Fv16vo9/vu45ZcRy7fHlm7FBQWT9nOp1ic3MTg8HAddTiRqwkSdy1mYo5GAwQRRFqtZoTfem2oeDTbdPtbnWKlMXYpGjzGlx4ykixJ7s9DRyE+KrvXlGOPl6LPn32LK3QaDTQaDSQZRkuXbqE8XiMMAzRbDZdLj/97u122xVao9jLDB1pidMdlCRJoek6ACRJgjAMkaZpodLnPL83FwteW7ZjZAbQvGqb8phvVrVPpSIURbl5vBb9yWTiRP7kyZO4dOkSXnnllUL7RKZnXr161RVB4y7bKIpc4TVgJvrcnUuxz7LM+f6Hw2GhUBqx1iIMQxcApvBLEef1uNDwKWCey8ZnIZ2XSeTbQqQoyo3jdfaOFMfvfe97OH/+vPOdJ0mCLMvQarWwubnpUjtpqbMvLgCX7cO6O7JWD90w3AvABUEiN3eRckqjrMsjBb+M+sgVRTlMvBZ9uckpTdNCr1ymRLZaLZe+yVx65snLln/NZtO5WVqtVqH8Aj9P0ee1pRXPLlm8pkzDLJ8vKQdDy5a/PE8XA0VR9huvRb9SqWB1dRVJkqDVaiGOY2RZ5rJtuAmL+fYsgQDA+fur1SouXrzo6uFEUeQyeObl+tO1wyCsLK7G60oo+NKtM8+lU14Qdvv5MPE5xqAoys3htehPp1Osr6+jXq9jY2MDvV4PlUoF3W7XFTMbDoeFNE5a7Nyxy3aJ3W7XpXeyEieFmm0TWYoZKIo7/fW8/naplbIGT/n34HX4GfmdHJbwzwsu65OHohxNvA7kMguGTcgnk4n73uv10G63XRG1EydOYDAYuIYqg8EAANxGqV6vh0aj4YSdnbho0WdZVqiESTFnOWX6/QkXBdmJi08GUtTlorBdBo/vyKchRVGWG6//N1erVfzgD/4gNjc3C7nz3DCVZRnOnTuHEydOoNPpFEomNxoNrKysuDo7FHg2XaH/njX0Wb6h3HtXWvHyO5G9csv1eeTnt/P3LwPLtkgpirI9Xot+pVLBcDh0G60YuG02m66AmjEGL7zwAjqdDobDIfr9Pnq9HobDoUvtjOMY7XbbZfBUq1VUKhXXdpENVriTl0hRptjTsmcwl58HUNgFzM/vVm/HJ7ZLKfV1vIqi7B2vRZ+7aynQLHZGS50C9fLLL2NlZcVl0LBePrNuuIOXGT5cDKSLJgiCwo5cirtsgSjHJYO+FEjp6pGCL88ru3x8Yrv8fLX0FeXo4LXoU2gp9rLtIEU9CAK3I5dVNFdXV10JBgZo5edlyiWtcwaAKfTc6EWBpqXPXbn8jPTnX0+55GUpV7xTZVBFUZYXr0V/MplgfX3dpWZSjKvVKsIwRBiGiKIIzWYTm5ubWF1dRavVci4XZvsEQeAKtgVBgH6/74S62WwWFhBen756nicbpwNbYjjPh88FQf5cxjcrel4Gz3bvKYqyvHidvUP3DBuhZ1nm/Pisd1+tVnHixAlcuXIFp0+fxmuvvQYAaLfbWF9fR6vVKmzakgXUuCBQyFmDB4BbADgOYEv8WM2Tx5jSKZ9Eyki3Ttlffpiiej0bxDR7R1GODt7/b2YbQmDLPSMbnYxGIwRBgEuXLqHRaKDX6yHLMjQaDbRaLZflQ+FvNpsAZkLGVomyCxbhAiA7avFz8jspi33Zf79dRU0pur6kc+4WiFYUZXnZVfSNMZEx5m+MMX9vjHnRGPOf8uNvMMZ82Rhz1hjz340x9fx4mP98Nn//LnGtD+fHv2mM+ZnrGeDKyoorq8zyxyx6VqlU8LrXvQ4XLlzA+vp6Ibun1+u5xuRhGLp0zUaj4erzMMDKXHxpwbP9IZ8GgK3+uBKZuy/r7wBbewTm/E0PPauHwWkZv+Bx1hkaDodyMTOLnFfFG3RejxnXY+mnAB6w1t4D4M0A3m6MOQPgdwD8nrX2nwK4AuAD+fkfAHAlP/57+XkwxtwN4BcB/AiAtwP4r8aYonldgn52mXXTaDQQx7Hra8uSyAzcAnDZOxTzOI7RarWc4HMjFtsqrq6uuuweWaOf92TDdWDLMmfgl8fo3im7a2RmzzyLX3LQVj6fZjh2uszkjmZ2JQNwCxY0r4pX6LweM3YVfTujl/8Y5F8WwAMA/kd+/DMA3p2/flf+M/L3/7WZqdu7APyZtTa11r4C4CyA+3e6N0WRTU4mkwk6nQ5WVlbQaDRw4sQJVKtVrK6uuqyawWCAwWCAarWKkydPOnGOosjV7KGwm7zGDksyULyNKXa7opjLWj1yYZDZOvOs9XmtFcuvD5qdxsrfMQgCt5AC6GBB86p4hc7rMeO6fPrGmKox5u8AXADwNIB/ALBhrWXy+ncBvD5//XoA5wAgf/8qgJPy+JzPzKVarWJlZQUrKytOvK9cuYKNjQ0YY9But12Al2WU19bWnN+fxdhokbMEA7N7+ATBjB369WVKJxcEHieyxk65rn75uNzwxc9KpP//IKGLZ57bCkDhqQdAHQuaV8UrdF6PGdcl+tbaibX2zQBux2y1/2f7NSBjzMPGmK8YY76Spin6/b5LteQmqyzL8L3vfc81UJHdsOj2GY1GrrWhzdslBkGA4XDoeuVSaIfDIQA4/z0FvSzswFb3LQaS5xVg47XkhqzS7+hFWQY+vZTLQu/kgrqJe7l5XcgFFW/QuV0u9pS9Y63dAPAsgJ8A0DHGMIp5O4Dz+evzAO4AgPz9VQCX5PE5n5H3+JS19j5r7X0UpH6/73zLrIgJAK+88go6nQ7a7barj0//e5qmqFQq7gmA9XfY9pDX6Ha7rqG5tOr5s7TC5ROADAS7P6bYwMVryOwd6VKZtxgcVukDjqu8cJXiECMsaF738VdR9s5NzSugc7tsXE/2ziljTCd/3QDw0wC+gZn4/9v8tIcAPJm//nz+M/L3/7edqcjnAfxint3zBgBvBPA3O917PB679oZspMKNVJVKBZcuXYK1FqdOnXL++jAMYa11gVzuxuWTArN6mKUCFEWOriE+YVAMmdpZRgp1OVhb3s1btqDlz9I9dBDIDWflncc8zn0RORtY0LwqXqHzesy4ns1ZtwH4TB65rwB4wlr7BWPM1wH8mTHmUQD/D8Cn8/M/DeBxY8xZAJcxywCAtfZFY8wTAL4OYAzg16y1E+yAMQbD4RArKyuFssUyc+aFF17AmTNnXMom3+MmriAIkCSJ67oVxzGGwyGSJHEF1pi1MxqNsLGx4UoxsDIn0zeBa9M2t+uFK0WVzLPsDzOYK2MN8sklyzL3+7MKKYCLAE4uYl4Vr9B5PWYYHzYDbUcURfaee+5BvV5Ho9HAeDx2dXiiKHIi9eCDD+L8+fN405vehCAI8I//+I+FMsppmjqx/8Y3voHhcIhLly65tM1+v48oijAcDnHhwgW3E5hZP7IcgyyhTNHkkwO/y01krMUPoLBwyYwh6UbigjLP4l/0U4BcpGjly+9MiW00Gjh//vzfLurx3Rjj7z+648fC5hXQufUJa+1cwfC+DIN8zRx75uaTK1euoNPpIAxDTKdTNBoNpGnqrG9aq1euXHEtDSlo0+kUURQhTVOEYYi1tTX0ej3XVpEwHsDX0vUj3ysHb8vWv4wVyPd9RGYwKYpyNPC6DIO0nplxQ0GniI7HY1dvB4BzT3CzFneb8jVz85nKGUURVldXXfomC7lxsxYAF0cgdAnRWpeWPOMCO1nsPgr9dk8W2+XzK4qynHgv+hR8flUqFaysrBSEqNvturr5rMYZBIH7PDdVZVmGwWDgBJ6pnHEcO6ueFnuWZddY42XxL5dZKAdxZWkD+fts93v6gMxOKh9TFGX58Vr0ga0aONPpFFmWod/vA5j59GnpX716FYPBwPn7adGz+QpdQ/ThA1sCHoYhLly4UOhty2Bt2RcvYe0fvs/CbBxzecOVb0FciXwqkQuXXMAURTkaeB3INcZsAvjmYY9jDrdglvXgG/s5rjuttacWcSFjzDqAPo7f3/Bm2K9xLWxeAf0/ewMc+Lx6HcgF8E0fN3wYY76i47pxrLWnfB2rjuum0f+ze+AwxuW9e0dRFEVZHCr6iqIoxwjfRf9Thz2AbdBx3Ty+jlXHdXP4Ok4dV47XgVxFURRlsfhu6SuKoigLxFvRN8a83cx6c541xjxyAPf7I2PMBWPM18SxE8aYp40xL+Xf1/Ljxhjz+/nYXjDGvEV85qH8/JeMMQ/Nu9cex3WHMeZZY8zXzaxH8a/7MrYb/H10XqHzuoD76bzeKHITkS9fAKqYdef6J5h19vl7AHfv8z3/JYC3APiaOPa7AB7JXz8C4Hfy1w8C+J8ADIAzAL6cHz8B4OX8+1r+eu0mx3UbgLfkr9sAvgXgbh/GpvOq86rzunzzemD/MPb4h/sJAE+Jnz8M4MMHcN+7Sv+IvgngNjGZ38xffxLAe8rnAXgPgE+K44XzFjTGJzHraeDd2HRedV51Xv2fV1/dO77057zVWvtq/vr7AG7NX283vn0dtzHmLgD3Aviyb2O7TnwYA+DZ307ndWF49bfzdV59FX3vsLPl9tBSnYwxLQCfA/Ab1tqufO+wx7bMHPbfTud1fzjsv53P8+qr6F93f8595jVjzG0AkH+/kB/fbnz7Mm5jTIDZP6A/sdb+uU9j2yM+jAHw5G+n87pwvPjbeT+vB+n724MfrIZZ4OIN2AoM/cgB3PcuFH2E/xnF4Mvv5q//DYrBl7/Jj58A8ApmgZe1/PWJmxyTAfDHAD5eOn7oY9N51XnVeV2+eT2wfxg38Md7ELPI9z8A+I8HcL8/BfAqgAwz/9kHAJwE8AyAlwD8L/7R8wn6RD62rwK4T1zn/QDO5l+/soBx/QvMHgVfAPB3+deDPoxN51XnVed1+eZVd+QqiqIcI3z16SuKoij7gIq+oijKMUJFX1EU5Rihoq8oinKMUNFXFEU5RqjoK4qiHCNU9BVFUY4RKvqKoijHiP8P4ssC3b8EmdUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data transformer"
      ],
      "metadata": {
        "id": "K8GPt23nzvQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ToTensor(object):\n",
        "    def __call__(self,data):\n",
        "        label, input = data['label'], data['input']\n",
        "\n",
        "        input = input.astype(np.float32)\n",
        "        label = label.astype(np.float32)\n",
        "        \n",
        "        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n",
        "\n",
        "        return data\n",
        "\n",
        "class Normalization(object):\n",
        "    def __init__(self, mean = 0.5, std = 0.5):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, data):\n",
        "        label, input = data['label'], data['input']\n",
        "\n",
        "        input = (input - self.mean) / self.std\n",
        "        data = {'label': label, 'input' : input}\n",
        "\n",
        "        return data\n",
        "\n",
        "class RandomFlip(object):\n",
        "    def __call__(self, data):\n",
        "        label, input = data['label'], data['input']\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            label = np.fliplr(label)\n",
        "            input = np.fliplr(input)\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            label = np.flipud(label)\n",
        "            input = np.flipud(input)\n",
        "\n",
        "        data = {'label':label, 'input':input}\n",
        "\n",
        "        return data\n",
        "\n",
        "class Rescale(object):\n",
        "    def __call__(self, data):\n",
        "        label, input = data['label'], data['input']\n",
        "\n",
        "        # h, w = label.shape[1], label.shape[2]\n",
        "        \n",
        "        # small_val = h if h <= w else w\n",
        "        \n",
        "        # exp = 0\n",
        "\n",
        "        # while small_val>=1:\n",
        "        #     small_val = small_val / 2\n",
        "        #     exp += 1\n",
        "        \n",
        "        # resize = 2**(exp-1)\n",
        "\n",
        "        input = cv2.resize(input.transpose(1,2,0), (1024,1024)) \n",
        "        label = cv2.resize(label.transpose(1,2,0), (1024,1024))\n",
        "  \n",
        "        data = {'label':label.transpose(2,0,1), 'input':input}\n",
        "\n",
        "        return data\n",
        "\n",
        "class Rescale_testset(object):\n",
        "    def __call__(self, data):\n",
        "        label, input = data['label'], data['input']\n",
        "\n",
        "        # h, w = label.shape[1], label.shape[2]\n",
        "        \n",
        "        # small_val = h if h <= w else w\n",
        "        \n",
        "        # exp = 0\n",
        "\n",
        "        # while small_val>=1:\n",
        "        #     small_val = small_val / 2\n",
        "        #     exp += 1\n",
        "        \n",
        "        # resize = 2**(exp-1)\n",
        "\n",
        "        input = cv2.resize(input.transpose(1,2,0), (1024,1024)) \n",
        "        label = cv2.resize(label.transpose(1,2,0), (1024,1024))\n",
        "  \n",
        "        data = {'label':label, 'input':input}\n",
        "\n",
        "        return data        "
      ],
      "metadata": {
        "id": "O4p68T-tzyD6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data transform example"
      ],
      "metadata": {
        "id": "QIeh69nzQKtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compose transform\n",
        "transform_example_trainset = transforms.Compose([\n",
        "    # transforms.Grayscale(num_output_channels=1),\n",
        "    # transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # transforms.RandomRotation((-72,72)),\n",
        "    # transforms.RandomAffine(0,translate=(0.1,0.1)),\n",
        "    Normalization(mean=0.5, std=0.5),\n",
        "    RandomFlip(),\n",
        "    # Rescale(),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "transform_exmaple_testset = transforms.Compose([\n",
        "    # transforms.Grayscale(num_output_channels=1),\n",
        "    # transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # transforms.RandomRotation((-72,72)),\n",
        "    # transforms.RandomAffine(0,translate=(0.1,0.1)),\n",
        "    Normalization(),\n",
        "    # RandomFlip(),\n",
        "    ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "IBtPeOt47T13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_db = Dataset(data_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/train', transform=transform_example_trainset)\n",
        "data = dataset_db.__getitem__(1)\n",
        "\n",
        "input = data['input']\n",
        "label = data['label']\n",
        "\n",
        "print(\"=========input==========\")\n",
        "print(input.shape)\n",
        "print(type(input))\n",
        "print(input)\n",
        "print(\"=========label==========\")\n",
        "print(label.shape)\n",
        "print(type(label))\n",
        "print(label)\n",
        "print(\"===================\")\n",
        "plt.subplot(121)\n",
        "plt.imshow(input.squeeze(), cmap='gray')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(label[2], cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7S5IIZk7WjS",
        "outputId": "ffac2700-9350-4c2e-eb7d-ef79a3571de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========input==========\n",
            "torch.Size([2048, 2048])\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[23.1549, 23.0506, 22.4809,  ..., 28.1161, 28.6763, 29.1965],\n",
            "        [23.3534, 23.0108, 22.4393,  ..., 28.3407, 28.5373, 29.3542],\n",
            "        [24.0697, 23.0865, 22.4852,  ..., 28.4495, 28.5158, 29.1485],\n",
            "        ...,\n",
            "        [32.5115, 31.9423, 31.3404,  ..., 29.5181, 29.3880, 29.4674],\n",
            "        [31.5061, 30.9562, 30.3015,  ..., 30.1901, 30.0526, 29.9397],\n",
            "        [29.9773, 29.9616, 29.4195,  ..., 30.3305, 30.5096, 30.3370]])\n",
            "=========label==========\n",
            "torch.Size([6, 2048, 2048])\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
            "===================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7efea8c37cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 182
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "5f57qvSqINlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "batch_size = 2\n",
        "num_epoch = 30\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "q5d6QOUr347b"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "class trainer:\n",
        "\n",
        "    def __init__(self, model, train_loader, opt, epoch_size = num_epoch, learning_rate = lr, use_cuda =True):\n",
        "        \n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        #Use GPU, model to GPU\n",
        "        if self.use_cuda:\n",
        "            self.net = model.cuda()\n",
        "\n",
        "        else:\n",
        "            self.net = model\n",
        "    \n",
        "        self.train_loader = train_loader\n",
        "        self.optimizer = opt\n",
        "        self.epoch_size = epoch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "        # self.criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "        self.history = {'train_acc':[],'val_acc':[],'test_acc':[],'train_loss':[],'val_loss':[],'test_loss':[]}\n",
        "\n",
        "    def train(self, validation_loader = None):\n",
        "\n",
        "        #Choose optimizer\n",
        "        if self.optimizer == \"SGD\":\n",
        "          optimizer = torch.optim.SGD(self.net.parameters(), lr = self.learning_rate)\n",
        "        elif self.optimizer == \"Adam\":\n",
        "          optimizer = torch.optim.Adam(self.net.parameters(), lr = self.learning_rate)\n",
        "        elif self.optimizer == \"Adagrad\":\n",
        "          optimizer = torch.optim.Adagrad(self.net.parameters(), lr = self.learning_rate)\n",
        "        elif self.optimizer == \"RMSprop\":\n",
        "          optimizer = torch.optim.RMSprop(self.net.parameters(), lr = self.learning_rate)\n",
        "\n",
        "        for epoch in tqdm(range(self.epoch_size)):\n",
        "            \n",
        "            self.net.train()\n",
        "            loss_arr = []\n",
        "\n",
        "            for batch, data in enumerate(self.train_loader):\n",
        "                \n",
        "                #forward pass\n",
        "                if self.use_cuda:\n",
        "                    input = data['input'].to(device)\n",
        "                    label = data['label'].to(device)\n",
        "\n",
        "                #unsqueeze depends on batch size to match shape\n",
        "                if batch_size ==1:\n",
        "                    output = self.net(input)\n",
        "                else:\n",
        "                    output = self.net(input.unsqueeze(1))\n",
        "\n",
        "                #backward pass & loss\n",
        "                optimizer.zero_grad() #gradient initialization\n",
        "                loss = self.criterion(output, label) #output:prediction, label: answer tensor\n",
        "                loss.backward() #backpropagation\n",
        "                optimizer.step()\n",
        "                \n",
        "                #history\n",
        "                loss_arr += [loss.item()]\n",
        "                \n",
        "                print(\"Train: epoch %04d / %04d | batch %04d / %04d | loss %.4f\" % (epoch+1, self.epoch_size, batch, num_batch_train, np.mean(loss_arr)))\n",
        "                \n",
        "                #tensor board\n",
        "            #     print(label.shape)\n",
        "            #     label = fn_tonumpy(label)\n",
        "            #     input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "            #     output = fn_tonumpy(fn_class(output))\n",
        "\n",
        "            #     writer_train.add_image('label', label, num_batch_train * (epoch-1)+batch, dataformats ='NHWC')\n",
        "            #     writer_train.add_image('input', input, num_batch_train * (epoch-1)+batch, dataformats ='NHWC')\n",
        "            #     writer_train.add_image('output', output, num_batch_train * (epoch-1)+batch, dataformats ='NHWC')\n",
        "            \n",
        "\n",
        "            # writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "            self.history['train_loss'].append(np.mean(loss_arr))\n",
        "\n",
        "            if validation_loader != None:\n",
        "                self.validate(validation_loader)\n",
        "\n",
        "            save(ckpt_dir = ckpt_dir, net = self.net, optim = optimizer, epoch = epoch)\n",
        "                ############################\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, data_loader):\n",
        "        \n",
        "        self.net.eval() #no weight update\n",
        "        val_loss_arr = []\n",
        "        \n",
        "        num_correct = 0\n",
        "        num_pixels = 0\n",
        "        \n",
        "\n",
        "        for batch, data in enumerate(data_loader):\n",
        "            #forward pass\n",
        "            if self.use_cuda:\n",
        "                input = data['input'].to(device)\n",
        "                label = data['label'].to(device)\n",
        "            \n",
        "            #unsqueeze depends on batch size to match shape\n",
        "            if batch_size ==1:\n",
        "                output = self.net(input)\n",
        "            else:\n",
        "                output = self.net(input.unsqueeze(1))\n",
        "            \n",
        "            #loss function\n",
        "            val_loss = self.criterion(output, label)\n",
        "            val_loss_arr += [val_loss.item()]\n",
        "\n",
        "            print(\"Val loss %.4f\" % (np.mean(val_loss_arr)))\n",
        "            \n",
        "            #dice loss\n",
        "            for j in range(output.shape[0]):\n",
        "                preds = torch.clone(output[j]) #(N, 6, 1024,1024)\n",
        "\n",
        "                for i in range(preds.shape[0]):\n",
        "                    preds[i] = (preds[i]-torch.mean(preds[i]))/torch.std(preds[i])\n",
        "                    preds[i] = torch.sigmoid(preds[i])\n",
        "                    preds[i] = fn_classifier(preds[i])\n",
        "            \n",
        "            num_correct += (preds == label).sum()\n",
        "            num_pixels += torch.numel(preds)\n",
        "            \n",
        "            dice_score = (2*(preds*label).sum()) / ((preds+label).sum() + 1e-8)\n",
        "\n",
        "            print(f\"Dice score : {dice_score}\")\n",
        "\n",
        "        #     #tensor board\n",
        "        #     label = fn_tonumpy(label)\n",
        "        #     input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "        #     output = fn_tonumpy(fn_class(output))\n",
        "\n",
        "        #     writer_val.add_image('label', label, num_batch_train * (epoch-1)+batch, dataformats ='NHWC')\n",
        "        #     writer_val.add_image('input', input, num_batch_train * (epoch-1)+batch, dataformats ='NHWC')\n",
        "        #     writer_val.add_image('output', output, num_batch_train * (epoch-1)+batch, dataformats ='NHWC')\n",
        "\n",
        "        # writer_val.add_scalar('loss', np.mean(val_loss_arr), epoch)\n",
        "\n",
        "        if  data_loader != self.train_loader: \n",
        "            self.history['val_loss'].append(np.mean(val_loss_arr))\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test(self, data_loader):\n",
        "        \n",
        "        test_img_arr = os.listdir(os.path.join(test_dir,'img'))\n",
        "        test_img_arr.sort()\n",
        "            \n",
        "        self.net.eval() #no weight update\n",
        "        test_loss_arr = []\n",
        "\n",
        "        for batch, data in enumerate(data_loader):\n",
        "            #forward pass\n",
        "            if self.use_cuda:\n",
        "                input = data['input'].to(device)\n",
        "                # label = data['label'].to(device)\n",
        "\n",
        "            #unsqueeze depends on batch size to match shape\n",
        "            if batch_size ==1:\n",
        "                output = self.net(input)\n",
        "            else:\n",
        "                output = self.net(input.unsqueeze(1))\n",
        "            \n",
        "            #######################################\n",
        "            ###there is no backward pass in test###\n",
        "            #######################################\n",
        "\n",
        "            #loss function\n",
        "            # test_loss = self.criterion(output, label)\n",
        "\n",
        "            # test_loss_arr += [test_loss.item()]\n",
        "\n",
        "            #print test loss\n",
        "            # print(\"Test:  batch %04d / %04d | loss %.4f\" % (batch, num_batch_test, np.mean(test_loss_arr)))\n",
        "\n",
        "            #tensor board\n",
        "            # label = fn_tonumpy(label)\n",
        "\n",
        "\n",
        "            #save output image and numpy \n",
        "            \n",
        "            input = fn_denorm(input, mean=0.5, std=0.5)\n",
        "            input = fn_tonumpy(input)\n",
        "\n",
        "            # print(type(input)) #<class 'torch.Tensor'>\n",
        "            # print(type(output)) #<class 'torch.Tensor'>\n",
        "\n",
        "            # output = fn_tonumpy(fn_classifier(output))\n",
        "            # output = fn_tonumpy(output)\n",
        "\n",
        "            # print(f\"input: {type(input)} : {input.shape}\") #(4, 1024, 1024)\n",
        "            # print(f\"output: {type(output)} : {output.shape}\") #(4, 6, 1024, 1024)\n",
        "            print(1)\n",
        "            for j in range(output.shape[0]):\n",
        "                \n",
        "                id = num_batch_test * (batch -1)+j\n",
        "                print(2)\n",
        "                # plt.imsave(os.path.join(result_dir, 'png' ,'label_%04d.png' %id), label[j].squeeze(), cmap='gray')\n",
        "                plt.imsave(os.path.join(result_dir, 'png' ,'input_%04d.png' %id), input[j].squeeze(), cmap='gray')\n",
        "\n",
        "\n",
        "                print(3)\n",
        "                preds = output[j].clone()  # (6,1024,1024)\n",
        "                for i in range(preds.shape[0]):\n",
        "                    preds[i] = (preds[i]-torch.mean(preds[i]))/torch.std(preds[i])\n",
        "                    preds[i] = torch.sigmoid(preds[i])\n",
        "                    preds[i] = fn_classifier(preds[i])\n",
        "                \n",
        "                preds = fn_tonumpy(preds)\n",
        "                np.save(os.path.join(result_dir, 'numpy','output_%04d.npy' %id), preds.squeeze())\n",
        "                print(4)\n",
        "\n",
        "                # output_ = fn_tonumpy(fn_classifier(output))\n",
        "                preds_torch = torch.Tensor(preds)\n",
        "                print(5)\n",
        "                for k in range(6):\n",
        "                    plt.imsave(os.path.join(result_dir, 'png' ,'output_%04d_%04d.png' %(id, k)), preds[k].squeeze(), cmap='gray')\n",
        "\n",
        "                # print(f\"input shape {input[j].squeeze().shape}\") #(1024, 1024)\n",
        "                # print(f\"output shape {output[j].squeeze().shape}\") #(6, 1024, 1024)\n",
        "                \n",
        "                # np.save(os.path.join(result_dir, 'numpy','label_%04d.npy' %id), label[j].squeeze())\n",
        "                \n",
        "                # np.save(os.path.join(result_dir, 'numpy','input_%04d.npy' %id), input[j].squeeze())\n",
        "                # np.save(os.path.join(result_dir, 'numpy','output_%04d.npy' %id), output[j].squeeze())\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "                if test_img_arr:\n",
        "                    print(test_img_arr)\n",
        "                    test_input_abs_path = os.path.join(test_dir, 'img', test_img_arr[0])\n",
        "                    images = sitk.ReadImage(test_input_abs_path)\n",
        "                    test_input_shape =sitk.GetArrayFromImage(images).shape #(1,3232,2588)\n",
        "\n",
        "                    print(test_input_shape)\n",
        "                    print(output.shape) #(2,6,1024,1024)\n",
        "                    print(output[j].shape) #(6,1024,1024)\n",
        "                    # 7 채널로..\n",
        "                    preds_tensor = torch.cat((preds_torch, preds_torch[5].unsqueeze(0)))\n",
        "                    preds_np = cv2.resize(fn_tonumpy(preds_tensor).transpose(1,2,0), (test_input_shape[2],test_input_shape[1])) \n",
        "                    \n",
        "                    print(np.amax(preds_np))\n",
        "                    #7채널\n",
        "                    # output_np = cv2.resize(fn_tonumpy(output[j]).transpose(1,2,0), (test_input_shape[2],test_input_shape[1])) \n",
        "                    \n",
        "                    print(preds_np.shape)\n",
        "\n",
        "                    \n",
        "                    #preds save\n",
        "                    np.save(os.path.join(board_dir, test_img_arr.pop(0)[:-4]), np.uint8(preds_np))\n",
        "\n",
        "                    #output save\n",
        "                    # np.save(os.path.join(board_dir, test_img_arr.pop(0)[:-4]), np.uint8(output_np))\n",
        "                \n",
        "                \n",
        "        self.history['test_loss'].append(np.mean(test_loss_arr))\n",
        "\n",
        "        # print(\"Average test: loss %.4f\" % (np.mean(test_loss_arr)))\n"
      ],
      "metadata": {
        "id": "XRrrIgui2sdf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data transform"
      ],
      "metadata": {
        "id": "sNkoc8ofPfO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compose transform\n",
        "transform_wholeset = transforms.Compose([\n",
        "    # transforms.Grayscale(num_output_channels=1),\n",
        "    # transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # transforms.RandomRotation((-72,72)),\n",
        "    # transforms.RandomAffine(0,translate=(0.1,0.1)),\n",
        "    Normalization(),\n",
        "    RandomFlip(),\n",
        "    Rescale(),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "transform_testset = transforms.Compose([\n",
        "    # transforms.Grayscale(num_output_channels=1),\n",
        "    # transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # transforms.RandomRotation((-72,72)),\n",
        "    # transforms.RandomAffine(0,translate=(0.1,0.1)),\n",
        "    Normalization(),\n",
        "    # RandomFlip(),\n",
        "    Rescale_testset(),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "#train, val dataset, dataloader\n",
        "whole_dataset = Dataset(data_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/train', transform = transform_wholeset)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(whole_dataset, [100, 20])\n",
        "test_dataset = Dataset(data_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/test' , transform = transform_testset)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 8)\n",
        "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True, num_workers = 8)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = 8)"
      ],
      "metadata": {
        "id": "ZH_ElWavs423"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(test_dataset))\n",
        "# print(test_dataset.lst_input[0])\n",
        "# print(test_dataset.lst_label)\n",
        "# # aaa = test_dataset.__getitem__(1)\n",
        "# # aaa['input']\n",
        "# print(len(test_loader.dataset))\n",
        "\n",
        "\n",
        "# for batch, data in enumerate(test_loader):\n",
        "#     print(data['input'].shape)\n",
        "\n",
        "#     print(data['input'])\n",
        "    \n",
        "#     print(data['input'].unsqueeze(1).shape)\n",
        "    \n",
        "#     print(data['input'].unsqueeze(1))\n",
        "\n",
        "#     break\n",
        "#     # #forward pass\n",
        "    # if self.use_cuda:\n",
        "    #     input = data['input'].to(device)\n",
        "    #     label = data['label'].to(device)\n",
        "\n",
        "    # output = self.net(input.unsqueeze(1))"
      ],
      "metadata": {
        "id": "UWDD_QKTvmmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before Train"
      ],
      "metadata": {
        "id": "tB4Ir8fqqwnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#other variables\n",
        "num_data_train = len(train_dataset)\n",
        "num_data_val = len(val_dataset)\n",
        "num_data_test = len(test_dataset)\n",
        "\n",
        "num_batch_train = np.ceil(num_data_train / batch_size)\n",
        "num_batch_val = np.ceil(num_data_val / batch_size)\n",
        "num_batch_test = np.ceil(num_data_test / batch_size)\n",
        "\n",
        "\n",
        "# 기타 function 설정\n",
        "# fn_tonumpy = lambda x : x.to('cpu').detach().numpy().transpose(0,2,3,1) # device 위에 올라간 텐서를 detach 한 뒤 numpy로 변환\n",
        "fn_tonumpy = lambda x : x.to('cpu').detach().numpy() # device 위에 올라간 텐서를 detach 한 뒤 numpy로 변환\n",
        "fn_denorm = lambda x, mean, std : (x * std) + mean \n",
        "fn_classifier = lambda x :  1.0 * (x > 0.65)  # threshold 0.5 기준으로 indicator function으로 classifier 구현\n",
        "\n",
        "# tensorboard, SummaryWriter\n",
        "writer_train = SummaryWriter(log_dir = os.path.join(log_dir, 'train'))\n",
        "writer_val = SummaryWriter(log_dir = os.path.join(log_dir, 'val'))\n",
        "\n",
        "#save network\n",
        "def save(ckpt_dir, net, optim, epoch):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    torch.save({'net':net.state_dict(),'optim':optim.state_dict()},'%s/model_epoch%d.pth'%(ckpt_dir,epoch))\n",
        "\n",
        "#load network\n",
        "def load(ckpt_dir, net, optim):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        epoch = 0\n",
        "        return net, optim, epoch\n",
        "\n",
        "    ckpt_lst = os.listdir(ckpt_dir)\n",
        "    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit,f))))\n",
        "\n",
        "    dict_model = torch.load('%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
        "\n",
        "    net.load_state_dict(dict_model['net'])\n",
        "    optim.load_state_dict(dict_model['optim'])\n",
        "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
        "\n",
        "    return net, optim, epoch\n"
      ],
      "metadata": {
        "id": "S4CL1gldqwHw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## example model summary"
      ],
      "metadata": {
        "id": "Cp72Zn3C2p_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_model = UNET()\n",
        "example_model.cuda()\n",
        "\n",
        "summary(example_model, input_size=(1,1024,1024))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cNQu3ef6w-J",
        "outputId": "8d223f0d-841d-43cf-fcb6-dacee56265b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1       [-1, 64, 1024, 1024]             576\n",
            "       BatchNorm2d-2       [-1, 64, 1024, 1024]             128\n",
            "              ReLU-3       [-1, 64, 1024, 1024]               0\n",
            "            Conv2d-4       [-1, 64, 1024, 1024]          36,864\n",
            "       BatchNorm2d-5       [-1, 64, 1024, 1024]             128\n",
            "              ReLU-6       [-1, 64, 1024, 1024]               0\n",
            "        DoubleConv-7       [-1, 64, 1024, 1024]               0\n",
            "         MaxPool2d-8         [-1, 64, 512, 512]               0\n",
            "            Conv2d-9        [-1, 128, 512, 512]          73,728\n",
            "      BatchNorm2d-10        [-1, 128, 512, 512]             256\n",
            "             ReLU-11        [-1, 128, 512, 512]               0\n",
            "           Conv2d-12        [-1, 128, 512, 512]         147,456\n",
            "      BatchNorm2d-13        [-1, 128, 512, 512]             256\n",
            "             ReLU-14        [-1, 128, 512, 512]               0\n",
            "       DoubleConv-15        [-1, 128, 512, 512]               0\n",
            "        MaxPool2d-16        [-1, 128, 256, 256]               0\n",
            "           Conv2d-17        [-1, 256, 256, 256]         294,912\n",
            "      BatchNorm2d-18        [-1, 256, 256, 256]             512\n",
            "             ReLU-19        [-1, 256, 256, 256]               0\n",
            "           Conv2d-20        [-1, 256, 256, 256]         589,824\n",
            "      BatchNorm2d-21        [-1, 256, 256, 256]             512\n",
            "             ReLU-22        [-1, 256, 256, 256]               0\n",
            "       DoubleConv-23        [-1, 256, 256, 256]               0\n",
            "        MaxPool2d-24        [-1, 256, 128, 128]               0\n",
            "           Conv2d-25        [-1, 512, 128, 128]       1,179,648\n",
            "      BatchNorm2d-26        [-1, 512, 128, 128]           1,024\n",
            "             ReLU-27        [-1, 512, 128, 128]               0\n",
            "           Conv2d-28        [-1, 512, 128, 128]       2,359,296\n",
            "      BatchNorm2d-29        [-1, 512, 128, 128]           1,024\n",
            "             ReLU-30        [-1, 512, 128, 128]               0\n",
            "       DoubleConv-31        [-1, 512, 128, 128]               0\n",
            "        MaxPool2d-32          [-1, 512, 64, 64]               0\n",
            "           Conv2d-33         [-1, 1024, 64, 64]       4,718,592\n",
            "      BatchNorm2d-34         [-1, 1024, 64, 64]           2,048\n",
            "             ReLU-35         [-1, 1024, 64, 64]               0\n",
            "           Conv2d-36         [-1, 1024, 64, 64]       9,437,184\n",
            "      BatchNorm2d-37         [-1, 1024, 64, 64]           2,048\n",
            "             ReLU-38         [-1, 1024, 64, 64]               0\n",
            "       DoubleConv-39         [-1, 1024, 64, 64]               0\n",
            "  ConvTranspose2d-40        [-1, 512, 128, 128]       2,097,664\n",
            "           Conv2d-41        [-1, 512, 128, 128]       4,718,592\n",
            "      BatchNorm2d-42        [-1, 512, 128, 128]           1,024\n",
            "             ReLU-43        [-1, 512, 128, 128]               0\n",
            "           Conv2d-44        [-1, 512, 128, 128]       2,359,296\n",
            "      BatchNorm2d-45        [-1, 512, 128, 128]           1,024\n",
            "             ReLU-46        [-1, 512, 128, 128]               0\n",
            "       DoubleConv-47        [-1, 512, 128, 128]               0\n",
            "  ConvTranspose2d-48        [-1, 256, 256, 256]         524,544\n",
            "           Conv2d-49        [-1, 256, 256, 256]       1,179,648\n",
            "      BatchNorm2d-50        [-1, 256, 256, 256]             512\n",
            "             ReLU-51        [-1, 256, 256, 256]               0\n",
            "           Conv2d-52        [-1, 256, 256, 256]         589,824\n",
            "      BatchNorm2d-53        [-1, 256, 256, 256]             512\n",
            "             ReLU-54        [-1, 256, 256, 256]               0\n",
            "       DoubleConv-55        [-1, 256, 256, 256]               0\n",
            "  ConvTranspose2d-56        [-1, 128, 512, 512]         131,200\n",
            "           Conv2d-57        [-1, 128, 512, 512]         294,912\n",
            "      BatchNorm2d-58        [-1, 128, 512, 512]             256\n",
            "             ReLU-59        [-1, 128, 512, 512]               0\n",
            "           Conv2d-60        [-1, 128, 512, 512]         147,456\n",
            "      BatchNorm2d-61        [-1, 128, 512, 512]             256\n",
            "             ReLU-62        [-1, 128, 512, 512]               0\n",
            "       DoubleConv-63        [-1, 128, 512, 512]               0\n",
            "  ConvTranspose2d-64       [-1, 64, 1024, 1024]          32,832\n",
            "           Conv2d-65       [-1, 64, 1024, 1024]          73,728\n",
            "      BatchNorm2d-66       [-1, 64, 1024, 1024]             128\n",
            "             ReLU-67       [-1, 64, 1024, 1024]               0\n",
            "           Conv2d-68       [-1, 64, 1024, 1024]          36,864\n",
            "      BatchNorm2d-69       [-1, 64, 1024, 1024]             128\n",
            "             ReLU-70       [-1, 64, 1024, 1024]               0\n",
            "       DoubleConv-71       [-1, 64, 1024, 1024]               0\n",
            "           Conv2d-72        [-1, 7, 1024, 1024]             455\n",
            "================================================================\n",
            "Total params: 31,036,871\n",
            "Trainable params: 31,036,871\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 4.00\n",
            "Forward/backward pass size (MB): 14920.00\n",
            "Params size (MB): 118.40\n",
            "Estimated Total Size (MB): 15042.40\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_model_64 = UNet()\n",
        "example_model_64.cuda()\n",
        "\n",
        "summary(example_model_64, input_size=(1,1024,1024))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0I0bzMc2s5w",
        "outputId": "1990212d-1b33-4b0f-f2b6-1f42582554c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1       [-1, 64, 1024, 1024]             640\n",
            "       BatchNorm2d-2       [-1, 64, 1024, 1024]             128\n",
            "              ReLU-3       [-1, 64, 1024, 1024]               0\n",
            "            Conv2d-4       [-1, 64, 1024, 1024]          36,928\n",
            "       BatchNorm2d-5       [-1, 64, 1024, 1024]             128\n",
            "              ReLU-6       [-1, 64, 1024, 1024]               0\n",
            "         MaxPool2d-7         [-1, 64, 512, 512]               0\n",
            "            Conv2d-8        [-1, 128, 512, 512]          73,856\n",
            "       BatchNorm2d-9        [-1, 128, 512, 512]             256\n",
            "             ReLU-10        [-1, 128, 512, 512]               0\n",
            "           Conv2d-11        [-1, 128, 512, 512]         147,584\n",
            "      BatchNorm2d-12        [-1, 128, 512, 512]             256\n",
            "             ReLU-13        [-1, 128, 512, 512]               0\n",
            "        MaxPool2d-14        [-1, 128, 256, 256]               0\n",
            "           Conv2d-15        [-1, 256, 256, 256]         295,168\n",
            "      BatchNorm2d-16        [-1, 256, 256, 256]             512\n",
            "             ReLU-17        [-1, 256, 256, 256]               0\n",
            "           Conv2d-18        [-1, 256, 256, 256]         590,080\n",
            "      BatchNorm2d-19        [-1, 256, 256, 256]             512\n",
            "             ReLU-20        [-1, 256, 256, 256]               0\n",
            "        MaxPool2d-21        [-1, 256, 128, 128]               0\n",
            "           Conv2d-22        [-1, 512, 128, 128]       1,180,160\n",
            "      BatchNorm2d-23        [-1, 512, 128, 128]           1,024\n",
            "             ReLU-24        [-1, 512, 128, 128]               0\n",
            "           Conv2d-25        [-1, 512, 128, 128]       2,359,808\n",
            "      BatchNorm2d-26        [-1, 512, 128, 128]           1,024\n",
            "             ReLU-27        [-1, 512, 128, 128]               0\n",
            "        MaxPool2d-28          [-1, 512, 64, 64]               0\n",
            "           Conv2d-29         [-1, 1024, 64, 64]       4,719,616\n",
            "      BatchNorm2d-30         [-1, 1024, 64, 64]           2,048\n",
            "             ReLU-31         [-1, 1024, 64, 64]               0\n",
            "           Conv2d-32          [-1, 512, 64, 64]       4,719,104\n",
            "      BatchNorm2d-33          [-1, 512, 64, 64]           1,024\n",
            "             ReLU-34          [-1, 512, 64, 64]               0\n",
            "  ConvTranspose2d-35        [-1, 512, 128, 128]       1,049,088\n",
            "           Conv2d-36        [-1, 512, 128, 128]       4,719,104\n",
            "      BatchNorm2d-37        [-1, 512, 128, 128]           1,024\n",
            "             ReLU-38        [-1, 512, 128, 128]               0\n",
            "           Conv2d-39        [-1, 256, 128, 128]       1,179,904\n",
            "      BatchNorm2d-40        [-1, 256, 128, 128]             512\n",
            "             ReLU-41        [-1, 256, 128, 128]               0\n",
            "  ConvTranspose2d-42        [-1, 256, 256, 256]         262,400\n",
            "           Conv2d-43        [-1, 256, 256, 256]       1,179,904\n",
            "      BatchNorm2d-44        [-1, 256, 256, 256]             512\n",
            "             ReLU-45        [-1, 256, 256, 256]               0\n",
            "           Conv2d-46        [-1, 128, 256, 256]         295,040\n",
            "      BatchNorm2d-47        [-1, 128, 256, 256]             256\n",
            "             ReLU-48        [-1, 128, 256, 256]               0\n",
            "  ConvTranspose2d-49        [-1, 128, 512, 512]          65,664\n",
            "           Conv2d-50        [-1, 128, 512, 512]         295,040\n",
            "      BatchNorm2d-51        [-1, 128, 512, 512]             256\n",
            "             ReLU-52        [-1, 128, 512, 512]               0\n",
            "           Conv2d-53         [-1, 64, 512, 512]          73,792\n",
            "      BatchNorm2d-54         [-1, 64, 512, 512]             128\n",
            "             ReLU-55         [-1, 64, 512, 512]               0\n",
            "  ConvTranspose2d-56       [-1, 64, 1024, 1024]          16,448\n",
            "           Conv2d-57       [-1, 64, 1024, 1024]          73,792\n",
            "      BatchNorm2d-58       [-1, 64, 1024, 1024]             128\n",
            "             ReLU-59       [-1, 64, 1024, 1024]               0\n",
            "           Conv2d-60       [-1, 64, 1024, 1024]          36,928\n",
            "      BatchNorm2d-61       [-1, 64, 1024, 1024]             128\n",
            "             ReLU-62       [-1, 64, 1024, 1024]               0\n",
            "           Conv2d-63        [-1, 6, 1024, 1024]             390\n",
            "================================================================\n",
            "Total params: 23,380,294\n",
            "Trainable params: 23,380,294\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 4.00\n",
            "Forward/backward pass size (MB): 12240.00\n",
            "Params size (MB): 89.19\n",
            "Estimated Total Size (MB): 12333.19\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_model_32 = UNet()\n",
        "example_model_32.cuda()\n",
        "\n",
        "summary(example_model_32, input_size=(1,1024,1024))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN5EU-i8NrwG",
        "outputId": "4cc8aee7-50cc-4edd-8eab-642edcd3201b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 256, 128, 128]) torch.Size([2, 256, 128, 128])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1       [-1, 32, 1024, 1024]             320\n",
            "       BatchNorm2d-2       [-1, 32, 1024, 1024]              64\n",
            "              ReLU-3       [-1, 32, 1024, 1024]               0\n",
            "            Conv2d-4       [-1, 32, 1024, 1024]           9,248\n",
            "       BatchNorm2d-5       [-1, 32, 1024, 1024]              64\n",
            "              ReLU-6       [-1, 32, 1024, 1024]               0\n",
            "         MaxPool2d-7         [-1, 32, 512, 512]               0\n",
            "            Conv2d-8         [-1, 64, 512, 512]          18,496\n",
            "       BatchNorm2d-9         [-1, 64, 512, 512]             128\n",
            "             ReLU-10         [-1, 64, 512, 512]               0\n",
            "           Conv2d-11         [-1, 64, 512, 512]          36,928\n",
            "      BatchNorm2d-12         [-1, 64, 512, 512]             128\n",
            "             ReLU-13         [-1, 64, 512, 512]               0\n",
            "        MaxPool2d-14         [-1, 64, 256, 256]               0\n",
            "           Conv2d-15        [-1, 128, 256, 256]          73,856\n",
            "      BatchNorm2d-16        [-1, 128, 256, 256]             256\n",
            "             ReLU-17        [-1, 128, 256, 256]               0\n",
            "           Conv2d-18        [-1, 128, 256, 256]         147,584\n",
            "      BatchNorm2d-19        [-1, 128, 256, 256]             256\n",
            "             ReLU-20        [-1, 128, 256, 256]               0\n",
            "        MaxPool2d-21        [-1, 128, 128, 128]               0\n",
            "           Conv2d-22        [-1, 256, 128, 128]         295,168\n",
            "      BatchNorm2d-23        [-1, 256, 128, 128]             512\n",
            "             ReLU-24        [-1, 256, 128, 128]               0\n",
            "           Conv2d-25        [-1, 256, 128, 128]         590,080\n",
            "      BatchNorm2d-26        [-1, 256, 128, 128]             512\n",
            "             ReLU-27        [-1, 256, 128, 128]               0\n",
            "        MaxPool2d-28          [-1, 256, 64, 64]               0\n",
            "           Conv2d-29          [-1, 512, 64, 64]       1,180,160\n",
            "      BatchNorm2d-30          [-1, 512, 64, 64]           1,024\n",
            "             ReLU-31          [-1, 512, 64, 64]               0\n",
            "           Conv2d-32          [-1, 256, 64, 64]       1,179,904\n",
            "      BatchNorm2d-33          [-1, 256, 64, 64]             512\n",
            "             ReLU-34          [-1, 256, 64, 64]               0\n",
            "  ConvTranspose2d-35        [-1, 256, 128, 128]         262,400\n",
            "           Conv2d-36        [-1, 256, 128, 128]       1,179,904\n",
            "      BatchNorm2d-37        [-1, 256, 128, 128]             512\n",
            "             ReLU-38        [-1, 256, 128, 128]               0\n",
            "           Conv2d-39        [-1, 128, 128, 128]         295,040\n",
            "      BatchNorm2d-40        [-1, 128, 128, 128]             256\n",
            "             ReLU-41        [-1, 128, 128, 128]               0\n",
            "  ConvTranspose2d-42        [-1, 128, 256, 256]          65,664\n",
            "           Conv2d-43        [-1, 128, 256, 256]         295,040\n",
            "      BatchNorm2d-44        [-1, 128, 256, 256]             256\n",
            "             ReLU-45        [-1, 128, 256, 256]               0\n",
            "           Conv2d-46         [-1, 64, 256, 256]          73,792\n",
            "      BatchNorm2d-47         [-1, 64, 256, 256]             128\n",
            "             ReLU-48         [-1, 64, 256, 256]               0\n",
            "  ConvTranspose2d-49         [-1, 64, 512, 512]          16,448\n",
            "           Conv2d-50         [-1, 64, 512, 512]          73,792\n",
            "      BatchNorm2d-51         [-1, 64, 512, 512]             128\n",
            "             ReLU-52         [-1, 64, 512, 512]               0\n",
            "           Conv2d-53         [-1, 32, 512, 512]          18,464\n",
            "      BatchNorm2d-54         [-1, 32, 512, 512]              64\n",
            "             ReLU-55         [-1, 32, 512, 512]               0\n",
            "  ConvTranspose2d-56       [-1, 32, 1024, 1024]           4,128\n",
            "           Conv2d-57       [-1, 32, 1024, 1024]          18,464\n",
            "      BatchNorm2d-58       [-1, 32, 1024, 1024]              64\n",
            "             ReLU-59       [-1, 32, 1024, 1024]               0\n",
            "           Conv2d-60       [-1, 32, 1024, 1024]           9,248\n",
            "      BatchNorm2d-61       [-1, 32, 1024, 1024]              64\n",
            "             ReLU-62       [-1, 32, 1024, 1024]               0\n",
            "           Conv2d-63        [-1, 6, 1024, 1024]             198\n",
            "================================================================\n",
            "Total params: 5,849,254\n",
            "Trainable params: 5,849,254\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 4.00\n",
            "Forward/backward pass size (MB): 6144.00\n",
            "Params size (MB): 22.31\n",
            "Estimated Total Size (MB): 6170.31\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "2YE49-OEd_Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Tvoa6l6lHwXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Spine_segment = UNET()\n",
        "Spine_segment.cuda()\n",
        "trainer_Spine_segment = trainer(Spine_segment, train_loader,\"Adam\", epoch_size=100, learning_rate=0.001)\n",
        "trainer_Spine_segment.train(val_loader)"
      ],
      "metadata": {
        "id": "ZWfK7q9K0fN-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5afa9fa-2b82-4ca4-bc6e-b5cb5ddbcaa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0001 / 0100 | batch 0000 / 0050 | loss 0.7060\n",
            "Train: epoch 0001 / 0100 | batch 0001 / 0050 | loss 0.6882\n",
            "Train: epoch 0001 / 0100 | batch 0002 / 0050 | loss 0.6752\n",
            "Train: epoch 0001 / 0100 | batch 0003 / 0050 | loss 0.6557\n",
            "Train: epoch 0001 / 0100 | batch 0004 / 0050 | loss 0.6356\n",
            "Train: epoch 0001 / 0100 | batch 0005 / 0050 | loss 0.6222\n",
            "Train: epoch 0001 / 0100 | batch 0006 / 0050 | loss 0.6065\n",
            "Train: epoch 0001 / 0100 | batch 0007 / 0050 | loss 0.5957\n",
            "Train: epoch 0001 / 0100 | batch 0008 / 0050 | loss 0.5834\n",
            "Train: epoch 0001 / 0100 | batch 0009 / 0050 | loss 0.5722\n",
            "Train: epoch 0001 / 0100 | batch 0010 / 0050 | loss 0.5633\n",
            "Train: epoch 0001 / 0100 | batch 0011 / 0050 | loss 0.5543\n",
            "Train: epoch 0001 / 0100 | batch 0012 / 0050 | loss 0.5462\n",
            "Train: epoch 0001 / 0100 | batch 0013 / 0050 | loss 0.5388\n",
            "Train: epoch 0001 / 0100 | batch 0014 / 0050 | loss 0.5316\n",
            "Train: epoch 0001 / 0100 | batch 0015 / 0050 | loss 0.5255\n",
            "Train: epoch 0001 / 0100 | batch 0016 / 0050 | loss 0.5194\n",
            "Train: epoch 0001 / 0100 | batch 0017 / 0050 | loss 0.5132\n",
            "Train: epoch 0001 / 0100 | batch 0018 / 0050 | loss 0.5071\n",
            "Train: epoch 0001 / 0100 | batch 0019 / 0050 | loss 0.5015\n",
            "Train: epoch 0001 / 0100 | batch 0020 / 0050 | loss 0.4968\n",
            "Train: epoch 0001 / 0100 | batch 0021 / 0050 | loss 0.4914\n",
            "Train: epoch 0001 / 0100 | batch 0022 / 0050 | loss 0.4864\n",
            "Train: epoch 0001 / 0100 | batch 0023 / 0050 | loss 0.4815\n",
            "Train: epoch 0001 / 0100 | batch 0024 / 0050 | loss 0.4767\n",
            "Train: epoch 0001 / 0100 | batch 0025 / 0050 | loss 0.4722\n",
            "Train: epoch 0001 / 0100 | batch 0026 / 0050 | loss 0.4681\n",
            "Train: epoch 0001 / 0100 | batch 0027 / 0050 | loss 0.4636\n",
            "Train: epoch 0001 / 0100 | batch 0028 / 0050 | loss 0.4594\n",
            "Train: epoch 0001 / 0100 | batch 0029 / 0050 | loss 0.4552\n",
            "Train: epoch 0001 / 0100 | batch 0030 / 0050 | loss 0.4512\n",
            "Train: epoch 0001 / 0100 | batch 0031 / 0050 | loss 0.4471\n",
            "Train: epoch 0001 / 0100 | batch 0032 / 0050 | loss 0.4430\n",
            "Train: epoch 0001 / 0100 | batch 0033 / 0050 | loss 0.4391\n",
            "Train: epoch 0001 / 0100 | batch 0034 / 0050 | loss 0.4355\n",
            "Train: epoch 0001 / 0100 | batch 0035 / 0050 | loss 0.4318\n",
            "Train: epoch 0001 / 0100 | batch 0036 / 0050 | loss 0.4280\n",
            "Train: epoch 0001 / 0100 | batch 0037 / 0050 | loss 0.4244\n",
            "Train: epoch 0001 / 0100 | batch 0038 / 0050 | loss 0.4208\n",
            "Train: epoch 0001 / 0100 | batch 0039 / 0050 | loss 0.4172\n",
            "Train: epoch 0001 / 0100 | batch 0040 / 0050 | loss 0.4138\n",
            "Train: epoch 0001 / 0100 | batch 0041 / 0050 | loss 0.4105\n",
            "Train: epoch 0001 / 0100 | batch 0042 / 0050 | loss 0.4072\n",
            "Train: epoch 0001 / 0100 | batch 0043 / 0050 | loss 0.4039\n",
            "Train: epoch 0001 / 0100 | batch 0044 / 0050 | loss 0.4007\n",
            "Train: epoch 0001 / 0100 | batch 0045 / 0050 | loss 0.3975\n",
            "Train: epoch 0001 / 0100 | batch 0046 / 0050 | loss 0.3944\n",
            "Train: epoch 0001 / 0100 | batch 0047 / 0050 | loss 0.3914\n",
            "Train: epoch 0001 / 0100 | batch 0048 / 0050 | loss 0.3882\n",
            "Train: epoch 0001 / 0100 | batch 0049 / 0050 | loss 0.3851\n",
            "Val loss 0.2411\n",
            "Dice score : 0.023350924253463745\n",
            "Val loss 0.2348\n",
            "Dice score : 0.012921089306473732\n",
            "Val loss 0.2328\n",
            "Dice score : 0.014842950738966465\n",
            "Val loss 0.2309\n",
            "Dice score : 0.016863081604242325\n",
            "Val loss 0.2317\n",
            "Dice score : 0.02171635627746582\n",
            "Val loss 0.2317\n",
            "Dice score : 0.008240028284490108\n",
            "Val loss 0.2309\n",
            "Dice score : 0.0099610211327672\n",
            "Val loss 0.2303\n",
            "Dice score : 0.008322987705469131\n",
            "Val loss 0.2301\n",
            "Dice score : 0.015099695883691311\n",
            "Val loss 0.2302\n",
            "Dice score : 0.017390329390764236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 1/100 [01:23<2:18:22, 83.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0002 / 0100 | batch 0000 / 0050 | loss 0.2310\n",
            "Train: epoch 0002 / 0100 | batch 0001 / 0050 | loss 0.2264\n",
            "Train: epoch 0002 / 0100 | batch 0002 / 0050 | loss 0.2249\n",
            "Train: epoch 0002 / 0100 | batch 0003 / 0050 | loss 0.2244\n",
            "Train: epoch 0002 / 0100 | batch 0004 / 0050 | loss 0.2239\n",
            "Train: epoch 0002 / 0100 | batch 0005 / 0050 | loss 0.2215\n",
            "Train: epoch 0002 / 0100 | batch 0006 / 0050 | loss 0.2205\n",
            "Train: epoch 0002 / 0100 | batch 0007 / 0050 | loss 0.2185\n",
            "Train: epoch 0002 / 0100 | batch 0008 / 0050 | loss 0.2160\n",
            "Train: epoch 0002 / 0100 | batch 0009 / 0050 | loss 0.2138\n",
            "Train: epoch 0002 / 0100 | batch 0010 / 0050 | loss 0.2116\n",
            "Train: epoch 0002 / 0100 | batch 0011 / 0050 | loss 0.2114\n",
            "Train: epoch 0002 / 0100 | batch 0012 / 0050 | loss 0.2092\n",
            "Train: epoch 0002 / 0100 | batch 0013 / 0050 | loss 0.2075\n",
            "Train: epoch 0002 / 0100 | batch 0014 / 0050 | loss 0.2062\n",
            "Train: epoch 0002 / 0100 | batch 0015 / 0050 | loss 0.2043\n",
            "Train: epoch 0002 / 0100 | batch 0016 / 0050 | loss 0.2027\n",
            "Train: epoch 0002 / 0100 | batch 0017 / 0050 | loss 0.2011\n",
            "Train: epoch 0002 / 0100 | batch 0018 / 0050 | loss 0.1999\n",
            "Train: epoch 0002 / 0100 | batch 0019 / 0050 | loss 0.1994\n",
            "Train: epoch 0002 / 0100 | batch 0020 / 0050 | loss 0.1976\n",
            "Train: epoch 0002 / 0100 | batch 0021 / 0050 | loss 0.1964\n",
            "Train: epoch 0002 / 0100 | batch 0022 / 0050 | loss 0.1952\n",
            "Train: epoch 0002 / 0100 | batch 0023 / 0050 | loss 0.1938\n",
            "Train: epoch 0002 / 0100 | batch 0024 / 0050 | loss 0.1926\n",
            "Train: epoch 0002 / 0100 | batch 0025 / 0050 | loss 0.1910\n",
            "Train: epoch 0002 / 0100 | batch 0026 / 0050 | loss 0.1899\n",
            "Train: epoch 0002 / 0100 | batch 0027 / 0050 | loss 0.1882\n",
            "Train: epoch 0002 / 0100 | batch 0028 / 0050 | loss 0.1873\n",
            "Train: epoch 0002 / 0100 | batch 0029 / 0050 | loss 0.1860\n",
            "Train: epoch 0002 / 0100 | batch 0030 / 0050 | loss 0.1848\n",
            "Train: epoch 0002 / 0100 | batch 0031 / 0050 | loss 0.1835\n",
            "Train: epoch 0002 / 0100 | batch 0032 / 0050 | loss 0.1823\n",
            "Train: epoch 0002 / 0100 | batch 0033 / 0050 | loss 0.1808\n",
            "Train: epoch 0002 / 0100 | batch 0034 / 0050 | loss 0.1792\n",
            "Train: epoch 0002 / 0100 | batch 0035 / 0050 | loss 0.1780\n",
            "Train: epoch 0002 / 0100 | batch 0036 / 0050 | loss 0.1767\n",
            "Train: epoch 0002 / 0100 | batch 0037 / 0050 | loss 0.1754\n",
            "Train: epoch 0002 / 0100 | batch 0038 / 0050 | loss 0.1740\n",
            "Train: epoch 0002 / 0100 | batch 0039 / 0050 | loss 0.1725\n",
            "Train: epoch 0002 / 0100 | batch 0040 / 0050 | loss 0.1713\n",
            "Train: epoch 0002 / 0100 | batch 0041 / 0050 | loss 0.1701\n",
            "Train: epoch 0002 / 0100 | batch 0042 / 0050 | loss 0.1687\n",
            "Train: epoch 0002 / 0100 | batch 0043 / 0050 | loss 0.1674\n",
            "Train: epoch 0002 / 0100 | batch 0044 / 0050 | loss 0.1662\n",
            "Train: epoch 0002 / 0100 | batch 0045 / 0050 | loss 0.1652\n",
            "Train: epoch 0002 / 0100 | batch 0046 / 0050 | loss 0.1641\n",
            "Train: epoch 0002 / 0100 | batch 0047 / 0050 | loss 0.1631\n",
            "Train: epoch 0002 / 0100 | batch 0048 / 0050 | loss 0.1622\n",
            "Train: epoch 0002 / 0100 | batch 0049 / 0050 | loss 0.1613\n",
            "Val loss 0.1143\n",
            "Dice score : 0.02751990035176277\n",
            "Val loss 0.1204\n",
            "Dice score : 0.03009232133626938\n",
            "Val loss 0.1205\n",
            "Dice score : 0.015707189217209816\n",
            "Val loss 0.1190\n",
            "Dice score : 0.03574303910136223\n",
            "Val loss 0.1208\n",
            "Dice score : 0.03816104680299759\n",
            "Val loss 0.1206\n",
            "Dice score : 0.024196084588766098\n",
            "Val loss 0.1205\n",
            "Dice score : 0.02210736647248268\n",
            "Val loss 0.1196\n",
            "Dice score : 0.02190284989774227\n",
            "Val loss 0.1189\n",
            "Dice score : 0.02473776414990425\n",
            "Val loss 0.1201\n",
            "Dice score : 0.024043820798397064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/100 [02:44<2:13:40, 81.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0003 / 0100 | batch 0000 / 0050 | loss 0.1035\n",
            "Train: epoch 0003 / 0100 | batch 0001 / 0050 | loss 0.1104\n",
            "Train: epoch 0003 / 0100 | batch 0002 / 0050 | loss 0.1103\n",
            "Train: epoch 0003 / 0100 | batch 0003 / 0050 | loss 0.1073\n",
            "Train: epoch 0003 / 0100 | batch 0004 / 0050 | loss 0.1077\n",
            "Train: epoch 0003 / 0100 | batch 0005 / 0050 | loss 0.1059\n",
            "Train: epoch 0003 / 0100 | batch 0006 / 0050 | loss 0.1046\n",
            "Train: epoch 0003 / 0100 | batch 0007 / 0050 | loss 0.1036\n",
            "Train: epoch 0003 / 0100 | batch 0008 / 0050 | loss 0.1026\n",
            "Train: epoch 0003 / 0100 | batch 0009 / 0050 | loss 0.1016\n",
            "Train: epoch 0003 / 0100 | batch 0010 / 0050 | loss 0.1018\n",
            "Train: epoch 0003 / 0100 | batch 0011 / 0050 | loss 0.1020\n",
            "Train: epoch 0003 / 0100 | batch 0012 / 0050 | loss 0.1015\n",
            "Train: epoch 0003 / 0100 | batch 0013 / 0050 | loss 0.1032\n",
            "Train: epoch 0003 / 0100 | batch 0014 / 0050 | loss 0.1027\n",
            "Train: epoch 0003 / 0100 | batch 0015 / 0050 | loss 0.1025\n",
            "Train: epoch 0003 / 0100 | batch 0016 / 0050 | loss 0.1031\n",
            "Train: epoch 0003 / 0100 | batch 0017 / 0050 | loss 0.1021\n",
            "Train: epoch 0003 / 0100 | batch 0018 / 0050 | loss 0.1023\n",
            "Train: epoch 0003 / 0100 | batch 0019 / 0050 | loss 0.1029\n",
            "Train: epoch 0003 / 0100 | batch 0020 / 0050 | loss 0.1022\n",
            "Train: epoch 0003 / 0100 | batch 0021 / 0050 | loss 0.1015\n",
            "Train: epoch 0003 / 0100 | batch 0022 / 0050 | loss 0.1016\n",
            "Train: epoch 0003 / 0100 | batch 0023 / 0050 | loss 0.1015\n",
            "Train: epoch 0003 / 0100 | batch 0024 / 0050 | loss 0.1006\n",
            "Train: epoch 0003 / 0100 | batch 0025 / 0050 | loss 0.1003\n",
            "Train: epoch 0003 / 0100 | batch 0026 / 0050 | loss 0.0995\n",
            "Train: epoch 0003 / 0100 | batch 0027 / 0050 | loss 0.0988\n",
            "Train: epoch 0003 / 0100 | batch 0028 / 0050 | loss 0.0982\n",
            "Train: epoch 0003 / 0100 | batch 0029 / 0050 | loss 0.0975\n",
            "Train: epoch 0003 / 0100 | batch 0030 / 0050 | loss 0.0969\n",
            "Train: epoch 0003 / 0100 | batch 0031 / 0050 | loss 0.0963\n",
            "Train: epoch 0003 / 0100 | batch 0032 / 0050 | loss 0.0958\n",
            "Train: epoch 0003 / 0100 | batch 0033 / 0050 | loss 0.0954\n",
            "Train: epoch 0003 / 0100 | batch 0034 / 0050 | loss 0.0950\n",
            "Train: epoch 0003 / 0100 | batch 0035 / 0050 | loss 0.0945\n",
            "Train: epoch 0003 / 0100 | batch 0036 / 0050 | loss 0.0942\n",
            "Train: epoch 0003 / 0100 | batch 0037 / 0050 | loss 0.0936\n",
            "Train: epoch 0003 / 0100 | batch 0038 / 0050 | loss 0.0936\n",
            "Train: epoch 0003 / 0100 | batch 0039 / 0050 | loss 0.0933\n",
            "Train: epoch 0003 / 0100 | batch 0040 / 0050 | loss 0.0926\n",
            "Train: epoch 0003 / 0100 | batch 0041 / 0050 | loss 0.0921\n",
            "Train: epoch 0003 / 0100 | batch 0042 / 0050 | loss 0.0915\n",
            "Train: epoch 0003 / 0100 | batch 0043 / 0050 | loss 0.0909\n",
            "Train: epoch 0003 / 0100 | batch 0044 / 0050 | loss 0.0904\n",
            "Train: epoch 0003 / 0100 | batch 0045 / 0050 | loss 0.0902\n",
            "Train: epoch 0003 / 0100 | batch 0046 / 0050 | loss 0.0904\n",
            "Train: epoch 0003 / 0100 | batch 0047 / 0050 | loss 0.0899\n",
            "Train: epoch 0003 / 0100 | batch 0048 / 0050 | loss 0.0898\n",
            "Train: epoch 0003 / 0100 | batch 0049 / 0050 | loss 0.0898\n",
            "Val loss 0.0870\n",
            "Dice score : 0.024435723200440407\n",
            "Val loss 0.0761\n",
            "Dice score : 0.03178824484348297\n",
            "Val loss 0.0727\n",
            "Dice score : 0.02453891560435295\n",
            "Val loss 0.0714\n",
            "Dice score : 0.025966010987758636\n",
            "Val loss 0.0700\n",
            "Dice score : 0.015983497723937035\n",
            "Val loss 0.0705\n",
            "Dice score : 0.02786550112068653\n",
            "Val loss 0.0718\n",
            "Dice score : 0.035597652196884155\n",
            "Val loss 0.0726\n",
            "Dice score : 0.05020875111222267\n",
            "Val loss 0.0747\n",
            "Dice score : 0.052782632410526276\n",
            "Val loss 0.0743\n",
            "Dice score : 0.030129898339509964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 3/100 [04:05<2:11:47, 81.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0004 / 0100 | batch 0000 / 0050 | loss 0.0740\n",
            "Train: epoch 0004 / 0100 | batch 0001 / 0050 | loss 0.0744\n",
            "Train: epoch 0004 / 0100 | batch 0002 / 0050 | loss 0.0709\n",
            "Train: epoch 0004 / 0100 | batch 0003 / 0050 | loss 0.0696\n",
            "Train: epoch 0004 / 0100 | batch 0004 / 0050 | loss 0.0694\n",
            "Train: epoch 0004 / 0100 | batch 0005 / 0050 | loss 0.0703\n",
            "Train: epoch 0004 / 0100 | batch 0006 / 0050 | loss 0.0719\n",
            "Train: epoch 0004 / 0100 | batch 0007 / 0050 | loss 0.0713\n",
            "Train: epoch 0004 / 0100 | batch 0008 / 0050 | loss 0.0728\n",
            "Train: epoch 0004 / 0100 | batch 0009 / 0050 | loss 0.0730\n",
            "Train: epoch 0004 / 0100 | batch 0010 / 0050 | loss 0.0725\n",
            "Train: epoch 0004 / 0100 | batch 0011 / 0050 | loss 0.0728\n",
            "Train: epoch 0004 / 0100 | batch 0012 / 0050 | loss 0.0722\n",
            "Train: epoch 0004 / 0100 | batch 0013 / 0050 | loss 0.0725\n",
            "Train: epoch 0004 / 0100 | batch 0014 / 0050 | loss 0.0719\n",
            "Train: epoch 0004 / 0100 | batch 0015 / 0050 | loss 0.0718\n",
            "Train: epoch 0004 / 0100 | batch 0016 / 0050 | loss 0.0715\n",
            "Train: epoch 0004 / 0100 | batch 0017 / 0050 | loss 0.0711\n",
            "Train: epoch 0004 / 0100 | batch 0018 / 0050 | loss 0.0709\n",
            "Train: epoch 0004 / 0100 | batch 0019 / 0050 | loss 0.0710\n",
            "Train: epoch 0004 / 0100 | batch 0020 / 0050 | loss 0.0709\n",
            "Train: epoch 0004 / 0100 | batch 0021 / 0050 | loss 0.0709\n",
            "Train: epoch 0004 / 0100 | batch 0022 / 0050 | loss 0.0706\n",
            "Train: epoch 0004 / 0100 | batch 0023 / 0050 | loss 0.0708\n",
            "Train: epoch 0004 / 0100 | batch 0024 / 0050 | loss 0.0702\n",
            "Train: epoch 0004 / 0100 | batch 0025 / 0050 | loss 0.0704\n",
            "Train: epoch 0004 / 0100 | batch 0026 / 0050 | loss 0.0709\n",
            "Train: epoch 0004 / 0100 | batch 0027 / 0050 | loss 0.0710\n",
            "Train: epoch 0004 / 0100 | batch 0028 / 0050 | loss 0.0719\n",
            "Train: epoch 0004 / 0100 | batch 0029 / 0050 | loss 0.0721\n",
            "Train: epoch 0004 / 0100 | batch 0030 / 0050 | loss 0.0723\n",
            "Train: epoch 0004 / 0100 | batch 0031 / 0050 | loss 0.0729\n",
            "Train: epoch 0004 / 0100 | batch 0032 / 0050 | loss 0.0725\n",
            "Train: epoch 0004 / 0100 | batch 0033 / 0050 | loss 0.0723\n",
            "Train: epoch 0004 / 0100 | batch 0034 / 0050 | loss 0.0730\n",
            "Train: epoch 0004 / 0100 | batch 0035 / 0050 | loss 0.0733\n",
            "Train: epoch 0004 / 0100 | batch 0036 / 0050 | loss 0.0731\n",
            "Train: epoch 0004 / 0100 | batch 0037 / 0050 | loss 0.0726\n",
            "Train: epoch 0004 / 0100 | batch 0038 / 0050 | loss 0.0722\n",
            "Train: epoch 0004 / 0100 | batch 0039 / 0050 | loss 0.0720\n",
            "Train: epoch 0004 / 0100 | batch 0040 / 0050 | loss 0.0715\n",
            "Train: epoch 0004 / 0100 | batch 0041 / 0050 | loss 0.0714\n",
            "Train: epoch 0004 / 0100 | batch 0042 / 0050 | loss 0.0712\n",
            "Train: epoch 0004 / 0100 | batch 0043 / 0050 | loss 0.0713\n",
            "Train: epoch 0004 / 0100 | batch 0044 / 0050 | loss 0.0710\n",
            "Train: epoch 0004 / 0100 | batch 0045 / 0050 | loss 0.0708\n",
            "Train: epoch 0004 / 0100 | batch 0046 / 0050 | loss 0.0709\n",
            "Train: epoch 0004 / 0100 | batch 0047 / 0050 | loss 0.0708\n",
            "Train: epoch 0004 / 0100 | batch 0048 / 0050 | loss 0.0704\n",
            "Train: epoch 0004 / 0100 | batch 0049 / 0050 | loss 0.0704\n",
            "Val loss 0.0618\n",
            "Dice score : 0.027204005047678947\n",
            "Val loss 0.0608\n",
            "Dice score : 0.034432005137205124\n",
            "Val loss 0.0636\n",
            "Dice score : 0.030973615124821663\n",
            "Val loss 0.0618\n",
            "Dice score : 0.02459176629781723\n",
            "Val loss 0.0648\n",
            "Dice score : 0.03091714344918728\n",
            "Val loss 0.0673\n",
            "Dice score : 0.050489384680986404\n",
            "Val loss 0.0657\n",
            "Dice score : 0.03266337513923645\n",
            "Val loss 0.0646\n",
            "Dice score : 0.025426389649510384\n",
            "Val loss 0.0632\n",
            "Dice score : 0.020694805309176445\n",
            "Val loss 0.0634\n",
            "Dice score : 0.0321631021797657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 4/100 [05:26<2:10:09, 81.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0005 / 0100 | batch 0000 / 0050 | loss 0.0613\n",
            "Train: epoch 0005 / 0100 | batch 0001 / 0050 | loss 0.0821\n",
            "Train: epoch 0005 / 0100 | batch 0002 / 0050 | loss 0.0758\n",
            "Train: epoch 0005 / 0100 | batch 0003 / 0050 | loss 0.0711\n",
            "Train: epoch 0005 / 0100 | batch 0004 / 0050 | loss 0.0705\n",
            "Train: epoch 0005 / 0100 | batch 0005 / 0050 | loss 0.0680\n",
            "Train: epoch 0005 / 0100 | batch 0006 / 0050 | loss 0.0681\n",
            "Train: epoch 0005 / 0100 | batch 0007 / 0050 | loss 0.0665\n",
            "Train: epoch 0005 / 0100 | batch 0008 / 0050 | loss 0.0667\n",
            "Train: epoch 0005 / 0100 | batch 0009 / 0050 | loss 0.0668\n",
            "Train: epoch 0005 / 0100 | batch 0010 / 0050 | loss 0.0674\n",
            "Train: epoch 0005 / 0100 | batch 0011 / 0050 | loss 0.0669\n",
            "Train: epoch 0005 / 0100 | batch 0012 / 0050 | loss 0.0663\n",
            "Train: epoch 0005 / 0100 | batch 0013 / 0050 | loss 0.0653\n",
            "Train: epoch 0005 / 0100 | batch 0014 / 0050 | loss 0.0646\n",
            "Train: epoch 0005 / 0100 | batch 0015 / 0050 | loss 0.0646\n",
            "Train: epoch 0005 / 0100 | batch 0016 / 0050 | loss 0.0659\n",
            "Train: epoch 0005 / 0100 | batch 0017 / 0050 | loss 0.0653\n",
            "Train: epoch 0005 / 0100 | batch 0018 / 0050 | loss 0.0648\n",
            "Train: epoch 0005 / 0100 | batch 0019 / 0050 | loss 0.0645\n",
            "Train: epoch 0005 / 0100 | batch 0020 / 0050 | loss 0.0642\n",
            "Train: epoch 0005 / 0100 | batch 0021 / 0050 | loss 0.0640\n",
            "Train: epoch 0005 / 0100 | batch 0022 / 0050 | loss 0.0640\n",
            "Train: epoch 0005 / 0100 | batch 0023 / 0050 | loss 0.0638\n",
            "Train: epoch 0005 / 0100 | batch 0024 / 0050 | loss 0.0634\n",
            "Train: epoch 0005 / 0100 | batch 0025 / 0050 | loss 0.0632\n",
            "Train: epoch 0005 / 0100 | batch 0026 / 0050 | loss 0.0636\n",
            "Train: epoch 0005 / 0100 | batch 0027 / 0050 | loss 0.0635\n",
            "Train: epoch 0005 / 0100 | batch 0028 / 0050 | loss 0.0636\n",
            "Train: epoch 0005 / 0100 | batch 0029 / 0050 | loss 0.0632\n",
            "Train: epoch 0005 / 0100 | batch 0030 / 0050 | loss 0.0640\n",
            "Train: epoch 0005 / 0100 | batch 0031 / 0050 | loss 0.0640\n",
            "Train: epoch 0005 / 0100 | batch 0032 / 0050 | loss 0.0642\n",
            "Train: epoch 0005 / 0100 | batch 0033 / 0050 | loss 0.0642\n",
            "Train: epoch 0005 / 0100 | batch 0034 / 0050 | loss 0.0638\n",
            "Train: epoch 0005 / 0100 | batch 0035 / 0050 | loss 0.0634\n",
            "Train: epoch 0005 / 0100 | batch 0036 / 0050 | loss 0.0636\n",
            "Train: epoch 0005 / 0100 | batch 0037 / 0050 | loss 0.0633\n",
            "Train: epoch 0005 / 0100 | batch 0038 / 0050 | loss 0.0631\n",
            "Train: epoch 0005 / 0100 | batch 0039 / 0050 | loss 0.0630\n",
            "Train: epoch 0005 / 0100 | batch 0040 / 0050 | loss 0.0633\n",
            "Train: epoch 0005 / 0100 | batch 0041 / 0050 | loss 0.0629\n",
            "Train: epoch 0005 / 0100 | batch 0042 / 0050 | loss 0.0626\n",
            "Train: epoch 0005 / 0100 | batch 0043 / 0050 | loss 0.0631\n",
            "Train: epoch 0005 / 0100 | batch 0044 / 0050 | loss 0.0633\n",
            "Train: epoch 0005 / 0100 | batch 0045 / 0050 | loss 0.0631\n",
            "Train: epoch 0005 / 0100 | batch 0046 / 0050 | loss 0.0633\n",
            "Train: epoch 0005 / 0100 | batch 0047 / 0050 | loss 0.0636\n",
            "Train: epoch 0005 / 0100 | batch 0048 / 0050 | loss 0.0638\n",
            "Train: epoch 0005 / 0100 | batch 0049 / 0050 | loss 0.0635\n",
            "Val loss 0.0568\n",
            "Dice score : 0.042466506361961365\n",
            "Val loss 0.0648\n",
            "Dice score : 0.05611474812030792\n",
            "Val loss 0.0586\n",
            "Dice score : 0.02193635143339634\n",
            "Val loss 0.0561\n",
            "Dice score : 0.02043047547340393\n",
            "Val loss 0.0548\n",
            "Dice score : 0.05374652519822121\n",
            "Val loss 0.0540\n",
            "Dice score : 0.02591356635093689\n",
            "Val loss 0.0539\n",
            "Dice score : 0.037756938487291336\n",
            "Val loss 0.0551\n",
            "Dice score : 0.048612646758556366\n",
            "Val loss 0.0551\n",
            "Dice score : 0.015359326265752316\n",
            "Val loss 0.0581\n",
            "Dice score : 0.050445184111595154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 5/100 [06:45<2:07:21, 80.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0006 / 0100 | batch 0000 / 0050 | loss 0.0660\n",
            "Train: epoch 0006 / 0100 | batch 0001 / 0050 | loss 0.0620\n",
            "Train: epoch 0006 / 0100 | batch 0002 / 0050 | loss 0.0648\n",
            "Train: epoch 0006 / 0100 | batch 0003 / 0050 | loss 0.0666\n",
            "Train: epoch 0006 / 0100 | batch 0004 / 0050 | loss 0.0666\n",
            "Train: epoch 0006 / 0100 | batch 0005 / 0050 | loss 0.0637\n",
            "Train: epoch 0006 / 0100 | batch 0006 / 0050 | loss 0.0619\n",
            "Train: epoch 0006 / 0100 | batch 0007 / 0050 | loss 0.0603\n",
            "Train: epoch 0006 / 0100 | batch 0008 / 0050 | loss 0.0619\n",
            "Train: epoch 0006 / 0100 | batch 0009 / 0050 | loss 0.0635\n",
            "Train: epoch 0006 / 0100 | batch 0010 / 0050 | loss 0.0625\n",
            "Train: epoch 0006 / 0100 | batch 0011 / 0050 | loss 0.0629\n",
            "Train: epoch 0006 / 0100 | batch 0012 / 0050 | loss 0.0624\n",
            "Train: epoch 0006 / 0100 | batch 0013 / 0050 | loss 0.0630\n",
            "Train: epoch 0006 / 0100 | batch 0014 / 0050 | loss 0.0619\n",
            "Train: epoch 0006 / 0100 | batch 0015 / 0050 | loss 0.0614\n",
            "Train: epoch 0006 / 0100 | batch 0016 / 0050 | loss 0.0611\n",
            "Train: epoch 0006 / 0100 | batch 0017 / 0050 | loss 0.0606\n",
            "Train: epoch 0006 / 0100 | batch 0018 / 0050 | loss 0.0606\n",
            "Train: epoch 0006 / 0100 | batch 0019 / 0050 | loss 0.0605\n",
            "Train: epoch 0006 / 0100 | batch 0020 / 0050 | loss 0.0605\n",
            "Train: epoch 0006 / 0100 | batch 0021 / 0050 | loss 0.0600\n",
            "Train: epoch 0006 / 0100 | batch 0022 / 0050 | loss 0.0599\n",
            "Train: epoch 0006 / 0100 | batch 0023 / 0050 | loss 0.0591\n",
            "Train: epoch 0006 / 0100 | batch 0024 / 0050 | loss 0.0607\n",
            "Train: epoch 0006 / 0100 | batch 0025 / 0050 | loss 0.0615\n",
            "Train: epoch 0006 / 0100 | batch 0026 / 0050 | loss 0.0616\n",
            "Train: epoch 0006 / 0100 | batch 0027 / 0050 | loss 0.0614\n",
            "Train: epoch 0006 / 0100 | batch 0028 / 0050 | loss 0.0614\n",
            "Train: epoch 0006 / 0100 | batch 0029 / 0050 | loss 0.0617\n",
            "Train: epoch 0006 / 0100 | batch 0030 / 0050 | loss 0.0615\n",
            "Train: epoch 0006 / 0100 | batch 0031 / 0050 | loss 0.0612\n",
            "Train: epoch 0006 / 0100 | batch 0032 / 0050 | loss 0.0608\n",
            "Train: epoch 0006 / 0100 | batch 0033 / 0050 | loss 0.0608\n",
            "Train: epoch 0006 / 0100 | batch 0034 / 0050 | loss 0.0606\n",
            "Train: epoch 0006 / 0100 | batch 0035 / 0050 | loss 0.0603\n",
            "Train: epoch 0006 / 0100 | batch 0036 / 0050 | loss 0.0601\n",
            "Train: epoch 0006 / 0100 | batch 0037 / 0050 | loss 0.0600\n",
            "Train: epoch 0006 / 0100 | batch 0038 / 0050 | loss 0.0595\n",
            "Train: epoch 0006 / 0100 | batch 0039 / 0050 | loss 0.0597\n",
            "Train: epoch 0006 / 0100 | batch 0040 / 0050 | loss 0.0599\n",
            "Train: epoch 0006 / 0100 | batch 0041 / 0050 | loss 0.0602\n",
            "Train: epoch 0006 / 0100 | batch 0042 / 0050 | loss 0.0608\n",
            "Train: epoch 0006 / 0100 | batch 0043 / 0050 | loss 0.0605\n",
            "Train: epoch 0006 / 0100 | batch 0044 / 0050 | loss 0.0605\n",
            "Train: epoch 0006 / 0100 | batch 0045 / 0050 | loss 0.0604\n",
            "Train: epoch 0006 / 0100 | batch 0046 / 0050 | loss 0.0608\n",
            "Train: epoch 0006 / 0100 | batch 0047 / 0050 | loss 0.0605\n",
            "Train: epoch 0006 / 0100 | batch 0048 / 0050 | loss 0.0603\n",
            "Train: epoch 0006 / 0100 | batch 0049 / 0050 | loss 0.0603\n",
            "Val loss 0.0786\n",
            "Dice score : 0.03933190926909447\n",
            "Val loss 0.0603\n",
            "Dice score : 0.025464102625846863\n",
            "Val loss 0.0598\n",
            "Dice score : 0.03732023015618324\n",
            "Val loss 0.0576\n",
            "Dice score : 0.028535855934023857\n",
            "Val loss 0.0550\n",
            "Dice score : 0.027594121173024178\n",
            "Val loss 0.0538\n",
            "Dice score : 0.05011015012860298\n",
            "Val loss 0.0551\n",
            "Dice score : 0.03606746718287468\n",
            "Val loss 0.0578\n",
            "Dice score : 0.03952960669994354\n",
            "Val loss 0.0561\n",
            "Dice score : 0.03274701535701752\n",
            "Val loss 0.0562\n",
            "Dice score : 0.04052961990237236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 6/100 [08:07<2:06:43, 80.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0007 / 0100 | batch 0000 / 0050 | loss 0.0492\n",
            "Train: epoch 0007 / 0100 | batch 0001 / 0050 | loss 0.0533\n",
            "Train: epoch 0007 / 0100 | batch 0002 / 0050 | loss 0.0543\n",
            "Train: epoch 0007 / 0100 | batch 0003 / 0050 | loss 0.0557\n",
            "Train: epoch 0007 / 0100 | batch 0004 / 0050 | loss 0.0564\n",
            "Train: epoch 0007 / 0100 | batch 0005 / 0050 | loss 0.0593\n",
            "Train: epoch 0007 / 0100 | batch 0006 / 0050 | loss 0.0597\n",
            "Train: epoch 0007 / 0100 | batch 0007 / 0050 | loss 0.0581\n",
            "Train: epoch 0007 / 0100 | batch 0008 / 0050 | loss 0.0567\n",
            "Train: epoch 0007 / 0100 | batch 0009 / 0050 | loss 0.0597\n",
            "Train: epoch 0007 / 0100 | batch 0010 / 0050 | loss 0.0595\n",
            "Train: epoch 0007 / 0100 | batch 0011 / 0050 | loss 0.0588\n",
            "Train: epoch 0007 / 0100 | batch 0012 / 0050 | loss 0.0587\n",
            "Train: epoch 0007 / 0100 | batch 0013 / 0050 | loss 0.0589\n",
            "Train: epoch 0007 / 0100 | batch 0014 / 0050 | loss 0.0580\n",
            "Train: epoch 0007 / 0100 | batch 0015 / 0050 | loss 0.0590\n",
            "Train: epoch 0007 / 0100 | batch 0016 / 0050 | loss 0.0583\n",
            "Train: epoch 0007 / 0100 | batch 0017 / 0050 | loss 0.0580\n",
            "Train: epoch 0007 / 0100 | batch 0018 / 0050 | loss 0.0583\n",
            "Train: epoch 0007 / 0100 | batch 0019 / 0050 | loss 0.0579\n",
            "Train: epoch 0007 / 0100 | batch 0020 / 0050 | loss 0.0581\n",
            "Train: epoch 0007 / 0100 | batch 0021 / 0050 | loss 0.0583\n",
            "Train: epoch 0007 / 0100 | batch 0022 / 0050 | loss 0.0579\n",
            "Train: epoch 0007 / 0100 | batch 0023 / 0050 | loss 0.0579\n",
            "Train: epoch 0007 / 0100 | batch 0024 / 0050 | loss 0.0576\n",
            "Train: epoch 0007 / 0100 | batch 0025 / 0050 | loss 0.0577\n",
            "Train: epoch 0007 / 0100 | batch 0026 / 0050 | loss 0.0579\n",
            "Train: epoch 0007 / 0100 | batch 0027 / 0050 | loss 0.0582\n",
            "Train: epoch 0007 / 0100 | batch 0028 / 0050 | loss 0.0577\n",
            "Train: epoch 0007 / 0100 | batch 0029 / 0050 | loss 0.0573\n",
            "Train: epoch 0007 / 0100 | batch 0030 / 0050 | loss 0.0572\n",
            "Train: epoch 0007 / 0100 | batch 0031 / 0050 | loss 0.0569\n",
            "Train: epoch 0007 / 0100 | batch 0032 / 0050 | loss 0.0569\n",
            "Train: epoch 0007 / 0100 | batch 0033 / 0050 | loss 0.0567\n",
            "Train: epoch 0007 / 0100 | batch 0034 / 0050 | loss 0.0569\n",
            "Train: epoch 0007 / 0100 | batch 0035 / 0050 | loss 0.0565\n",
            "Train: epoch 0007 / 0100 | batch 0036 / 0050 | loss 0.0570\n",
            "Train: epoch 0007 / 0100 | batch 0037 / 0050 | loss 0.0576\n",
            "Train: epoch 0007 / 0100 | batch 0038 / 0050 | loss 0.0576\n",
            "Train: epoch 0007 / 0100 | batch 0039 / 0050 | loss 0.0579\n",
            "Train: epoch 0007 / 0100 | batch 0040 / 0050 | loss 0.0580\n",
            "Train: epoch 0007 / 0100 | batch 0041 / 0050 | loss 0.0577\n",
            "Train: epoch 0007 / 0100 | batch 0042 / 0050 | loss 0.0585\n",
            "Train: epoch 0007 / 0100 | batch 0043 / 0050 | loss 0.0582\n",
            "Train: epoch 0007 / 0100 | batch 0044 / 0050 | loss 0.0581\n",
            "Train: epoch 0007 / 0100 | batch 0045 / 0050 | loss 0.0582\n",
            "Train: epoch 0007 / 0100 | batch 0046 / 0050 | loss 0.0582\n",
            "Train: epoch 0007 / 0100 | batch 0047 / 0050 | loss 0.0583\n",
            "Train: epoch 0007 / 0100 | batch 0048 / 0050 | loss 0.0584\n",
            "Train: epoch 0007 / 0100 | batch 0049 / 0050 | loss 0.0581\n",
            "Val loss 0.0436\n",
            "Dice score : 0.0191482063382864\n",
            "Val loss 0.0608\n",
            "Dice score : 0.027418920770287514\n",
            "Val loss 0.0564\n",
            "Dice score : 0.03261440619826317\n",
            "Val loss 0.0587\n",
            "Dice score : 0.04356816038489342\n",
            "Val loss 0.0621\n",
            "Dice score : 0.03008534200489521\n",
            "Val loss 0.0589\n",
            "Dice score : 0.03244511038064957\n",
            "Val loss 0.0594\n",
            "Dice score : 0.0279360543936491\n",
            "Val loss 0.0578\n",
            "Dice score : 0.033253662288188934\n",
            "Val loss 0.0577\n",
            "Dice score : 0.026650751009583473\n",
            "Val loss 0.0563\n",
            "Dice score : 0.019992820918560028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 7/100 [09:29<2:05:59, 81.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0008 / 0100 | batch 0000 / 0050 | loss 0.0450\n",
            "Train: epoch 0008 / 0100 | batch 0001 / 0050 | loss 0.0537\n",
            "Train: epoch 0008 / 0100 | batch 0002 / 0050 | loss 0.0562\n",
            "Train: epoch 0008 / 0100 | batch 0003 / 0050 | loss 0.0527\n",
            "Train: epoch 0008 / 0100 | batch 0004 / 0050 | loss 0.0522\n",
            "Train: epoch 0008 / 0100 | batch 0005 / 0050 | loss 0.0515\n",
            "Train: epoch 0008 / 0100 | batch 0006 / 0050 | loss 0.0519\n",
            "Train: epoch 0008 / 0100 | batch 0007 / 0050 | loss 0.0525\n",
            "Train: epoch 0008 / 0100 | batch 0008 / 0050 | loss 0.0541\n",
            "Train: epoch 0008 / 0100 | batch 0009 / 0050 | loss 0.0544\n",
            "Train: epoch 0008 / 0100 | batch 0010 / 0050 | loss 0.0552\n",
            "Train: epoch 0008 / 0100 | batch 0011 / 0050 | loss 0.0563\n",
            "Train: epoch 0008 / 0100 | batch 0012 / 0050 | loss 0.0554\n",
            "Train: epoch 0008 / 0100 | batch 0013 / 0050 | loss 0.0545\n",
            "Train: epoch 0008 / 0100 | batch 0014 / 0050 | loss 0.0540\n",
            "Train: epoch 0008 / 0100 | batch 0015 / 0050 | loss 0.0548\n",
            "Train: epoch 0008 / 0100 | batch 0016 / 0050 | loss 0.0542\n",
            "Train: epoch 0008 / 0100 | batch 0017 / 0050 | loss 0.0546\n",
            "Train: epoch 0008 / 0100 | batch 0018 / 0050 | loss 0.0541\n",
            "Train: epoch 0008 / 0100 | batch 0019 / 0050 | loss 0.0541\n",
            "Train: epoch 0008 / 0100 | batch 0020 / 0050 | loss 0.0537\n",
            "Train: epoch 0008 / 0100 | batch 0021 / 0050 | loss 0.0545\n",
            "Train: epoch 0008 / 0100 | batch 0022 / 0050 | loss 0.0540\n",
            "Train: epoch 0008 / 0100 | batch 0023 / 0050 | loss 0.0545\n",
            "Train: epoch 0008 / 0100 | batch 0024 / 0050 | loss 0.0550\n",
            "Train: epoch 0008 / 0100 | batch 0025 / 0050 | loss 0.0547\n",
            "Train: epoch 0008 / 0100 | batch 0026 / 0050 | loss 0.0560\n",
            "Train: epoch 0008 / 0100 | batch 0027 / 0050 | loss 0.0563\n",
            "Train: epoch 0008 / 0100 | batch 0028 / 0050 | loss 0.0569\n",
            "Train: epoch 0008 / 0100 | batch 0029 / 0050 | loss 0.0571\n",
            "Train: epoch 0008 / 0100 | batch 0030 / 0050 | loss 0.0574\n",
            "Train: epoch 0008 / 0100 | batch 0031 / 0050 | loss 0.0573\n",
            "Train: epoch 0008 / 0100 | batch 0032 / 0050 | loss 0.0570\n",
            "Train: epoch 0008 / 0100 | batch 0033 / 0050 | loss 0.0571\n",
            "Train: epoch 0008 / 0100 | batch 0034 / 0050 | loss 0.0574\n",
            "Train: epoch 0008 / 0100 | batch 0035 / 0050 | loss 0.0572\n",
            "Train: epoch 0008 / 0100 | batch 0036 / 0050 | loss 0.0569\n",
            "Train: epoch 0008 / 0100 | batch 0037 / 0050 | loss 0.0571\n",
            "Train: epoch 0008 / 0100 | batch 0038 / 0050 | loss 0.0578\n",
            "Train: epoch 0008 / 0100 | batch 0039 / 0050 | loss 0.0575\n",
            "Train: epoch 0008 / 0100 | batch 0040 / 0050 | loss 0.0574\n",
            "Train: epoch 0008 / 0100 | batch 0041 / 0050 | loss 0.0572\n",
            "Train: epoch 0008 / 0100 | batch 0042 / 0050 | loss 0.0574\n",
            "Train: epoch 0008 / 0100 | batch 0043 / 0050 | loss 0.0573\n",
            "Train: epoch 0008 / 0100 | batch 0044 / 0050 | loss 0.0571\n",
            "Train: epoch 0008 / 0100 | batch 0045 / 0050 | loss 0.0573\n",
            "Train: epoch 0008 / 0100 | batch 0046 / 0050 | loss 0.0574\n",
            "Train: epoch 0008 / 0100 | batch 0047 / 0050 | loss 0.0574\n",
            "Train: epoch 0008 / 0100 | batch 0048 / 0050 | loss 0.0573\n",
            "Train: epoch 0008 / 0100 | batch 0049 / 0050 | loss 0.0576\n",
            "Val loss 0.0513\n",
            "Dice score : 0.029828820377588272\n",
            "Val loss 0.0471\n",
            "Dice score : 0.02876455895602703\n",
            "Val loss 0.0497\n",
            "Dice score : 0.029574664309620857\n",
            "Val loss 0.0546\n",
            "Dice score : 0.04586378484964371\n",
            "Val loss 0.0538\n",
            "Dice score : 0.04183225706219673\n",
            "Val loss 0.0518\n",
            "Dice score : 0.036059074103832245\n",
            "Val loss 0.0571\n",
            "Dice score : 0.048060934990644455\n",
            "Val loss 0.0561\n",
            "Dice score : 0.00481074582785368\n",
            "Val loss 0.0548\n",
            "Dice score : 0.029881944879889488\n",
            "Val loss 0.0539\n",
            "Dice score : 0.042065877467393875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 8/100 [10:49<2:03:55, 80.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0009 / 0100 | batch 0000 / 0050 | loss 0.0457\n",
            "Train: epoch 0009 / 0100 | batch 0001 / 0050 | loss 0.0602\n",
            "Train: epoch 0009 / 0100 | batch 0002 / 0050 | loss 0.0552\n",
            "Train: epoch 0009 / 0100 | batch 0003 / 0050 | loss 0.0579\n",
            "Train: epoch 0009 / 0100 | batch 0004 / 0050 | loss 0.0593\n",
            "Train: epoch 0009 / 0100 | batch 0005 / 0050 | loss 0.0584\n",
            "Train: epoch 0009 / 0100 | batch 0006 / 0050 | loss 0.0601\n",
            "Train: epoch 0009 / 0100 | batch 0007 / 0050 | loss 0.0613\n",
            "Train: epoch 0009 / 0100 | batch 0008 / 0050 | loss 0.0619\n",
            "Train: epoch 0009 / 0100 | batch 0009 / 0050 | loss 0.0639\n",
            "Train: epoch 0009 / 0100 | batch 0010 / 0050 | loss 0.0630\n",
            "Train: epoch 0009 / 0100 | batch 0011 / 0050 | loss 0.0627\n",
            "Train: epoch 0009 / 0100 | batch 0012 / 0050 | loss 0.0616\n",
            "Train: epoch 0009 / 0100 | batch 0013 / 0050 | loss 0.0616\n",
            "Train: epoch 0009 / 0100 | batch 0014 / 0050 | loss 0.0607\n",
            "Train: epoch 0009 / 0100 | batch 0015 / 0050 | loss 0.0599\n",
            "Train: epoch 0009 / 0100 | batch 0016 / 0050 | loss 0.0594\n",
            "Train: epoch 0009 / 0100 | batch 0017 / 0050 | loss 0.0588\n",
            "Train: epoch 0009 / 0100 | batch 0018 / 0050 | loss 0.0579\n",
            "Train: epoch 0009 / 0100 | batch 0019 / 0050 | loss 0.0576\n",
            "Train: epoch 0009 / 0100 | batch 0020 / 0050 | loss 0.0569\n",
            "Train: epoch 0009 / 0100 | batch 0021 / 0050 | loss 0.0568\n",
            "Train: epoch 0009 / 0100 | batch 0022 / 0050 | loss 0.0572\n",
            "Train: epoch 0009 / 0100 | batch 0023 / 0050 | loss 0.0570\n",
            "Train: epoch 0009 / 0100 | batch 0024 / 0050 | loss 0.0568\n",
            "Train: epoch 0009 / 0100 | batch 0025 / 0050 | loss 0.0567\n",
            "Train: epoch 0009 / 0100 | batch 0026 / 0050 | loss 0.0568\n",
            "Train: epoch 0009 / 0100 | batch 0027 / 0050 | loss 0.0567\n",
            "Train: epoch 0009 / 0100 | batch 0028 / 0050 | loss 0.0564\n",
            "Train: epoch 0009 / 0100 | batch 0029 / 0050 | loss 0.0560\n",
            "Train: epoch 0009 / 0100 | batch 0030 / 0050 | loss 0.0560\n",
            "Train: epoch 0009 / 0100 | batch 0031 / 0050 | loss 0.0564\n",
            "Train: epoch 0009 / 0100 | batch 0032 / 0050 | loss 0.0561\n",
            "Train: epoch 0009 / 0100 | batch 0033 / 0050 | loss 0.0561\n",
            "Train: epoch 0009 / 0100 | batch 0034 / 0050 | loss 0.0566\n",
            "Train: epoch 0009 / 0100 | batch 0035 / 0050 | loss 0.0563\n",
            "Train: epoch 0009 / 0100 | batch 0036 / 0050 | loss 0.0560\n",
            "Train: epoch 0009 / 0100 | batch 0037 / 0050 | loss 0.0559\n",
            "Train: epoch 0009 / 0100 | batch 0038 / 0050 | loss 0.0555\n",
            "Train: epoch 0009 / 0100 | batch 0039 / 0050 | loss 0.0554\n",
            "Train: epoch 0009 / 0100 | batch 0040 / 0050 | loss 0.0556\n",
            "Train: epoch 0009 / 0100 | batch 0041 / 0050 | loss 0.0558\n",
            "Train: epoch 0009 / 0100 | batch 0042 / 0050 | loss 0.0558\n",
            "Train: epoch 0009 / 0100 | batch 0043 / 0050 | loss 0.0561\n",
            "Train: epoch 0009 / 0100 | batch 0044 / 0050 | loss 0.0563\n",
            "Train: epoch 0009 / 0100 | batch 0045 / 0050 | loss 0.0564\n",
            "Train: epoch 0009 / 0100 | batch 0046 / 0050 | loss 0.0567\n",
            "Train: epoch 0009 / 0100 | batch 0047 / 0050 | loss 0.0568\n",
            "Train: epoch 0009 / 0100 | batch 0048 / 0050 | loss 0.0565\n",
            "Train: epoch 0009 / 0100 | batch 0049 / 0050 | loss 0.0564\n",
            "Val loss 0.0737\n",
            "Dice score : 0.013881029561161995\n",
            "Val loss 0.0587\n",
            "Dice score : 0.026002204045653343\n",
            "Val loss 0.0533\n",
            "Dice score : 0.02562093734741211\n",
            "Val loss 0.0628\n",
            "Dice score : 0.04309678077697754\n",
            "Val loss 0.0666\n",
            "Dice score : 0.030678823590278625\n",
            "Val loss 0.0651\n",
            "Dice score : 0.0318671278655529\n",
            "Val loss 0.0632\n",
            "Dice score : 0.02180337905883789\n",
            "Val loss 0.0613\n",
            "Dice score : 0.025127440690994263\n",
            "Val loss 0.0594\n",
            "Dice score : 0.021965228021144867\n",
            "Val loss 0.0582\n",
            "Dice score : 0.020067982375621796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 9/100 [12:09<2:02:35, 80.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0010 / 0100 | batch 0000 / 0050 | loss 0.0709\n",
            "Train: epoch 0010 / 0100 | batch 0001 / 0050 | loss 0.0746\n",
            "Train: epoch 0010 / 0100 | batch 0002 / 0050 | loss 0.0672\n",
            "Train: epoch 0010 / 0100 | batch 0003 / 0050 | loss 0.0633\n",
            "Train: epoch 0010 / 0100 | batch 0004 / 0050 | loss 0.0595\n",
            "Train: epoch 0010 / 0100 | batch 0005 / 0050 | loss 0.0564\n",
            "Train: epoch 0010 / 0100 | batch 0006 / 0050 | loss 0.0568\n",
            "Train: epoch 0010 / 0100 | batch 0007 / 0050 | loss 0.0554\n",
            "Train: epoch 0010 / 0100 | batch 0008 / 0050 | loss 0.0568\n",
            "Train: epoch 0010 / 0100 | batch 0009 / 0050 | loss 0.0554\n",
            "Train: epoch 0010 / 0100 | batch 0010 / 0050 | loss 0.0561\n",
            "Train: epoch 0010 / 0100 | batch 0011 / 0050 | loss 0.0581\n",
            "Train: epoch 0010 / 0100 | batch 0012 / 0050 | loss 0.0571\n",
            "Train: epoch 0010 / 0100 | batch 0013 / 0050 | loss 0.0571\n",
            "Train: epoch 0010 / 0100 | batch 0014 / 0050 | loss 0.0576\n",
            "Train: epoch 0010 / 0100 | batch 0015 / 0050 | loss 0.0579\n",
            "Train: epoch 0010 / 0100 | batch 0016 / 0050 | loss 0.0589\n",
            "Train: epoch 0010 / 0100 | batch 0017 / 0050 | loss 0.0584\n",
            "Train: epoch 0010 / 0100 | batch 0018 / 0050 | loss 0.0585\n",
            "Train: epoch 0010 / 0100 | batch 0019 / 0050 | loss 0.0576\n",
            "Train: epoch 0010 / 0100 | batch 0020 / 0050 | loss 0.0572\n",
            "Train: epoch 0010 / 0100 | batch 0021 / 0050 | loss 0.0565\n",
            "Train: epoch 0010 / 0100 | batch 0022 / 0050 | loss 0.0563\n",
            "Train: epoch 0010 / 0100 | batch 0023 / 0050 | loss 0.0567\n",
            "Train: epoch 0010 / 0100 | batch 0024 / 0050 | loss 0.0565\n",
            "Train: epoch 0010 / 0100 | batch 0025 / 0050 | loss 0.0566\n",
            "Train: epoch 0010 / 0100 | batch 0026 / 0050 | loss 0.0569\n",
            "Train: epoch 0010 / 0100 | batch 0027 / 0050 | loss 0.0564\n",
            "Train: epoch 0010 / 0100 | batch 0028 / 0050 | loss 0.0562\n",
            "Train: epoch 0010 / 0100 | batch 0029 / 0050 | loss 0.0566\n",
            "Train: epoch 0010 / 0100 | batch 0030 / 0050 | loss 0.0562\n",
            "Train: epoch 0010 / 0100 | batch 0031 / 0050 | loss 0.0561\n",
            "Train: epoch 0010 / 0100 | batch 0032 / 0050 | loss 0.0556\n",
            "Train: epoch 0010 / 0100 | batch 0033 / 0050 | loss 0.0566\n",
            "Train: epoch 0010 / 0100 | batch 0034 / 0050 | loss 0.0572\n",
            "Train: epoch 0010 / 0100 | batch 0035 / 0050 | loss 0.0567\n",
            "Train: epoch 0010 / 0100 | batch 0036 / 0050 | loss 0.0565\n",
            "Train: epoch 0010 / 0100 | batch 0037 / 0050 | loss 0.0565\n",
            "Train: epoch 0010 / 0100 | batch 0038 / 0050 | loss 0.0565\n",
            "Train: epoch 0010 / 0100 | batch 0039 / 0050 | loss 0.0563\n",
            "Train: epoch 0010 / 0100 | batch 0040 / 0050 | loss 0.0564\n",
            "Train: epoch 0010 / 0100 | batch 0041 / 0050 | loss 0.0561\n",
            "Train: epoch 0010 / 0100 | batch 0042 / 0050 | loss 0.0559\n",
            "Train: epoch 0010 / 0100 | batch 0043 / 0050 | loss 0.0557\n",
            "Train: epoch 0010 / 0100 | batch 0044 / 0050 | loss 0.0557\n",
            "Train: epoch 0010 / 0100 | batch 0045 / 0050 | loss 0.0555\n",
            "Train: epoch 0010 / 0100 | batch 0046 / 0050 | loss 0.0553\n",
            "Train: epoch 0010 / 0100 | batch 0047 / 0050 | loss 0.0553\n",
            "Train: epoch 0010 / 0100 | batch 0048 / 0050 | loss 0.0553\n",
            "Train: epoch 0010 / 0100 | batch 0049 / 0050 | loss 0.0553\n",
            "Val loss 0.0512\n",
            "Dice score : 0.031029030680656433\n",
            "Val loss 0.0502\n",
            "Dice score : 0.027739688754081726\n",
            "Val loss 0.0490\n",
            "Dice score : 0.0267060287296772\n",
            "Val loss 0.0511\n",
            "Dice score : 0.031773585826158524\n",
            "Val loss 0.0500\n",
            "Dice score : 0.041413430124521255\n",
            "Val loss 0.0515\n",
            "Dice score : 0.03533202409744263\n",
            "Val loss 0.0552\n",
            "Dice score : 0.046822283416986465\n",
            "Val loss 0.0537\n",
            "Dice score : 0.023294737562537193\n",
            "Val loss 0.0568\n",
            "Dice score : 0.04842215031385422\n",
            "Val loss 0.0580\n",
            "Dice score : 0.03353390470147133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 10/100 [13:30<2:00:55, 80.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0011 / 0100 | batch 0000 / 0050 | loss 0.0458\n",
            "Train: epoch 0011 / 0100 | batch 0001 / 0050 | loss 0.0509\n",
            "Train: epoch 0011 / 0100 | batch 0002 / 0050 | loss 0.0557\n",
            "Train: epoch 0011 / 0100 | batch 0003 / 0050 | loss 0.0554\n",
            "Train: epoch 0011 / 0100 | batch 0004 / 0050 | loss 0.0536\n",
            "Train: epoch 0011 / 0100 | batch 0005 / 0050 | loss 0.0560\n",
            "Train: epoch 0011 / 0100 | batch 0006 / 0050 | loss 0.0575\n",
            "Train: epoch 0011 / 0100 | batch 0007 / 0050 | loss 0.0564\n",
            "Train: epoch 0011 / 0100 | batch 0008 / 0050 | loss 0.0554\n",
            "Train: epoch 0011 / 0100 | batch 0009 / 0050 | loss 0.0555\n",
            "Train: epoch 0011 / 0100 | batch 0010 / 0050 | loss 0.0544\n",
            "Train: epoch 0011 / 0100 | batch 0011 / 0050 | loss 0.0535\n",
            "Train: epoch 0011 / 0100 | batch 0012 / 0050 | loss 0.0525\n",
            "Train: epoch 0011 / 0100 | batch 0013 / 0050 | loss 0.0520\n",
            "Train: epoch 0011 / 0100 | batch 0014 / 0050 | loss 0.0514\n",
            "Train: epoch 0011 / 0100 | batch 0015 / 0050 | loss 0.0521\n",
            "Train: epoch 0011 / 0100 | batch 0016 / 0050 | loss 0.0517\n",
            "Train: epoch 0011 / 0100 | batch 0017 / 0050 | loss 0.0525\n",
            "Train: epoch 0011 / 0100 | batch 0018 / 0050 | loss 0.0528\n",
            "Train: epoch 0011 / 0100 | batch 0019 / 0050 | loss 0.0523\n",
            "Train: epoch 0011 / 0100 | batch 0020 / 0050 | loss 0.0519\n",
            "Train: epoch 0011 / 0100 | batch 0021 / 0050 | loss 0.0520\n",
            "Train: epoch 0011 / 0100 | batch 0022 / 0050 | loss 0.0527\n",
            "Train: epoch 0011 / 0100 | batch 0023 / 0050 | loss 0.0525\n",
            "Train: epoch 0011 / 0100 | batch 0024 / 0050 | loss 0.0536\n",
            "Train: epoch 0011 / 0100 | batch 0025 / 0050 | loss 0.0540\n",
            "Train: epoch 0011 / 0100 | batch 0026 / 0050 | loss 0.0538\n",
            "Train: epoch 0011 / 0100 | batch 0027 / 0050 | loss 0.0534\n",
            "Train: epoch 0011 / 0100 | batch 0028 / 0050 | loss 0.0536\n",
            "Train: epoch 0011 / 0100 | batch 0029 / 0050 | loss 0.0539\n",
            "Train: epoch 0011 / 0100 | batch 0030 / 0050 | loss 0.0534\n",
            "Train: epoch 0011 / 0100 | batch 0031 / 0050 | loss 0.0537\n",
            "Train: epoch 0011 / 0100 | batch 0032 / 0050 | loss 0.0534\n",
            "Train: epoch 0011 / 0100 | batch 0033 / 0050 | loss 0.0534\n",
            "Train: epoch 0011 / 0100 | batch 0034 / 0050 | loss 0.0532\n",
            "Train: epoch 0011 / 0100 | batch 0035 / 0050 | loss 0.0531\n",
            "Train: epoch 0011 / 0100 | batch 0036 / 0050 | loss 0.0528\n",
            "Train: epoch 0011 / 0100 | batch 0037 / 0050 | loss 0.0525\n",
            "Train: epoch 0011 / 0100 | batch 0038 / 0050 | loss 0.0526\n",
            "Train: epoch 0011 / 0100 | batch 0039 / 0050 | loss 0.0528\n",
            "Train: epoch 0011 / 0100 | batch 0040 / 0050 | loss 0.0531\n",
            "Train: epoch 0011 / 0100 | batch 0041 / 0050 | loss 0.0539\n",
            "Train: epoch 0011 / 0100 | batch 0042 / 0050 | loss 0.0537\n",
            "Train: epoch 0011 / 0100 | batch 0043 / 0050 | loss 0.0544\n",
            "Train: epoch 0011 / 0100 | batch 0044 / 0050 | loss 0.0546\n",
            "Train: epoch 0011 / 0100 | batch 0045 / 0050 | loss 0.0546\n",
            "Train: epoch 0011 / 0100 | batch 0046 / 0050 | loss 0.0548\n",
            "Train: epoch 0011 / 0100 | batch 0047 / 0050 | loss 0.0550\n",
            "Train: epoch 0011 / 0100 | batch 0048 / 0050 | loss 0.0554\n",
            "Train: epoch 0011 / 0100 | batch 0049 / 0050 | loss 0.0552\n",
            "Val loss 0.0523\n",
            "Dice score : 0.024869736284017563\n",
            "Val loss 0.0491\n",
            "Dice score : 0.022163771092891693\n",
            "Val loss 0.0525\n",
            "Dice score : 0.018371708691120148\n",
            "Val loss 0.0506\n",
            "Dice score : 0.01540165115147829\n",
            "Val loss 0.0587\n",
            "Dice score : 0.04077601432800293\n",
            "Val loss 0.0609\n",
            "Dice score : 0.04043731465935707\n",
            "Val loss 0.0615\n",
            "Dice score : 0.024037031456828117\n",
            "Val loss 0.0647\n",
            "Dice score : 0.04509539157152176\n",
            "Val loss 0.0632\n",
            "Dice score : 0.027646664530038834\n",
            "Val loss 0.0618\n",
            "Dice score : 0.02728830650448799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 11/100 [14:49<1:59:15, 80.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0012 / 0100 | batch 0000 / 0050 | loss 0.0544\n",
            "Train: epoch 0012 / 0100 | batch 0001 / 0050 | loss 0.0577\n",
            "Train: epoch 0012 / 0100 | batch 0002 / 0050 | loss 0.0534\n",
            "Train: epoch 0012 / 0100 | batch 0003 / 0050 | loss 0.0563\n",
            "Train: epoch 0012 / 0100 | batch 0004 / 0050 | loss 0.0551\n",
            "Train: epoch 0012 / 0100 | batch 0005 / 0050 | loss 0.0547\n",
            "Train: epoch 0012 / 0100 | batch 0006 / 0050 | loss 0.0554\n",
            "Train: epoch 0012 / 0100 | batch 0007 / 0050 | loss 0.0537\n",
            "Train: epoch 0012 / 0100 | batch 0008 / 0050 | loss 0.0520\n",
            "Train: epoch 0012 / 0100 | batch 0009 / 0050 | loss 0.0519\n",
            "Train: epoch 0012 / 0100 | batch 0010 / 0050 | loss 0.0529\n",
            "Train: epoch 0012 / 0100 | batch 0011 / 0050 | loss 0.0536\n",
            "Train: epoch 0012 / 0100 | batch 0012 / 0050 | loss 0.0538\n",
            "Train: epoch 0012 / 0100 | batch 0013 / 0050 | loss 0.0548\n",
            "Train: epoch 0012 / 0100 | batch 0014 / 0050 | loss 0.0542\n",
            "Train: epoch 0012 / 0100 | batch 0015 / 0050 | loss 0.0546\n",
            "Train: epoch 0012 / 0100 | batch 0016 / 0050 | loss 0.0541\n",
            "Train: epoch 0012 / 0100 | batch 0017 / 0050 | loss 0.0533\n",
            "Train: epoch 0012 / 0100 | batch 0018 / 0050 | loss 0.0538\n",
            "Train: epoch 0012 / 0100 | batch 0019 / 0050 | loss 0.0546\n",
            "Train: epoch 0012 / 0100 | batch 0020 / 0050 | loss 0.0543\n",
            "Train: epoch 0012 / 0100 | batch 0021 / 0050 | loss 0.0551\n",
            "Train: epoch 0012 / 0100 | batch 0022 / 0050 | loss 0.0544\n",
            "Train: epoch 0012 / 0100 | batch 0023 / 0050 | loss 0.0550\n",
            "Train: epoch 0012 / 0100 | batch 0024 / 0050 | loss 0.0545\n",
            "Train: epoch 0012 / 0100 | batch 0025 / 0050 | loss 0.0540\n",
            "Train: epoch 0012 / 0100 | batch 0026 / 0050 | loss 0.0539\n",
            "Train: epoch 0012 / 0100 | batch 0027 / 0050 | loss 0.0541\n",
            "Train: epoch 0012 / 0100 | batch 0028 / 0050 | loss 0.0537\n",
            "Train: epoch 0012 / 0100 | batch 0029 / 0050 | loss 0.0533\n",
            "Train: epoch 0012 / 0100 | batch 0030 / 0050 | loss 0.0531\n",
            "Train: epoch 0012 / 0100 | batch 0031 / 0050 | loss 0.0532\n",
            "Train: epoch 0012 / 0100 | batch 0032 / 0050 | loss 0.0540\n",
            "Train: epoch 0012 / 0100 | batch 0033 / 0050 | loss 0.0538\n",
            "Train: epoch 0012 / 0100 | batch 0034 / 0050 | loss 0.0540\n",
            "Train: epoch 0012 / 0100 | batch 0035 / 0050 | loss 0.0536\n",
            "Train: epoch 0012 / 0100 | batch 0036 / 0050 | loss 0.0541\n",
            "Train: epoch 0012 / 0100 | batch 0037 / 0050 | loss 0.0544\n",
            "Train: epoch 0012 / 0100 | batch 0038 / 0050 | loss 0.0543\n",
            "Train: epoch 0012 / 0100 | batch 0039 / 0050 | loss 0.0547\n",
            "Train: epoch 0012 / 0100 | batch 0040 / 0050 | loss 0.0544\n",
            "Train: epoch 0012 / 0100 | batch 0041 / 0050 | loss 0.0547\n",
            "Train: epoch 0012 / 0100 | batch 0042 / 0050 | loss 0.0544\n",
            "Train: epoch 0012 / 0100 | batch 0043 / 0050 | loss 0.0545\n",
            "Train: epoch 0012 / 0100 | batch 0044 / 0050 | loss 0.0544\n",
            "Train: epoch 0012 / 0100 | batch 0045 / 0050 | loss 0.0548\n",
            "Train: epoch 0012 / 0100 | batch 0046 / 0050 | loss 0.0550\n",
            "Train: epoch 0012 / 0100 | batch 0047 / 0050 | loss 0.0548\n",
            "Train: epoch 0012 / 0100 | batch 0048 / 0050 | loss 0.0546\n",
            "Train: epoch 0012 / 0100 | batch 0049 / 0050 | loss 0.0543\n",
            "Val loss 0.0405\n",
            "Dice score : 0.026458758860826492\n",
            "Val loss 0.0427\n",
            "Dice score : 0.028331346809864044\n",
            "Val loss 0.0414\n",
            "Dice score : 0.025200115516781807\n",
            "Val loss 0.0458\n",
            "Dice score : 0.03588235378265381\n",
            "Val loss 0.0489\n",
            "Dice score : 0.04314997047185898\n",
            "Val loss 0.0468\n",
            "Dice score : 0.028883734717965126\n",
            "Val loss 0.0525\n",
            "Dice score : 0.05279232934117317\n",
            "Val loss 0.0514\n",
            "Dice score : 0.05291273444890976\n",
            "Val loss 0.0499\n",
            "Dice score : 0.03461518511176109\n",
            "Val loss 0.0524\n",
            "Dice score : 0.04455602914094925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 12/100 [16:10<1:58:09, 80.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0013 / 0100 | batch 0000 / 0050 | loss 0.0558\n",
            "Train: epoch 0013 / 0100 | batch 0001 / 0050 | loss 0.0492\n",
            "Train: epoch 0013 / 0100 | batch 0002 / 0050 | loss 0.0472\n",
            "Train: epoch 0013 / 0100 | batch 0003 / 0050 | loss 0.0482\n",
            "Train: epoch 0013 / 0100 | batch 0004 / 0050 | loss 0.0487\n",
            "Train: epoch 0013 / 0100 | batch 0005 / 0050 | loss 0.0463\n",
            "Train: epoch 0013 / 0100 | batch 0006 / 0050 | loss 0.0489\n",
            "Train: epoch 0013 / 0100 | batch 0007 / 0050 | loss 0.0493\n",
            "Train: epoch 0013 / 0100 | batch 0008 / 0050 | loss 0.0507\n",
            "Train: epoch 0013 / 0100 | batch 0009 / 0050 | loss 0.0514\n",
            "Train: epoch 0013 / 0100 | batch 0010 / 0050 | loss 0.0522\n",
            "Train: epoch 0013 / 0100 | batch 0011 / 0050 | loss 0.0514\n",
            "Train: epoch 0013 / 0100 | batch 0012 / 0050 | loss 0.0511\n",
            "Train: epoch 0013 / 0100 | batch 0013 / 0050 | loss 0.0515\n",
            "Train: epoch 0013 / 0100 | batch 0014 / 0050 | loss 0.0536\n",
            "Train: epoch 0013 / 0100 | batch 0015 / 0050 | loss 0.0529\n",
            "Train: epoch 0013 / 0100 | batch 0016 / 0050 | loss 0.0535\n",
            "Train: epoch 0013 / 0100 | batch 0017 / 0050 | loss 0.0532\n",
            "Train: epoch 0013 / 0100 | batch 0018 / 0050 | loss 0.0536\n",
            "Train: epoch 0013 / 0100 | batch 0019 / 0050 | loss 0.0536\n",
            "Train: epoch 0013 / 0100 | batch 0020 / 0050 | loss 0.0529\n",
            "Train: epoch 0013 / 0100 | batch 0021 / 0050 | loss 0.0530\n",
            "Train: epoch 0013 / 0100 | batch 0022 / 0050 | loss 0.0528\n",
            "Train: epoch 0013 / 0100 | batch 0023 / 0050 | loss 0.0530\n",
            "Train: epoch 0013 / 0100 | batch 0024 / 0050 | loss 0.0525\n",
            "Train: epoch 0013 / 0100 | batch 0025 / 0050 | loss 0.0523\n",
            "Train: epoch 0013 / 0100 | batch 0026 / 0050 | loss 0.0525\n",
            "Train: epoch 0013 / 0100 | batch 0027 / 0050 | loss 0.0521\n",
            "Train: epoch 0013 / 0100 | batch 0028 / 0050 | loss 0.0522\n",
            "Train: epoch 0013 / 0100 | batch 0029 / 0050 | loss 0.0530\n",
            "Train: epoch 0013 / 0100 | batch 0030 / 0050 | loss 0.0531\n",
            "Train: epoch 0013 / 0100 | batch 0031 / 0050 | loss 0.0529\n",
            "Train: epoch 0013 / 0100 | batch 0032 / 0050 | loss 0.0531\n",
            "Train: epoch 0013 / 0100 | batch 0033 / 0050 | loss 0.0525\n",
            "Train: epoch 0013 / 0100 | batch 0034 / 0050 | loss 0.0522\n",
            "Train: epoch 0013 / 0100 | batch 0035 / 0050 | loss 0.0521\n",
            "Train: epoch 0013 / 0100 | batch 0036 / 0050 | loss 0.0526\n",
            "Train: epoch 0013 / 0100 | batch 0037 / 0050 | loss 0.0533\n",
            "Train: epoch 0013 / 0100 | batch 0038 / 0050 | loss 0.0530\n",
            "Train: epoch 0013 / 0100 | batch 0039 / 0050 | loss 0.0528\n",
            "Train: epoch 0013 / 0100 | batch 0040 / 0050 | loss 0.0533\n",
            "Train: epoch 0013 / 0100 | batch 0041 / 0050 | loss 0.0529\n",
            "Train: epoch 0013 / 0100 | batch 0042 / 0050 | loss 0.0529\n",
            "Train: epoch 0013 / 0100 | batch 0043 / 0050 | loss 0.0529\n",
            "Train: epoch 0013 / 0100 | batch 0044 / 0050 | loss 0.0532\n",
            "Train: epoch 0013 / 0100 | batch 0045 / 0050 | loss 0.0529\n",
            "Train: epoch 0013 / 0100 | batch 0046 / 0050 | loss 0.0534\n",
            "Train: epoch 0013 / 0100 | batch 0047 / 0050 | loss 0.0537\n",
            "Train: epoch 0013 / 0100 | batch 0048 / 0050 | loss 0.0535\n",
            "Train: epoch 0013 / 0100 | batch 0049 / 0050 | loss 0.0544\n",
            "Val loss 0.0330\n",
            "Dice score : 0.02880672924220562\n",
            "Val loss 0.0420\n",
            "Dice score : 0.03592011332511902\n",
            "Val loss 0.0441\n",
            "Dice score : 0.03914967179298401\n",
            "Val loss 0.0469\n",
            "Dice score : 0.03968048840761185\n",
            "Val loss 0.0466\n",
            "Dice score : 0.045350994914770126\n",
            "Val loss 0.0504\n",
            "Dice score : 0.054326873272657394\n",
            "Val loss 0.0511\n",
            "Dice score : 0.04207080975174904\n",
            "Val loss 0.0508\n",
            "Dice score : 0.02980721741914749\n",
            "Val loss 0.0503\n",
            "Dice score : 0.02626572921872139\n",
            "Val loss 0.0523\n",
            "Dice score : 0.04966593533754349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 13/100 [17:30<1:56:33, 80.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0014 / 0100 | batch 0000 / 0050 | loss 0.0458\n",
            "Train: epoch 0014 / 0100 | batch 0001 / 0050 | loss 0.0452\n",
            "Train: epoch 0014 / 0100 | batch 0002 / 0050 | loss 0.0443\n",
            "Train: epoch 0014 / 0100 | batch 0003 / 0050 | loss 0.0482\n",
            "Train: epoch 0014 / 0100 | batch 0004 / 0050 | loss 0.0478\n",
            "Train: epoch 0014 / 0100 | batch 0005 / 0050 | loss 0.0461\n",
            "Train: epoch 0014 / 0100 | batch 0006 / 0050 | loss 0.0449\n",
            "Train: epoch 0014 / 0100 | batch 0007 / 0050 | loss 0.0465\n",
            "Train: epoch 0014 / 0100 | batch 0008 / 0050 | loss 0.0503\n",
            "Train: epoch 0014 / 0100 | batch 0009 / 0050 | loss 0.0497\n",
            "Train: epoch 0014 / 0100 | batch 0010 / 0050 | loss 0.0507\n",
            "Train: epoch 0014 / 0100 | batch 0011 / 0050 | loss 0.0514\n",
            "Train: epoch 0014 / 0100 | batch 0012 / 0050 | loss 0.0511\n",
            "Train: epoch 0014 / 0100 | batch 0013 / 0050 | loss 0.0516\n",
            "Train: epoch 0014 / 0100 | batch 0014 / 0050 | loss 0.0537\n",
            "Train: epoch 0014 / 0100 | batch 0015 / 0050 | loss 0.0537\n",
            "Train: epoch 0014 / 0100 | batch 0016 / 0050 | loss 0.0545\n",
            "Train: epoch 0014 / 0100 | batch 0017 / 0050 | loss 0.0546\n",
            "Train: epoch 0014 / 0100 | batch 0018 / 0050 | loss 0.0544\n",
            "Train: epoch 0014 / 0100 | batch 0019 / 0050 | loss 0.0539\n",
            "Train: epoch 0014 / 0100 | batch 0020 / 0050 | loss 0.0536\n",
            "Train: epoch 0014 / 0100 | batch 0021 / 0050 | loss 0.0529\n",
            "Train: epoch 0014 / 0100 | batch 0022 / 0050 | loss 0.0527\n",
            "Train: epoch 0014 / 0100 | batch 0023 / 0050 | loss 0.0526\n",
            "Train: epoch 0014 / 0100 | batch 0024 / 0050 | loss 0.0545\n",
            "Train: epoch 0014 / 0100 | batch 0025 / 0050 | loss 0.0541\n",
            "Train: epoch 0014 / 0100 | batch 0026 / 0050 | loss 0.0537\n",
            "Train: epoch 0014 / 0100 | batch 0027 / 0050 | loss 0.0541\n",
            "Train: epoch 0014 / 0100 | batch 0028 / 0050 | loss 0.0539\n",
            "Train: epoch 0014 / 0100 | batch 0029 / 0050 | loss 0.0540\n",
            "Train: epoch 0014 / 0100 | batch 0030 / 0050 | loss 0.0536\n",
            "Train: epoch 0014 / 0100 | batch 0031 / 0050 | loss 0.0536\n",
            "Train: epoch 0014 / 0100 | batch 0032 / 0050 | loss 0.0531\n",
            "Train: epoch 0014 / 0100 | batch 0033 / 0050 | loss 0.0530\n",
            "Train: epoch 0014 / 0100 | batch 0034 / 0050 | loss 0.0538\n",
            "Train: epoch 0014 / 0100 | batch 0035 / 0050 | loss 0.0540\n",
            "Train: epoch 0014 / 0100 | batch 0036 / 0050 | loss 0.0543\n",
            "Train: epoch 0014 / 0100 | batch 0037 / 0050 | loss 0.0540\n",
            "Train: epoch 0014 / 0100 | batch 0038 / 0050 | loss 0.0540\n",
            "Train: epoch 0014 / 0100 | batch 0039 / 0050 | loss 0.0538\n",
            "Train: epoch 0014 / 0100 | batch 0040 / 0050 | loss 0.0540\n",
            "Train: epoch 0014 / 0100 | batch 0041 / 0050 | loss 0.0539\n",
            "Train: epoch 0014 / 0100 | batch 0042 / 0050 | loss 0.0538\n",
            "Train: epoch 0014 / 0100 | batch 0043 / 0050 | loss 0.0534\n",
            "Train: epoch 0014 / 0100 | batch 0044 / 0050 | loss 0.0532\n",
            "Train: epoch 0014 / 0100 | batch 0045 / 0050 | loss 0.0532\n",
            "Train: epoch 0014 / 0100 | batch 0046 / 0050 | loss 0.0533\n",
            "Train: epoch 0014 / 0100 | batch 0047 / 0050 | loss 0.0535\n",
            "Train: epoch 0014 / 0100 | batch 0048 / 0050 | loss 0.0538\n",
            "Train: epoch 0014 / 0100 | batch 0049 / 0050 | loss 0.0536\n",
            "Val loss 0.0622\n",
            "Dice score : 0.03898661211133003\n",
            "Val loss 0.0491\n",
            "Dice score : 0.03195412456989288\n",
            "Val loss 0.0578\n",
            "Dice score : 0.04887941852211952\n",
            "Val loss 0.0532\n",
            "Dice score : 0.041702087968587875\n",
            "Val loss 0.0506\n",
            "Dice score : 0.035317156463861465\n",
            "Val loss 0.0537\n",
            "Dice score : 0.04806976020336151\n",
            "Val loss 0.0511\n",
            "Dice score : 0.025951337069272995\n",
            "Val loss 0.0494\n",
            "Dice score : 0.05218377336859703\n",
            "Val loss 0.0496\n",
            "Dice score : 0.036758653819561005\n",
            "Val loss 0.0500\n",
            "Dice score : 0.03747318312525749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 14/100 [18:52<1:55:49, 80.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0015 / 0100 | batch 0000 / 0050 | loss 0.0523\n",
            "Train: epoch 0015 / 0100 | batch 0001 / 0050 | loss 0.0502\n",
            "Train: epoch 0015 / 0100 | batch 0002 / 0050 | loss 0.0526\n",
            "Train: epoch 0015 / 0100 | batch 0003 / 0050 | loss 0.0565\n",
            "Train: epoch 0015 / 0100 | batch 0004 / 0050 | loss 0.0556\n",
            "Train: epoch 0015 / 0100 | batch 0005 / 0050 | loss 0.0555\n",
            "Train: epoch 0015 / 0100 | batch 0006 / 0050 | loss 0.0549\n",
            "Train: epoch 0015 / 0100 | batch 0007 / 0050 | loss 0.0554\n",
            "Train: epoch 0015 / 0100 | batch 0008 / 0050 | loss 0.0562\n",
            "Train: epoch 0015 / 0100 | batch 0009 / 0050 | loss 0.0550\n",
            "Train: epoch 0015 / 0100 | batch 0010 / 0050 | loss 0.0544\n",
            "Train: epoch 0015 / 0100 | batch 0011 / 0050 | loss 0.0546\n",
            "Train: epoch 0015 / 0100 | batch 0012 / 0050 | loss 0.0550\n",
            "Train: epoch 0015 / 0100 | batch 0013 / 0050 | loss 0.0544\n",
            "Train: epoch 0015 / 0100 | batch 0014 / 0050 | loss 0.0535\n",
            "Train: epoch 0015 / 0100 | batch 0015 / 0050 | loss 0.0531\n",
            "Train: epoch 0015 / 0100 | batch 0016 / 0050 | loss 0.0533\n",
            "Train: epoch 0015 / 0100 | batch 0017 / 0050 | loss 0.0536\n",
            "Train: epoch 0015 / 0100 | batch 0018 / 0050 | loss 0.0534\n",
            "Train: epoch 0015 / 0100 | batch 0019 / 0050 | loss 0.0529\n",
            "Train: epoch 0015 / 0100 | batch 0020 / 0050 | loss 0.0527\n",
            "Train: epoch 0015 / 0100 | batch 0021 / 0050 | loss 0.0534\n",
            "Train: epoch 0015 / 0100 | batch 0022 / 0050 | loss 0.0537\n",
            "Train: epoch 0015 / 0100 | batch 0023 / 0050 | loss 0.0541\n",
            "Train: epoch 0015 / 0100 | batch 0024 / 0050 | loss 0.0534\n",
            "Train: epoch 0015 / 0100 | batch 0025 / 0050 | loss 0.0530\n",
            "Train: epoch 0015 / 0100 | batch 0026 / 0050 | loss 0.0534\n",
            "Train: epoch 0015 / 0100 | batch 0027 / 0050 | loss 0.0535\n",
            "Train: epoch 0015 / 0100 | batch 0028 / 0050 | loss 0.0535\n",
            "Train: epoch 0015 / 0100 | batch 0029 / 0050 | loss 0.0534\n",
            "Train: epoch 0015 / 0100 | batch 0030 / 0050 | loss 0.0546\n",
            "Train: epoch 0015 / 0100 | batch 0031 / 0050 | loss 0.0540\n",
            "Train: epoch 0015 / 0100 | batch 0032 / 0050 | loss 0.0534\n",
            "Train: epoch 0015 / 0100 | batch 0033 / 0050 | loss 0.0535\n",
            "Train: epoch 0015 / 0100 | batch 0034 / 0050 | loss 0.0543\n",
            "Train: epoch 0015 / 0100 | batch 0035 / 0050 | loss 0.0538\n",
            "Train: epoch 0015 / 0100 | batch 0036 / 0050 | loss 0.0543\n",
            "Train: epoch 0015 / 0100 | batch 0037 / 0050 | loss 0.0545\n",
            "Train: epoch 0015 / 0100 | batch 0038 / 0050 | loss 0.0541\n",
            "Train: epoch 0015 / 0100 | batch 0039 / 0050 | loss 0.0540\n",
            "Train: epoch 0015 / 0100 | batch 0040 / 0050 | loss 0.0537\n",
            "Train: epoch 0015 / 0100 | batch 0041 / 0050 | loss 0.0537\n",
            "Train: epoch 0015 / 0100 | batch 0042 / 0050 | loss 0.0536\n",
            "Train: epoch 0015 / 0100 | batch 0043 / 0050 | loss 0.0537\n",
            "Train: epoch 0015 / 0100 | batch 0044 / 0050 | loss 0.0534\n",
            "Train: epoch 0015 / 0100 | batch 0045 / 0050 | loss 0.0532\n",
            "Train: epoch 0015 / 0100 | batch 0046 / 0050 | loss 0.0529\n",
            "Train: epoch 0015 / 0100 | batch 0047 / 0050 | loss 0.0538\n",
            "Train: epoch 0015 / 0100 | batch 0048 / 0050 | loss 0.0537\n",
            "Train: epoch 0015 / 0100 | batch 0049 / 0050 | loss 0.0534\n",
            "Val loss 0.0475\n",
            "Dice score : 0.04227958247065544\n",
            "Val loss 0.0600\n",
            "Dice score : 0.04014361649751663\n",
            "Val loss 0.0555\n",
            "Dice score : 0.0411430187523365\n",
            "Val loss 0.0515\n",
            "Dice score : 0.03204403445124626\n",
            "Val loss 0.0568\n",
            "Dice score : 0.043809980154037476\n",
            "Val loss 0.0567\n",
            "Dice score : 0.03566736355423927\n",
            "Val loss 0.0568\n",
            "Dice score : 0.036591049283742905\n",
            "Val loss 0.0550\n",
            "Dice score : 0.0365227647125721\n",
            "Val loss 0.0536\n",
            "Dice score : 0.03990601748228073\n",
            "Val loss 0.0543\n",
            "Dice score : 0.03442798927426338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 15/100 [20:16<1:55:42, 81.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0016 / 0100 | batch 0000 / 0050 | loss 0.0436\n",
            "Train: epoch 0016 / 0100 | batch 0001 / 0050 | loss 0.0538\n",
            "Train: epoch 0016 / 0100 | batch 0002 / 0050 | loss 0.0534\n",
            "Train: epoch 0016 / 0100 | batch 0003 / 0050 | loss 0.0562\n",
            "Train: epoch 0016 / 0100 | batch 0004 / 0050 | loss 0.0540\n",
            "Train: epoch 0016 / 0100 | batch 0005 / 0050 | loss 0.0586\n",
            "Train: epoch 0016 / 0100 | batch 0006 / 0050 | loss 0.0557\n",
            "Train: epoch 0016 / 0100 | batch 0007 / 0050 | loss 0.0588\n",
            "Train: epoch 0016 / 0100 | batch 0008 / 0050 | loss 0.0597\n",
            "Train: epoch 0016 / 0100 | batch 0009 / 0050 | loss 0.0578\n",
            "Train: epoch 0016 / 0100 | batch 0010 / 0050 | loss 0.0564\n",
            "Train: epoch 0016 / 0100 | batch 0011 / 0050 | loss 0.0565\n",
            "Train: epoch 0016 / 0100 | batch 0012 / 0050 | loss 0.0553\n",
            "Train: epoch 0016 / 0100 | batch 0013 / 0050 | loss 0.0557\n",
            "Train: epoch 0016 / 0100 | batch 0014 / 0050 | loss 0.0560\n",
            "Train: epoch 0016 / 0100 | batch 0015 / 0050 | loss 0.0548\n",
            "Train: epoch 0016 / 0100 | batch 0016 / 0050 | loss 0.0558\n",
            "Train: epoch 0016 / 0100 | batch 0017 / 0050 | loss 0.0553\n",
            "Train: epoch 0016 / 0100 | batch 0018 / 0050 | loss 0.0555\n",
            "Train: epoch 0016 / 0100 | batch 0019 / 0050 | loss 0.0561\n",
            "Train: epoch 0016 / 0100 | batch 0020 / 0050 | loss 0.0558\n",
            "Train: epoch 0016 / 0100 | batch 0021 / 0050 | loss 0.0563\n",
            "Train: epoch 0016 / 0100 | batch 0022 / 0050 | loss 0.0562\n",
            "Train: epoch 0016 / 0100 | batch 0023 / 0050 | loss 0.0557\n",
            "Train: epoch 0016 / 0100 | batch 0024 / 0050 | loss 0.0551\n",
            "Train: epoch 0016 / 0100 | batch 0025 / 0050 | loss 0.0548\n",
            "Train: epoch 0016 / 0100 | batch 0026 / 0050 | loss 0.0543\n",
            "Train: epoch 0016 / 0100 | batch 0027 / 0050 | loss 0.0555\n",
            "Train: epoch 0016 / 0100 | batch 0028 / 0050 | loss 0.0554\n",
            "Train: epoch 0016 / 0100 | batch 0029 / 0050 | loss 0.0554\n",
            "Train: epoch 0016 / 0100 | batch 0030 / 0050 | loss 0.0555\n",
            "Train: epoch 0016 / 0100 | batch 0031 / 0050 | loss 0.0550\n",
            "Train: epoch 0016 / 0100 | batch 0032 / 0050 | loss 0.0552\n",
            "Train: epoch 0016 / 0100 | batch 0033 / 0050 | loss 0.0553\n",
            "Train: epoch 0016 / 0100 | batch 0034 / 0050 | loss 0.0548\n",
            "Train: epoch 0016 / 0100 | batch 0035 / 0050 | loss 0.0546\n",
            "Train: epoch 0016 / 0100 | batch 0036 / 0050 | loss 0.0542\n",
            "Train: epoch 0016 / 0100 | batch 0037 / 0050 | loss 0.0543\n",
            "Train: epoch 0016 / 0100 | batch 0038 / 0050 | loss 0.0541\n",
            "Train: epoch 0016 / 0100 | batch 0039 / 0050 | loss 0.0538\n",
            "Train: epoch 0016 / 0100 | batch 0040 / 0050 | loss 0.0536\n",
            "Train: epoch 0016 / 0100 | batch 0041 / 0050 | loss 0.0536\n",
            "Train: epoch 0016 / 0100 | batch 0042 / 0050 | loss 0.0532\n",
            "Train: epoch 0016 / 0100 | batch 0043 / 0050 | loss 0.0536\n",
            "Train: epoch 0016 / 0100 | batch 0044 / 0050 | loss 0.0534\n",
            "Train: epoch 0016 / 0100 | batch 0045 / 0050 | loss 0.0535\n",
            "Train: epoch 0016 / 0100 | batch 0046 / 0050 | loss 0.0534\n",
            "Train: epoch 0016 / 0100 | batch 0047 / 0050 | loss 0.0531\n",
            "Train: epoch 0016 / 0100 | batch 0048 / 0050 | loss 0.0531\n",
            "Train: epoch 0016 / 0100 | batch 0049 / 0050 | loss 0.0533\n",
            "Val loss 0.0566\n",
            "Dice score : 0.025534508749842644\n",
            "Val loss 0.0673\n",
            "Dice score : 0.03653234243392944\n",
            "Val loss 0.0610\n",
            "Dice score : 0.032488349825143814\n",
            "Val loss 0.0558\n",
            "Dice score : 0.048073988407850266\n",
            "Val loss 0.0545\n",
            "Dice score : 0.037295129150152206\n",
            "Val loss 0.0558\n",
            "Dice score : 0.03141562268137932\n",
            "Val loss 0.0572\n",
            "Dice score : 0.02807885967195034\n",
            "Val loss 0.0596\n",
            "Dice score : 0.03450196608901024\n",
            "Val loss 0.0594\n",
            "Dice score : 0.02838418260216713\n",
            "Val loss 0.0589\n",
            "Dice score : 0.0350162535905838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 16/100 [21:36<1:53:33, 81.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0017 / 0100 | batch 0000 / 0050 | loss 0.0666\n",
            "Train: epoch 0017 / 0100 | batch 0001 / 0050 | loss 0.0720\n",
            "Train: epoch 0017 / 0100 | batch 0002 / 0050 | loss 0.0586\n",
            "Train: epoch 0017 / 0100 | batch 0003 / 0050 | loss 0.0550\n",
            "Train: epoch 0017 / 0100 | batch 0004 / 0050 | loss 0.0529\n",
            "Train: epoch 0017 / 0100 | batch 0005 / 0050 | loss 0.0523\n",
            "Train: epoch 0017 / 0100 | batch 0006 / 0050 | loss 0.0518\n",
            "Train: epoch 0017 / 0100 | batch 0007 / 0050 | loss 0.0503\n",
            "Train: epoch 0017 / 0100 | batch 0008 / 0050 | loss 0.0504\n",
            "Train: epoch 0017 / 0100 | batch 0009 / 0050 | loss 0.0505\n",
            "Train: epoch 0017 / 0100 | batch 0010 / 0050 | loss 0.0494\n",
            "Train: epoch 0017 / 0100 | batch 0011 / 0050 | loss 0.0493\n",
            "Train: epoch 0017 / 0100 | batch 0012 / 0050 | loss 0.0486\n",
            "Train: epoch 0017 / 0100 | batch 0013 / 0050 | loss 0.0499\n",
            "Train: epoch 0017 / 0100 | batch 0014 / 0050 | loss 0.0495\n",
            "Train: epoch 0017 / 0100 | batch 0015 / 0050 | loss 0.0495\n",
            "Train: epoch 0017 / 0100 | batch 0016 / 0050 | loss 0.0492\n",
            "Train: epoch 0017 / 0100 | batch 0017 / 0050 | loss 0.0496\n",
            "Train: epoch 0017 / 0100 | batch 0018 / 0050 | loss 0.0500\n",
            "Train: epoch 0017 / 0100 | batch 0019 / 0050 | loss 0.0495\n",
            "Train: epoch 0017 / 0100 | batch 0020 / 0050 | loss 0.0491\n",
            "Train: epoch 0017 / 0100 | batch 0021 / 0050 | loss 0.0500\n",
            "Train: epoch 0017 / 0100 | batch 0022 / 0050 | loss 0.0523\n",
            "Train: epoch 0017 / 0100 | batch 0023 / 0050 | loss 0.0521\n",
            "Train: epoch 0017 / 0100 | batch 0024 / 0050 | loss 0.0516\n",
            "Train: epoch 0017 / 0100 | batch 0025 / 0050 | loss 0.0520\n",
            "Train: epoch 0017 / 0100 | batch 0026 / 0050 | loss 0.0522\n",
            "Train: epoch 0017 / 0100 | batch 0027 / 0050 | loss 0.0521\n",
            "Train: epoch 0017 / 0100 | batch 0028 / 0050 | loss 0.0525\n",
            "Train: epoch 0017 / 0100 | batch 0029 / 0050 | loss 0.0522\n",
            "Train: epoch 0017 / 0100 | batch 0030 / 0050 | loss 0.0525\n",
            "Train: epoch 0017 / 0100 | batch 0031 / 0050 | loss 0.0520\n",
            "Train: epoch 0017 / 0100 | batch 0032 / 0050 | loss 0.0520\n",
            "Train: epoch 0017 / 0100 | batch 0033 / 0050 | loss 0.0522\n",
            "Train: epoch 0017 / 0100 | batch 0034 / 0050 | loss 0.0525\n",
            "Train: epoch 0017 / 0100 | batch 0035 / 0050 | loss 0.0529\n",
            "Train: epoch 0017 / 0100 | batch 0036 / 0050 | loss 0.0529\n",
            "Train: epoch 0017 / 0100 | batch 0037 / 0050 | loss 0.0532\n",
            "Train: epoch 0017 / 0100 | batch 0038 / 0050 | loss 0.0533\n",
            "Train: epoch 0017 / 0100 | batch 0039 / 0050 | loss 0.0534\n",
            "Train: epoch 0017 / 0100 | batch 0040 / 0050 | loss 0.0531\n",
            "Train: epoch 0017 / 0100 | batch 0041 / 0050 | loss 0.0531\n",
            "Train: epoch 0017 / 0100 | batch 0042 / 0050 | loss 0.0530\n",
            "Train: epoch 0017 / 0100 | batch 0043 / 0050 | loss 0.0532\n",
            "Train: epoch 0017 / 0100 | batch 0044 / 0050 | loss 0.0533\n",
            "Train: epoch 0017 / 0100 | batch 0045 / 0050 | loss 0.0531\n",
            "Train: epoch 0017 / 0100 | batch 0046 / 0050 | loss 0.0533\n",
            "Train: epoch 0017 / 0100 | batch 0047 / 0050 | loss 0.0532\n",
            "Train: epoch 0017 / 0100 | batch 0048 / 0050 | loss 0.0530\n",
            "Train: epoch 0017 / 0100 | batch 0049 / 0050 | loss 0.0529\n",
            "Val loss 0.0473\n",
            "Dice score : 0.037377044558525085\n",
            "Val loss 0.0449\n",
            "Dice score : 0.023285046219825745\n",
            "Val loss 0.0618\n",
            "Dice score : 0.05222557112574577\n",
            "Val loss 0.0590\n",
            "Dice score : 0.034062158316373825\n",
            "Val loss 0.0554\n",
            "Dice score : 0.02262326329946518\n",
            "Val loss 0.0600\n",
            "Dice score : 0.038795147091150284\n",
            "Val loss 0.0615\n",
            "Dice score : 0.036996014416217804\n",
            "Val loss 0.0619\n",
            "Dice score : 0.034213729202747345\n",
            "Val loss 0.0606\n",
            "Dice score : 0.018055271357297897\n",
            "Val loss 0.0591\n",
            "Dice score : 0.0378456674516201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 17/100 [22:56<1:51:41, 80.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0018 / 0100 | batch 0000 / 0050 | loss 0.0546\n",
            "Train: epoch 0018 / 0100 | batch 0001 / 0050 | loss 0.0458\n",
            "Train: epoch 0018 / 0100 | batch 0002 / 0050 | loss 0.0424\n",
            "Train: epoch 0018 / 0100 | batch 0003 / 0050 | loss 0.0460\n",
            "Train: epoch 0018 / 0100 | batch 0004 / 0050 | loss 0.0439\n",
            "Train: epoch 0018 / 0100 | batch 0005 / 0050 | loss 0.0444\n",
            "Train: epoch 0018 / 0100 | batch 0006 / 0050 | loss 0.0457\n",
            "Train: epoch 0018 / 0100 | batch 0007 / 0050 | loss 0.0448\n",
            "Train: epoch 0018 / 0100 | batch 0008 / 0050 | loss 0.0449\n",
            "Train: epoch 0018 / 0100 | batch 0009 / 0050 | loss 0.0458\n",
            "Train: epoch 0018 / 0100 | batch 0010 / 0050 | loss 0.0453\n",
            "Train: epoch 0018 / 0100 | batch 0011 / 0050 | loss 0.0458\n",
            "Train: epoch 0018 / 0100 | batch 0012 / 0050 | loss 0.0453\n",
            "Train: epoch 0018 / 0100 | batch 0013 / 0050 | loss 0.0451\n",
            "Train: epoch 0018 / 0100 | batch 0014 / 0050 | loss 0.0454\n",
            "Train: epoch 0018 / 0100 | batch 0015 / 0050 | loss 0.0448\n",
            "Train: epoch 0018 / 0100 | batch 0016 / 0050 | loss 0.0445\n",
            "Train: epoch 0018 / 0100 | batch 0017 / 0050 | loss 0.0457\n",
            "Train: epoch 0018 / 0100 | batch 0018 / 0050 | loss 0.0461\n",
            "Train: epoch 0018 / 0100 | batch 0019 / 0050 | loss 0.0467\n",
            "Train: epoch 0018 / 0100 | batch 0020 / 0050 | loss 0.0474\n",
            "Train: epoch 0018 / 0100 | batch 0021 / 0050 | loss 0.0483\n",
            "Train: epoch 0018 / 0100 | batch 0022 / 0050 | loss 0.0486\n",
            "Train: epoch 0018 / 0100 | batch 0023 / 0050 | loss 0.0486\n",
            "Train: epoch 0018 / 0100 | batch 0024 / 0050 | loss 0.0485\n",
            "Train: epoch 0018 / 0100 | batch 0025 / 0050 | loss 0.0480\n",
            "Train: epoch 0018 / 0100 | batch 0026 / 0050 | loss 0.0492\n",
            "Train: epoch 0018 / 0100 | batch 0027 / 0050 | loss 0.0489\n",
            "Train: epoch 0018 / 0100 | batch 0028 / 0050 | loss 0.0493\n",
            "Train: epoch 0018 / 0100 | batch 0029 / 0050 | loss 0.0498\n",
            "Train: epoch 0018 / 0100 | batch 0030 / 0050 | loss 0.0501\n",
            "Train: epoch 0018 / 0100 | batch 0031 / 0050 | loss 0.0506\n",
            "Train: epoch 0018 / 0100 | batch 0032 / 0050 | loss 0.0510\n",
            "Train: epoch 0018 / 0100 | batch 0033 / 0050 | loss 0.0515\n",
            "Train: epoch 0018 / 0100 | batch 0034 / 0050 | loss 0.0525\n",
            "Train: epoch 0018 / 0100 | batch 0035 / 0050 | loss 0.0525\n",
            "Train: epoch 0018 / 0100 | batch 0036 / 0050 | loss 0.0530\n",
            "Train: epoch 0018 / 0100 | batch 0037 / 0050 | loss 0.0531\n",
            "Train: epoch 0018 / 0100 | batch 0038 / 0050 | loss 0.0527\n",
            "Train: epoch 0018 / 0100 | batch 0039 / 0050 | loss 0.0525\n",
            "Train: epoch 0018 / 0100 | batch 0040 / 0050 | loss 0.0522\n",
            "Train: epoch 0018 / 0100 | batch 0041 / 0050 | loss 0.0524\n",
            "Train: epoch 0018 / 0100 | batch 0042 / 0050 | loss 0.0526\n",
            "Train: epoch 0018 / 0100 | batch 0043 / 0050 | loss 0.0524\n",
            "Train: epoch 0018 / 0100 | batch 0044 / 0050 | loss 0.0524\n",
            "Train: epoch 0018 / 0100 | batch 0045 / 0050 | loss 0.0525\n",
            "Train: epoch 0018 / 0100 | batch 0046 / 0050 | loss 0.0522\n",
            "Train: epoch 0018 / 0100 | batch 0047 / 0050 | loss 0.0521\n",
            "Train: epoch 0018 / 0100 | batch 0048 / 0050 | loss 0.0520\n",
            "Train: epoch 0018 / 0100 | batch 0049 / 0050 | loss 0.0525\n",
            "Val loss 0.0479\n",
            "Dice score : 0.033682145178318024\n",
            "Val loss 0.0717\n",
            "Dice score : 0.04754715785384178\n",
            "Val loss 0.0661\n",
            "Dice score : 0.039188284426927567\n",
            "Val loss 0.0610\n",
            "Dice score : 0.02936628647148609\n",
            "Val loss 0.0655\n",
            "Dice score : 0.04793572053313255\n",
            "Val loss 0.0645\n",
            "Dice score : 0.030261022970080376\n",
            "Val loss 0.0616\n",
            "Dice score : 0.038690585643053055\n",
            "Val loss 0.0599\n",
            "Dice score : 0.03366589918732643\n",
            "Val loss 0.0583\n",
            "Dice score : 0.01969105750322342\n",
            "Val loss 0.0575\n",
            "Dice score : 0.026693323627114296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 18/100 [24:16<1:50:21, 80.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0019 / 0100 | batch 0000 / 0050 | loss 0.0673\n",
            "Train: epoch 0019 / 0100 | batch 0001 / 0050 | loss 0.0697\n",
            "Train: epoch 0019 / 0100 | batch 0002 / 0050 | loss 0.0645\n",
            "Train: epoch 0019 / 0100 | batch 0003 / 0050 | loss 0.0623\n",
            "Train: epoch 0019 / 0100 | batch 0004 / 0050 | loss 0.0591\n",
            "Train: epoch 0019 / 0100 | batch 0005 / 0050 | loss 0.0572\n",
            "Train: epoch 0019 / 0100 | batch 0006 / 0050 | loss 0.0549\n",
            "Train: epoch 0019 / 0100 | batch 0007 / 0050 | loss 0.0520\n",
            "Train: epoch 0019 / 0100 | batch 0008 / 0050 | loss 0.0551\n",
            "Train: epoch 0019 / 0100 | batch 0009 / 0050 | loss 0.0543\n",
            "Train: epoch 0019 / 0100 | batch 0010 / 0050 | loss 0.0536\n",
            "Train: epoch 0019 / 0100 | batch 0011 / 0050 | loss 0.0539\n",
            "Train: epoch 0019 / 0100 | batch 0012 / 0050 | loss 0.0554\n",
            "Train: epoch 0019 / 0100 | batch 0013 / 0050 | loss 0.0552\n",
            "Train: epoch 0019 / 0100 | batch 0014 / 0050 | loss 0.0538\n",
            "Train: epoch 0019 / 0100 | batch 0015 / 0050 | loss 0.0529\n",
            "Train: epoch 0019 / 0100 | batch 0016 / 0050 | loss 0.0524\n",
            "Train: epoch 0019 / 0100 | batch 0017 / 0050 | loss 0.0518\n",
            "Train: epoch 0019 / 0100 | batch 0018 / 0050 | loss 0.0535\n",
            "Train: epoch 0019 / 0100 | batch 0019 / 0050 | loss 0.0526\n",
            "Train: epoch 0019 / 0100 | batch 0020 / 0050 | loss 0.0521\n",
            "Train: epoch 0019 / 0100 | batch 0021 / 0050 | loss 0.0515\n",
            "Train: epoch 0019 / 0100 | batch 0022 / 0050 | loss 0.0514\n",
            "Train: epoch 0019 / 0100 | batch 0023 / 0050 | loss 0.0519\n",
            "Train: epoch 0019 / 0100 | batch 0024 / 0050 | loss 0.0513\n",
            "Train: epoch 0019 / 0100 | batch 0025 / 0050 | loss 0.0510\n",
            "Train: epoch 0019 / 0100 | batch 0026 / 0050 | loss 0.0507\n",
            "Train: epoch 0019 / 0100 | batch 0027 / 0050 | loss 0.0511\n",
            "Train: epoch 0019 / 0100 | batch 0028 / 0050 | loss 0.0512\n",
            "Train: epoch 0019 / 0100 | batch 0029 / 0050 | loss 0.0525\n",
            "Train: epoch 0019 / 0100 | batch 0030 / 0050 | loss 0.0522\n",
            "Train: epoch 0019 / 0100 | batch 0031 / 0050 | loss 0.0529\n",
            "Train: epoch 0019 / 0100 | batch 0032 / 0050 | loss 0.0532\n",
            "Train: epoch 0019 / 0100 | batch 0033 / 0050 | loss 0.0533\n",
            "Train: epoch 0019 / 0100 | batch 0034 / 0050 | loss 0.0534\n",
            "Train: epoch 0019 / 0100 | batch 0035 / 0050 | loss 0.0534\n",
            "Train: epoch 0019 / 0100 | batch 0036 / 0050 | loss 0.0529\n",
            "Train: epoch 0019 / 0100 | batch 0037 / 0050 | loss 0.0530\n",
            "Train: epoch 0019 / 0100 | batch 0038 / 0050 | loss 0.0528\n",
            "Train: epoch 0019 / 0100 | batch 0039 / 0050 | loss 0.0532\n",
            "Train: epoch 0019 / 0100 | batch 0040 / 0050 | loss 0.0532\n",
            "Train: epoch 0019 / 0100 | batch 0041 / 0050 | loss 0.0533\n",
            "Train: epoch 0019 / 0100 | batch 0042 / 0050 | loss 0.0529\n",
            "Train: epoch 0019 / 0100 | batch 0043 / 0050 | loss 0.0527\n",
            "Train: epoch 0019 / 0100 | batch 0044 / 0050 | loss 0.0526\n",
            "Train: epoch 0019 / 0100 | batch 0045 / 0050 | loss 0.0523\n",
            "Train: epoch 0019 / 0100 | batch 0046 / 0050 | loss 0.0523\n",
            "Train: epoch 0019 / 0100 | batch 0047 / 0050 | loss 0.0527\n",
            "Train: epoch 0019 / 0100 | batch 0048 / 0050 | loss 0.0525\n",
            "Train: epoch 0019 / 0100 | batch 0049 / 0050 | loss 0.0526\n",
            "Val loss 0.0350\n",
            "Dice score : 0.030611328780651093\n",
            "Val loss 0.0385\n",
            "Dice score : 0.03920416906476021\n",
            "Val loss 0.0577\n",
            "Dice score : 0.05421239137649536\n",
            "Val loss 0.0566\n",
            "Dice score : 0.03562887758016586\n",
            "Val loss 0.0544\n",
            "Dice score : 0.03560161218047142\n",
            "Val loss 0.0531\n",
            "Dice score : 0.027439266443252563\n",
            "Val loss 0.0539\n",
            "Dice score : 0.05202899128198624\n",
            "Val loss 0.0522\n",
            "Dice score : 0.05567929148674011\n",
            "Val loss 0.0540\n",
            "Dice score : 0.043585821986198425\n",
            "Val loss 0.0529\n",
            "Dice score : 0.04198696091771126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 19/100 [25:34<1:47:54, 79.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0020 / 0100 | batch 0000 / 0050 | loss 0.0608\n",
            "Train: epoch 0020 / 0100 | batch 0001 / 0050 | loss 0.0528\n",
            "Train: epoch 0020 / 0100 | batch 0002 / 0050 | loss 0.0533\n",
            "Train: epoch 0020 / 0100 | batch 0003 / 0050 | loss 0.0547\n",
            "Train: epoch 0020 / 0100 | batch 0004 / 0050 | loss 0.0588\n",
            "Train: epoch 0020 / 0100 | batch 0005 / 0050 | loss 0.0602\n",
            "Train: epoch 0020 / 0100 | batch 0006 / 0050 | loss 0.0575\n",
            "Train: epoch 0020 / 0100 | batch 0007 / 0050 | loss 0.0553\n",
            "Train: epoch 0020 / 0100 | batch 0008 / 0050 | loss 0.0546\n",
            "Train: epoch 0020 / 0100 | batch 0009 / 0050 | loss 0.0542\n",
            "Train: epoch 0020 / 0100 | batch 0010 / 0050 | loss 0.0524\n",
            "Train: epoch 0020 / 0100 | batch 0011 / 0050 | loss 0.0522\n",
            "Train: epoch 0020 / 0100 | batch 0012 / 0050 | loss 0.0549\n",
            "Train: epoch 0020 / 0100 | batch 0013 / 0050 | loss 0.0538\n",
            "Train: epoch 0020 / 0100 | batch 0014 / 0050 | loss 0.0530\n",
            "Train: epoch 0020 / 0100 | batch 0015 / 0050 | loss 0.0527\n",
            "Train: epoch 0020 / 0100 | batch 0016 / 0050 | loss 0.0529\n",
            "Train: epoch 0020 / 0100 | batch 0017 / 0050 | loss 0.0529\n",
            "Train: epoch 0020 / 0100 | batch 0018 / 0050 | loss 0.0527\n",
            "Train: epoch 0020 / 0100 | batch 0019 / 0050 | loss 0.0535\n",
            "Train: epoch 0020 / 0100 | batch 0020 / 0050 | loss 0.0531\n",
            "Train: epoch 0020 / 0100 | batch 0021 / 0050 | loss 0.0530\n",
            "Train: epoch 0020 / 0100 | batch 0022 / 0050 | loss 0.0524\n",
            "Train: epoch 0020 / 0100 | batch 0023 / 0050 | loss 0.0536\n",
            "Train: epoch 0020 / 0100 | batch 0024 / 0050 | loss 0.0537\n",
            "Train: epoch 0020 / 0100 | batch 0025 / 0050 | loss 0.0546\n",
            "Train: epoch 0020 / 0100 | batch 0026 / 0050 | loss 0.0555\n",
            "Train: epoch 0020 / 0100 | batch 0027 / 0050 | loss 0.0564\n",
            "Train: epoch 0020 / 0100 | batch 0028 / 0050 | loss 0.0559\n",
            "Train: epoch 0020 / 0100 | batch 0029 / 0050 | loss 0.0567\n",
            "Train: epoch 0020 / 0100 | batch 0030 / 0050 | loss 0.0564\n",
            "Train: epoch 0020 / 0100 | batch 0031 / 0050 | loss 0.0560\n",
            "Train: epoch 0020 / 0100 | batch 0032 / 0050 | loss 0.0562\n",
            "Train: epoch 0020 / 0100 | batch 0033 / 0050 | loss 0.0560\n",
            "Train: epoch 0020 / 0100 | batch 0034 / 0050 | loss 0.0560\n",
            "Train: epoch 0020 / 0100 | batch 0035 / 0050 | loss 0.0555\n",
            "Train: epoch 0020 / 0100 | batch 0036 / 0050 | loss 0.0558\n",
            "Train: epoch 0020 / 0100 | batch 0037 / 0050 | loss 0.0553\n",
            "Train: epoch 0020 / 0100 | batch 0038 / 0050 | loss 0.0555\n",
            "Train: epoch 0020 / 0100 | batch 0039 / 0050 | loss 0.0553\n",
            "Train: epoch 0020 / 0100 | batch 0040 / 0050 | loss 0.0553\n",
            "Train: epoch 0020 / 0100 | batch 0041 / 0050 | loss 0.0548\n",
            "Train: epoch 0020 / 0100 | batch 0042 / 0050 | loss 0.0544\n",
            "Train: epoch 0020 / 0100 | batch 0043 / 0050 | loss 0.0542\n",
            "Train: epoch 0020 / 0100 | batch 0044 / 0050 | loss 0.0538\n",
            "Train: epoch 0020 / 0100 | batch 0045 / 0050 | loss 0.0535\n",
            "Train: epoch 0020 / 0100 | batch 0046 / 0050 | loss 0.0538\n",
            "Train: epoch 0020 / 0100 | batch 0047 / 0050 | loss 0.0535\n",
            "Train: epoch 0020 / 0100 | batch 0048 / 0050 | loss 0.0535\n",
            "Train: epoch 0020 / 0100 | batch 0049 / 0050 | loss 0.0534\n",
            "Val loss 0.0451\n",
            "Dice score : 0.0160820409655571\n",
            "Val loss 0.0512\n",
            "Dice score : 0.029213083907961845\n",
            "Val loss 0.0614\n",
            "Dice score : 0.046009164303541183\n",
            "Val loss 0.0645\n",
            "Dice score : 0.015476088970899582\n",
            "Val loss 0.0601\n",
            "Dice score : 0.015141284093260765\n",
            "Val loss 0.0647\n",
            "Dice score : 0.03111601062119007\n",
            "Val loss 0.0624\n",
            "Dice score : 0.024439185857772827\n",
            "Val loss 0.0611\n",
            "Dice score : 0.023473689332604408\n",
            "Val loss 0.0592\n",
            "Dice score : 0.02650453895330429\n",
            "Val loss 0.0609\n",
            "Dice score : 0.038174115121364594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 20/100 [26:55<1:46:56, 80.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0021 / 0100 | batch 0000 / 0050 | loss 0.0622\n",
            "Train: epoch 0021 / 0100 | batch 0001 / 0050 | loss 0.0539\n",
            "Train: epoch 0021 / 0100 | batch 0002 / 0050 | loss 0.0499\n",
            "Train: epoch 0021 / 0100 | batch 0003 / 0050 | loss 0.0527\n",
            "Train: epoch 0021 / 0100 | batch 0004 / 0050 | loss 0.0536\n",
            "Train: epoch 0021 / 0100 | batch 0005 / 0050 | loss 0.0557\n",
            "Train: epoch 0021 / 0100 | batch 0006 / 0050 | loss 0.0564\n",
            "Train: epoch 0021 / 0100 | batch 0007 / 0050 | loss 0.0571\n",
            "Train: epoch 0021 / 0100 | batch 0008 / 0050 | loss 0.0547\n",
            "Train: epoch 0021 / 0100 | batch 0009 / 0050 | loss 0.0540\n",
            "Train: epoch 0021 / 0100 | batch 0010 / 0050 | loss 0.0528\n",
            "Train: epoch 0021 / 0100 | batch 0011 / 0050 | loss 0.0515\n",
            "Train: epoch 0021 / 0100 | batch 0012 / 0050 | loss 0.0510\n",
            "Train: epoch 0021 / 0100 | batch 0013 / 0050 | loss 0.0522\n",
            "Train: epoch 0021 / 0100 | batch 0014 / 0050 | loss 0.0526\n",
            "Train: epoch 0021 / 0100 | batch 0015 / 0050 | loss 0.0519\n",
            "Train: epoch 0021 / 0100 | batch 0016 / 0050 | loss 0.0516\n",
            "Train: epoch 0021 / 0100 | batch 0017 / 0050 | loss 0.0521\n",
            "Train: epoch 0021 / 0100 | batch 0018 / 0050 | loss 0.0520\n",
            "Train: epoch 0021 / 0100 | batch 0019 / 0050 | loss 0.0518\n",
            "Train: epoch 0021 / 0100 | batch 0020 / 0050 | loss 0.0525\n",
            "Train: epoch 0021 / 0100 | batch 0021 / 0050 | loss 0.0525\n",
            "Train: epoch 0021 / 0100 | batch 0022 / 0050 | loss 0.0522\n",
            "Train: epoch 0021 / 0100 | batch 0023 / 0050 | loss 0.0527\n",
            "Train: epoch 0021 / 0100 | batch 0024 / 0050 | loss 0.0530\n",
            "Train: epoch 0021 / 0100 | batch 0025 / 0050 | loss 0.0537\n",
            "Train: epoch 0021 / 0100 | batch 0026 / 0050 | loss 0.0534\n",
            "Train: epoch 0021 / 0100 | batch 0027 / 0050 | loss 0.0537\n",
            "Train: epoch 0021 / 0100 | batch 0028 / 0050 | loss 0.0534\n",
            "Train: epoch 0021 / 0100 | batch 0029 / 0050 | loss 0.0545\n",
            "Train: epoch 0021 / 0100 | batch 0030 / 0050 | loss 0.0544\n",
            "Train: epoch 0021 / 0100 | batch 0031 / 0050 | loss 0.0540\n",
            "Train: epoch 0021 / 0100 | batch 0032 / 0050 | loss 0.0535\n",
            "Train: epoch 0021 / 0100 | batch 0033 / 0050 | loss 0.0533\n",
            "Train: epoch 0021 / 0100 | batch 0034 / 0050 | loss 0.0535\n",
            "Train: epoch 0021 / 0100 | batch 0035 / 0050 | loss 0.0531\n",
            "Train: epoch 0021 / 0100 | batch 0036 / 0050 | loss 0.0529\n",
            "Train: epoch 0021 / 0100 | batch 0037 / 0050 | loss 0.0528\n",
            "Train: epoch 0021 / 0100 | batch 0038 / 0050 | loss 0.0523\n",
            "Train: epoch 0021 / 0100 | batch 0039 / 0050 | loss 0.0524\n",
            "Train: epoch 0021 / 0100 | batch 0040 / 0050 | loss 0.0528\n",
            "Train: epoch 0021 / 0100 | batch 0041 / 0050 | loss 0.0533\n",
            "Train: epoch 0021 / 0100 | batch 0042 / 0050 | loss 0.0532\n",
            "Train: epoch 0021 / 0100 | batch 0043 / 0050 | loss 0.0535\n",
            "Train: epoch 0021 / 0100 | batch 0044 / 0050 | loss 0.0531\n",
            "Train: epoch 0021 / 0100 | batch 0045 / 0050 | loss 0.0528\n",
            "Train: epoch 0021 / 0100 | batch 0046 / 0050 | loss 0.0529\n",
            "Train: epoch 0021 / 0100 | batch 0047 / 0050 | loss 0.0527\n",
            "Train: epoch 0021 / 0100 | batch 0048 / 0050 | loss 0.0529\n",
            "Train: epoch 0021 / 0100 | batch 0049 / 0050 | loss 0.0530\n",
            "Val loss 0.0465\n",
            "Dice score : 0.036051005125045776\n",
            "Val loss 0.0428\n",
            "Dice score : 0.028113393113017082\n",
            "Val loss 0.0418\n",
            "Dice score : 0.046742722392082214\n",
            "Val loss 0.0417\n",
            "Dice score : 0.03418896719813347\n",
            "Val loss 0.0444\n",
            "Dice score : 0.03461717441678047\n",
            "Val loss 0.0479\n",
            "Dice score : 0.03486126288771629\n",
            "Val loss 0.0510\n",
            "Dice score : 0.03730234503746033\n",
            "Val loss 0.0539\n",
            "Dice score : 0.03886788338422775\n",
            "Val loss 0.0544\n",
            "Dice score : 0.03665687143802643\n",
            "Val loss 0.0541\n",
            "Dice score : 0.03479592502117157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 21/100 [28:15<1:45:36, 80.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0022 / 0100 | batch 0000 / 0050 | loss 0.0782\n",
            "Train: epoch 0022 / 0100 | batch 0001 / 0050 | loss 0.0632\n",
            "Train: epoch 0022 / 0100 | batch 0002 / 0050 | loss 0.0572\n",
            "Train: epoch 0022 / 0100 | batch 0003 / 0050 | loss 0.0559\n",
            "Train: epoch 0022 / 0100 | batch 0004 / 0050 | loss 0.0529\n",
            "Train: epoch 0022 / 0100 | batch 0005 / 0050 | loss 0.0541\n",
            "Train: epoch 0022 / 0100 | batch 0006 / 0050 | loss 0.0579\n",
            "Train: epoch 0022 / 0100 | batch 0007 / 0050 | loss 0.0608\n",
            "Train: epoch 0022 / 0100 | batch 0008 / 0050 | loss 0.0585\n",
            "Train: epoch 0022 / 0100 | batch 0009 / 0050 | loss 0.0572\n",
            "Train: epoch 0022 / 0100 | batch 0010 / 0050 | loss 0.0563\n",
            "Train: epoch 0022 / 0100 | batch 0011 / 0050 | loss 0.0553\n",
            "Train: epoch 0022 / 0100 | batch 0012 / 0050 | loss 0.0536\n",
            "Train: epoch 0022 / 0100 | batch 0013 / 0050 | loss 0.0530\n",
            "Train: epoch 0022 / 0100 | batch 0014 / 0050 | loss 0.0527\n",
            "Train: epoch 0022 / 0100 | batch 0015 / 0050 | loss 0.0517\n",
            "Train: epoch 0022 / 0100 | batch 0016 / 0050 | loss 0.0524\n",
            "Train: epoch 0022 / 0100 | batch 0017 / 0050 | loss 0.0525\n",
            "Train: epoch 0022 / 0100 | batch 0018 / 0050 | loss 0.0526\n",
            "Train: epoch 0022 / 0100 | batch 0019 / 0050 | loss 0.0521\n",
            "Train: epoch 0022 / 0100 | batch 0020 / 0050 | loss 0.0524\n",
            "Train: epoch 0022 / 0100 | batch 0021 / 0050 | loss 0.0532\n",
            "Train: epoch 0022 / 0100 | batch 0022 / 0050 | loss 0.0528\n",
            "Train: epoch 0022 / 0100 | batch 0023 / 0050 | loss 0.0534\n",
            "Train: epoch 0022 / 0100 | batch 0024 / 0050 | loss 0.0531\n",
            "Train: epoch 0022 / 0100 | batch 0025 / 0050 | loss 0.0527\n",
            "Train: epoch 0022 / 0100 | batch 0026 / 0050 | loss 0.0539\n",
            "Train: epoch 0022 / 0100 | batch 0027 / 0050 | loss 0.0533\n",
            "Train: epoch 0022 / 0100 | batch 0028 / 0050 | loss 0.0533\n",
            "Train: epoch 0022 / 0100 | batch 0029 / 0050 | loss 0.0532\n",
            "Train: epoch 0022 / 0100 | batch 0030 / 0050 | loss 0.0538\n",
            "Train: epoch 0022 / 0100 | batch 0031 / 0050 | loss 0.0533\n",
            "Train: epoch 0022 / 0100 | batch 0032 / 0050 | loss 0.0527\n",
            "Train: epoch 0022 / 0100 | batch 0033 / 0050 | loss 0.0525\n",
            "Train: epoch 0022 / 0100 | batch 0034 / 0050 | loss 0.0528\n",
            "Train: epoch 0022 / 0100 | batch 0035 / 0050 | loss 0.0523\n",
            "Train: epoch 0022 / 0100 | batch 0036 / 0050 | loss 0.0520\n",
            "Train: epoch 0022 / 0100 | batch 0037 / 0050 | loss 0.0520\n",
            "Train: epoch 0022 / 0100 | batch 0038 / 0050 | loss 0.0521\n",
            "Train: epoch 0022 / 0100 | batch 0039 / 0050 | loss 0.0523\n",
            "Train: epoch 0022 / 0100 | batch 0040 / 0050 | loss 0.0525\n",
            "Train: epoch 0022 / 0100 | batch 0041 / 0050 | loss 0.0526\n",
            "Train: epoch 0022 / 0100 | batch 0042 / 0050 | loss 0.0529\n",
            "Train: epoch 0022 / 0100 | batch 0043 / 0050 | loss 0.0526\n",
            "Train: epoch 0022 / 0100 | batch 0044 / 0050 | loss 0.0526\n",
            "Train: epoch 0022 / 0100 | batch 0045 / 0050 | loss 0.0525\n",
            "Train: epoch 0022 / 0100 | batch 0046 / 0050 | loss 0.0527\n",
            "Train: epoch 0022 / 0100 | batch 0047 / 0050 | loss 0.0524\n",
            "Train: epoch 0022 / 0100 | batch 0048 / 0050 | loss 0.0523\n",
            "Train: epoch 0022 / 0100 | batch 0049 / 0050 | loss 0.0527\n",
            "Val loss 0.0453\n",
            "Dice score : 0.029007187113165855\n",
            "Val loss 0.0451\n",
            "Dice score : 0.032465316355228424\n",
            "Val loss 0.0465\n",
            "Dice score : 0.028706319630146027\n",
            "Val loss 0.0592\n",
            "Dice score : 0.061226986348629\n",
            "Val loss 0.0573\n",
            "Dice score : 0.03777201473712921\n",
            "Val loss 0.0618\n",
            "Dice score : 0.04322828724980354\n",
            "Val loss 0.0597\n",
            "Dice score : 0.03207135200500488\n",
            "Val loss 0.0578\n",
            "Dice score : 0.025274641811847687\n",
            "Val loss 0.0585\n",
            "Dice score : 0.03776441141963005\n",
            "Val loss 0.0608\n",
            "Dice score : 0.04383907467126846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 22/100 [29:34<1:43:30, 79.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0023 / 0100 | batch 0000 / 0050 | loss 0.0735\n",
            "Train: epoch 0023 / 0100 | batch 0001 / 0050 | loss 0.0762\n",
            "Train: epoch 0023 / 0100 | batch 0002 / 0050 | loss 0.0682\n",
            "Train: epoch 0023 / 0100 | batch 0003 / 0050 | loss 0.0616\n",
            "Train: epoch 0023 / 0100 | batch 0004 / 0050 | loss 0.0593\n",
            "Train: epoch 0023 / 0100 | batch 0005 / 0050 | loss 0.0580\n",
            "Train: epoch 0023 / 0100 | batch 0006 / 0050 | loss 0.0553\n",
            "Train: epoch 0023 / 0100 | batch 0007 / 0050 | loss 0.0537\n",
            "Train: epoch 0023 / 0100 | batch 0008 / 0050 | loss 0.0527\n",
            "Train: epoch 0023 / 0100 | batch 0009 / 0050 | loss 0.0534\n",
            "Train: epoch 0023 / 0100 | batch 0010 / 0050 | loss 0.0541\n",
            "Train: epoch 0023 / 0100 | batch 0011 / 0050 | loss 0.0540\n",
            "Train: epoch 0023 / 0100 | batch 0012 / 0050 | loss 0.0542\n",
            "Train: epoch 0023 / 0100 | batch 0013 / 0050 | loss 0.0532\n",
            "Train: epoch 0023 / 0100 | batch 0014 / 0050 | loss 0.0535\n",
            "Train: epoch 0023 / 0100 | batch 0015 / 0050 | loss 0.0525\n",
            "Train: epoch 0023 / 0100 | batch 0016 / 0050 | loss 0.0518\n",
            "Train: epoch 0023 / 0100 | batch 0017 / 0050 | loss 0.0534\n",
            "Train: epoch 0023 / 0100 | batch 0018 / 0050 | loss 0.0530\n",
            "Train: epoch 0023 / 0100 | batch 0019 / 0050 | loss 0.0522\n",
            "Train: epoch 0023 / 0100 | batch 0020 / 0050 | loss 0.0520\n",
            "Train: epoch 0023 / 0100 | batch 0021 / 0050 | loss 0.0526\n",
            "Train: epoch 0023 / 0100 | batch 0022 / 0050 | loss 0.0528\n",
            "Train: epoch 0023 / 0100 | batch 0023 / 0050 | loss 0.0526\n",
            "Train: epoch 0023 / 0100 | batch 0024 / 0050 | loss 0.0526\n",
            "Train: epoch 0023 / 0100 | batch 0025 / 0050 | loss 0.0525\n",
            "Train: epoch 0023 / 0100 | batch 0026 / 0050 | loss 0.0526\n",
            "Train: epoch 0023 / 0100 | batch 0027 / 0050 | loss 0.0525\n",
            "Train: epoch 0023 / 0100 | batch 0028 / 0050 | loss 0.0521\n",
            "Train: epoch 0023 / 0100 | batch 0029 / 0050 | loss 0.0528\n",
            "Train: epoch 0023 / 0100 | batch 0030 / 0050 | loss 0.0529\n",
            "Train: epoch 0023 / 0100 | batch 0031 / 0050 | loss 0.0532\n",
            "Train: epoch 0023 / 0100 | batch 0032 / 0050 | loss 0.0530\n",
            "Train: epoch 0023 / 0100 | batch 0033 / 0050 | loss 0.0530\n",
            "Train: epoch 0023 / 0100 | batch 0034 / 0050 | loss 0.0527\n",
            "Train: epoch 0023 / 0100 | batch 0035 / 0050 | loss 0.0528\n",
            "Train: epoch 0023 / 0100 | batch 0036 / 0050 | loss 0.0524\n",
            "Train: epoch 0023 / 0100 | batch 0037 / 0050 | loss 0.0521\n",
            "Train: epoch 0023 / 0100 | batch 0038 / 0050 | loss 0.0520\n",
            "Train: epoch 0023 / 0100 | batch 0039 / 0050 | loss 0.0518\n",
            "Train: epoch 0023 / 0100 | batch 0040 / 0050 | loss 0.0516\n",
            "Train: epoch 0023 / 0100 | batch 0041 / 0050 | loss 0.0518\n",
            "Train: epoch 0023 / 0100 | batch 0042 / 0050 | loss 0.0522\n",
            "Train: epoch 0023 / 0100 | batch 0043 / 0050 | loss 0.0523\n",
            "Train: epoch 0023 / 0100 | batch 0044 / 0050 | loss 0.0519\n",
            "Train: epoch 0023 / 0100 | batch 0045 / 0050 | loss 0.0517\n",
            "Train: epoch 0023 / 0100 | batch 0046 / 0050 | loss 0.0526\n",
            "Train: epoch 0023 / 0100 | batch 0047 / 0050 | loss 0.0523\n",
            "Train: epoch 0023 / 0100 | batch 0048 / 0050 | loss 0.0520\n",
            "Train: epoch 0023 / 0100 | batch 0049 / 0050 | loss 0.0522\n",
            "Val loss 0.0712\n",
            "Dice score : 0.04154413565993309\n",
            "Val loss 0.0568\n",
            "Dice score : 0.031125597655773163\n",
            "Val loss 0.0553\n",
            "Dice score : 0.03877977654337883\n",
            "Val loss 0.0568\n",
            "Dice score : 0.04198716580867767\n",
            "Val loss 0.0528\n",
            "Dice score : 0.03155222535133362\n",
            "Val loss 0.0524\n",
            "Dice score : 0.03808613866567612\n",
            "Val loss 0.0508\n",
            "Dice score : 0.02342940680682659\n",
            "Val loss 0.0521\n",
            "Dice score : 0.03575325384736061\n",
            "Val loss 0.0542\n",
            "Dice score : 0.0452345572412014\n",
            "Val loss 0.0530\n",
            "Dice score : 0.030949680134654045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 23/100 [30:52<1:41:34, 79.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0024 / 0100 | batch 0000 / 0050 | loss 0.0427\n",
            "Train: epoch 0024 / 0100 | batch 0001 / 0050 | loss 0.0410\n",
            "Train: epoch 0024 / 0100 | batch 0002 / 0050 | loss 0.0421\n",
            "Train: epoch 0024 / 0100 | batch 0003 / 0050 | loss 0.0460\n",
            "Train: epoch 0024 / 0100 | batch 0004 / 0050 | loss 0.0446\n",
            "Train: epoch 0024 / 0100 | batch 0005 / 0050 | loss 0.0468\n",
            "Train: epoch 0024 / 0100 | batch 0006 / 0050 | loss 0.0460\n",
            "Train: epoch 0024 / 0100 | batch 0007 / 0050 | loss 0.0481\n",
            "Train: epoch 0024 / 0100 | batch 0008 / 0050 | loss 0.0471\n",
            "Train: epoch 0024 / 0100 | batch 0009 / 0050 | loss 0.0471\n",
            "Train: epoch 0024 / 0100 | batch 0010 / 0050 | loss 0.0484\n",
            "Train: epoch 0024 / 0100 | batch 0011 / 0050 | loss 0.0480\n",
            "Train: epoch 0024 / 0100 | batch 0012 / 0050 | loss 0.0476\n",
            "Train: epoch 0024 / 0100 | batch 0013 / 0050 | loss 0.0488\n",
            "Train: epoch 0024 / 0100 | batch 0014 / 0050 | loss 0.0494\n",
            "Train: epoch 0024 / 0100 | batch 0015 / 0050 | loss 0.0497\n",
            "Train: epoch 0024 / 0100 | batch 0016 / 0050 | loss 0.0500\n",
            "Train: epoch 0024 / 0100 | batch 0017 / 0050 | loss 0.0497\n",
            "Train: epoch 0024 / 0100 | batch 0018 / 0050 | loss 0.0495\n",
            "Train: epoch 0024 / 0100 | batch 0019 / 0050 | loss 0.0494\n",
            "Train: epoch 0024 / 0100 | batch 0020 / 0050 | loss 0.0509\n",
            "Train: epoch 0024 / 0100 | batch 0021 / 0050 | loss 0.0522\n",
            "Train: epoch 0024 / 0100 | batch 0022 / 0050 | loss 0.0515\n",
            "Train: epoch 0024 / 0100 | batch 0023 / 0050 | loss 0.0511\n",
            "Train: epoch 0024 / 0100 | batch 0024 / 0050 | loss 0.0506\n",
            "Train: epoch 0024 / 0100 | batch 0025 / 0050 | loss 0.0516\n",
            "Train: epoch 0024 / 0100 | batch 0026 / 0050 | loss 0.0519\n",
            "Train: epoch 0024 / 0100 | batch 0027 / 0050 | loss 0.0518\n",
            "Train: epoch 0024 / 0100 | batch 0028 / 0050 | loss 0.0520\n",
            "Train: epoch 0024 / 0100 | batch 0029 / 0050 | loss 0.0519\n",
            "Train: epoch 0024 / 0100 | batch 0030 / 0050 | loss 0.0517\n",
            "Train: epoch 0024 / 0100 | batch 0031 / 0050 | loss 0.0520\n",
            "Train: epoch 0024 / 0100 | batch 0032 / 0050 | loss 0.0519\n",
            "Train: epoch 0024 / 0100 | batch 0033 / 0050 | loss 0.0515\n",
            "Train: epoch 0024 / 0100 | batch 0034 / 0050 | loss 0.0514\n",
            "Train: epoch 0024 / 0100 | batch 0035 / 0050 | loss 0.0523\n",
            "Train: epoch 0024 / 0100 | batch 0036 / 0050 | loss 0.0522\n",
            "Train: epoch 0024 / 0100 | batch 0037 / 0050 | loss 0.0525\n",
            "Train: epoch 0024 / 0100 | batch 0038 / 0050 | loss 0.0524\n",
            "Train: epoch 0024 / 0100 | batch 0039 / 0050 | loss 0.0524\n",
            "Train: epoch 0024 / 0100 | batch 0040 / 0050 | loss 0.0523\n",
            "Train: epoch 0024 / 0100 | batch 0041 / 0050 | loss 0.0523\n",
            "Train: epoch 0024 / 0100 | batch 0042 / 0050 | loss 0.0524\n",
            "Train: epoch 0024 / 0100 | batch 0043 / 0050 | loss 0.0521\n",
            "Train: epoch 0024 / 0100 | batch 0044 / 0050 | loss 0.0524\n",
            "Train: epoch 0024 / 0100 | batch 0045 / 0050 | loss 0.0526\n",
            "Train: epoch 0024 / 0100 | batch 0046 / 0050 | loss 0.0523\n",
            "Train: epoch 0024 / 0100 | batch 0047 / 0050 | loss 0.0524\n",
            "Train: epoch 0024 / 0100 | batch 0048 / 0050 | loss 0.0524\n",
            "Train: epoch 0024 / 0100 | batch 0049 / 0050 | loss 0.0521\n",
            "Val loss 0.0455\n",
            "Dice score : 0.027905791997909546\n",
            "Val loss 0.0620\n",
            "Dice score : 0.037932638078927994\n",
            "Val loss 0.0555\n",
            "Dice score : 0.03587689623236656\n",
            "Val loss 0.0587\n",
            "Dice score : 0.03714803606271744\n",
            "Val loss 0.0558\n",
            "Dice score : 0.03864732384681702\n",
            "Val loss 0.0534\n",
            "Dice score : 0.029244743287563324\n",
            "Val loss 0.0572\n",
            "Dice score : 0.04692888259887695\n",
            "Val loss 0.0580\n",
            "Dice score : 0.05498058348894119\n",
            "Val loss 0.0585\n",
            "Dice score : 0.039696644991636276\n",
            "Val loss 0.0577\n",
            "Dice score : 0.04465634375810623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 24/100 [32:14<1:41:36, 80.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0025 / 0100 | batch 0000 / 0050 | loss 0.0631\n",
            "Train: epoch 0025 / 0100 | batch 0001 / 0050 | loss 0.0489\n",
            "Train: epoch 0025 / 0100 | batch 0002 / 0050 | loss 0.0464\n",
            "Train: epoch 0025 / 0100 | batch 0003 / 0050 | loss 0.0514\n",
            "Train: epoch 0025 / 0100 | batch 0004 / 0050 | loss 0.0495\n",
            "Train: epoch 0025 / 0100 | batch 0005 / 0050 | loss 0.0522\n",
            "Train: epoch 0025 / 0100 | batch 0006 / 0050 | loss 0.0552\n",
            "Train: epoch 0025 / 0100 | batch 0007 / 0050 | loss 0.0529\n",
            "Train: epoch 0025 / 0100 | batch 0008 / 0050 | loss 0.0542\n",
            "Train: epoch 0025 / 0100 | batch 0009 / 0050 | loss 0.0524\n",
            "Train: epoch 0025 / 0100 | batch 0010 / 0050 | loss 0.0511\n",
            "Train: epoch 0025 / 0100 | batch 0011 / 0050 | loss 0.0501\n",
            "Train: epoch 0025 / 0100 | batch 0012 / 0050 | loss 0.0504\n",
            "Train: epoch 0025 / 0100 | batch 0013 / 0050 | loss 0.0512\n",
            "Train: epoch 0025 / 0100 | batch 0014 / 0050 | loss 0.0507\n",
            "Train: epoch 0025 / 0100 | batch 0015 / 0050 | loss 0.0503\n",
            "Train: epoch 0025 / 0100 | batch 0016 / 0050 | loss 0.0498\n",
            "Train: epoch 0025 / 0100 | batch 0017 / 0050 | loss 0.0508\n",
            "Train: epoch 0025 / 0100 | batch 0018 / 0050 | loss 0.0503\n",
            "Train: epoch 0025 / 0100 | batch 0019 / 0050 | loss 0.0500\n",
            "Train: epoch 0025 / 0100 | batch 0020 / 0050 | loss 0.0504\n",
            "Train: epoch 0025 / 0100 | batch 0021 / 0050 | loss 0.0503\n",
            "Train: epoch 0025 / 0100 | batch 0022 / 0050 | loss 0.0500\n",
            "Train: epoch 0025 / 0100 | batch 0023 / 0050 | loss 0.0503\n",
            "Train: epoch 0025 / 0100 | batch 0024 / 0050 | loss 0.0498\n",
            "Train: epoch 0025 / 0100 | batch 0025 / 0050 | loss 0.0502\n",
            "Train: epoch 0025 / 0100 | batch 0026 / 0050 | loss 0.0499\n",
            "Train: epoch 0025 / 0100 | batch 0027 / 0050 | loss 0.0501\n",
            "Train: epoch 0025 / 0100 | batch 0028 / 0050 | loss 0.0497\n",
            "Train: epoch 0025 / 0100 | batch 0029 / 0050 | loss 0.0498\n",
            "Train: epoch 0025 / 0100 | batch 0030 / 0050 | loss 0.0497\n",
            "Train: epoch 0025 / 0100 | batch 0031 / 0050 | loss 0.0495\n",
            "Train: epoch 0025 / 0100 | batch 0032 / 0050 | loss 0.0498\n",
            "Train: epoch 0025 / 0100 | batch 0033 / 0050 | loss 0.0495\n",
            "Train: epoch 0025 / 0100 | batch 0034 / 0050 | loss 0.0496\n",
            "Train: epoch 0025 / 0100 | batch 0035 / 0050 | loss 0.0499\n",
            "Train: epoch 0025 / 0100 | batch 0036 / 0050 | loss 0.0504\n",
            "Train: epoch 0025 / 0100 | batch 0037 / 0050 | loss 0.0506\n",
            "Train: epoch 0025 / 0100 | batch 0038 / 0050 | loss 0.0510\n",
            "Train: epoch 0025 / 0100 | batch 0039 / 0050 | loss 0.0509\n",
            "Train: epoch 0025 / 0100 | batch 0040 / 0050 | loss 0.0507\n",
            "Train: epoch 0025 / 0100 | batch 0041 / 0050 | loss 0.0514\n",
            "Train: epoch 0025 / 0100 | batch 0042 / 0050 | loss 0.0511\n",
            "Train: epoch 0025 / 0100 | batch 0043 / 0050 | loss 0.0515\n",
            "Train: epoch 0025 / 0100 | batch 0044 / 0050 | loss 0.0512\n",
            "Train: epoch 0025 / 0100 | batch 0045 / 0050 | loss 0.0518\n",
            "Train: epoch 0025 / 0100 | batch 0046 / 0050 | loss 0.0521\n",
            "Train: epoch 0025 / 0100 | batch 0047 / 0050 | loss 0.0517\n",
            "Train: epoch 0025 / 0100 | batch 0048 / 0050 | loss 0.0515\n",
            "Train: epoch 0025 / 0100 | batch 0049 / 0050 | loss 0.0514\n",
            "Val loss 0.0372\n",
            "Dice score : 0.03491358831524849\n",
            "Val loss 0.0368\n",
            "Dice score : 0.028070928528904915\n",
            "Val loss 0.0373\n",
            "Dice score : 0.026080505922436714\n",
            "Val loss 0.0463\n",
            "Dice score : 0.04770800471305847\n",
            "Val loss 0.0450\n",
            "Dice score : 0.024746736511588097\n",
            "Val loss 0.0437\n",
            "Dice score : 0.03530740365386009\n",
            "Val loss 0.0435\n",
            "Dice score : 0.03704212233424187\n",
            "Val loss 0.0453\n",
            "Dice score : 0.03919823095202446\n",
            "Val loss 0.0494\n",
            "Dice score : 0.05447284132242203\n",
            "Val loss 0.0492\n",
            "Dice score : 0.02602611482143402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 25/100 [33:36<1:40:46, 80.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0026 / 0100 | batch 0000 / 0050 | loss 0.0672\n",
            "Train: epoch 0026 / 0100 | batch 0001 / 0050 | loss 0.0567\n",
            "Train: epoch 0026 / 0100 | batch 0002 / 0050 | loss 0.0559\n",
            "Train: epoch 0026 / 0100 | batch 0003 / 0050 | loss 0.0553\n",
            "Train: epoch 0026 / 0100 | batch 0004 / 0050 | loss 0.0539\n",
            "Train: epoch 0026 / 0100 | batch 0005 / 0050 | loss 0.0525\n",
            "Train: epoch 0026 / 0100 | batch 0006 / 0050 | loss 0.0546\n",
            "Train: epoch 0026 / 0100 | batch 0007 / 0050 | loss 0.0539\n",
            "Train: epoch 0026 / 0100 | batch 0008 / 0050 | loss 0.0540\n",
            "Train: epoch 0026 / 0100 | batch 0009 / 0050 | loss 0.0525\n",
            "Train: epoch 0026 / 0100 | batch 0010 / 0050 | loss 0.0539\n",
            "Train: epoch 0026 / 0100 | batch 0011 / 0050 | loss 0.0548\n",
            "Train: epoch 0026 / 0100 | batch 0012 / 0050 | loss 0.0550\n",
            "Train: epoch 0026 / 0100 | batch 0013 / 0050 | loss 0.0554\n",
            "Train: epoch 0026 / 0100 | batch 0014 / 0050 | loss 0.0546\n",
            "Train: epoch 0026 / 0100 | batch 0015 / 0050 | loss 0.0537\n",
            "Train: epoch 0026 / 0100 | batch 0016 / 0050 | loss 0.0525\n",
            "Train: epoch 0026 / 0100 | batch 0017 / 0050 | loss 0.0552\n",
            "Train: epoch 0026 / 0100 | batch 0018 / 0050 | loss 0.0543\n",
            "Train: epoch 0026 / 0100 | batch 0019 / 0050 | loss 0.0538\n",
            "Train: epoch 0026 / 0100 | batch 0020 / 0050 | loss 0.0531\n",
            "Train: epoch 0026 / 0100 | batch 0021 / 0050 | loss 0.0524\n",
            "Train: epoch 0026 / 0100 | batch 0022 / 0050 | loss 0.0532\n",
            "Train: epoch 0026 / 0100 | batch 0023 / 0050 | loss 0.0530\n",
            "Train: epoch 0026 / 0100 | batch 0024 / 0050 | loss 0.0533\n",
            "Train: epoch 0026 / 0100 | batch 0025 / 0050 | loss 0.0531\n",
            "Train: epoch 0026 / 0100 | batch 0026 / 0050 | loss 0.0527\n",
            "Train: epoch 0026 / 0100 | batch 0027 / 0050 | loss 0.0521\n",
            "Train: epoch 0026 / 0100 | batch 0028 / 0050 | loss 0.0525\n",
            "Train: epoch 0026 / 0100 | batch 0029 / 0050 | loss 0.0527\n",
            "Train: epoch 0026 / 0100 | batch 0030 / 0050 | loss 0.0531\n",
            "Train: epoch 0026 / 0100 | batch 0031 / 0050 | loss 0.0528\n",
            "Train: epoch 0026 / 0100 | batch 0032 / 0050 | loss 0.0524\n",
            "Train: epoch 0026 / 0100 | batch 0033 / 0050 | loss 0.0530\n",
            "Train: epoch 0026 / 0100 | batch 0034 / 0050 | loss 0.0527\n",
            "Train: epoch 0026 / 0100 | batch 0035 / 0050 | loss 0.0525\n",
            "Train: epoch 0026 / 0100 | batch 0036 / 0050 | loss 0.0527\n",
            "Train: epoch 0026 / 0100 | batch 0037 / 0050 | loss 0.0525\n",
            "Train: epoch 0026 / 0100 | batch 0038 / 0050 | loss 0.0522\n",
            "Train: epoch 0026 / 0100 | batch 0039 / 0050 | loss 0.0523\n",
            "Train: epoch 0026 / 0100 | batch 0040 / 0050 | loss 0.0520\n",
            "Train: epoch 0026 / 0100 | batch 0041 / 0050 | loss 0.0517\n",
            "Train: epoch 0026 / 0100 | batch 0042 / 0050 | loss 0.0514\n",
            "Train: epoch 0026 / 0100 | batch 0043 / 0050 | loss 0.0513\n",
            "Train: epoch 0026 / 0100 | batch 0044 / 0050 | loss 0.0519\n",
            "Train: epoch 0026 / 0100 | batch 0045 / 0050 | loss 0.0516\n",
            "Train: epoch 0026 / 0100 | batch 0046 / 0050 | loss 0.0517\n",
            "Train: epoch 0026 / 0100 | batch 0047 / 0050 | loss 0.0514\n",
            "Train: epoch 0026 / 0100 | batch 0048 / 0050 | loss 0.0515\n",
            "Train: epoch 0026 / 0100 | batch 0049 / 0050 | loss 0.0515\n",
            "Val loss 0.0379\n",
            "Dice score : 0.02009674906730652\n",
            "Val loss 0.0419\n",
            "Dice score : 0.03312816843390465\n",
            "Val loss 0.0541\n",
            "Dice score : 0.04641557112336159\n",
            "Val loss 0.0564\n",
            "Dice score : 0.051389098167419434\n",
            "Val loss 0.0560\n",
            "Dice score : 0.02736377716064453\n",
            "Val loss 0.0556\n",
            "Dice score : 0.01396566815674305\n",
            "Val loss 0.0591\n",
            "Dice score : 0.056008338928222656\n",
            "Val loss 0.0571\n",
            "Dice score : 0.02857072278857231\n",
            "Val loss 0.0567\n",
            "Dice score : 0.03084009513258934\n",
            "Val loss 0.0570\n",
            "Dice score : 0.034456972032785416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 26/100 [34:55<1:39:02, 80.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0027 / 0100 | batch 0000 / 0050 | loss 0.0508\n",
            "Train: epoch 0027 / 0100 | batch 0001 / 0050 | loss 0.0518\n",
            "Train: epoch 0027 / 0100 | batch 0002 / 0050 | loss 0.0478\n",
            "Train: epoch 0027 / 0100 | batch 0003 / 0050 | loss 0.0511\n",
            "Train: epoch 0027 / 0100 | batch 0004 / 0050 | loss 0.0571\n",
            "Train: epoch 0027 / 0100 | batch 0005 / 0050 | loss 0.0541\n",
            "Train: epoch 0027 / 0100 | batch 0006 / 0050 | loss 0.0549\n",
            "Train: epoch 0027 / 0100 | batch 0007 / 0050 | loss 0.0526\n",
            "Train: epoch 0027 / 0100 | batch 0008 / 0050 | loss 0.0505\n",
            "Train: epoch 0027 / 0100 | batch 0009 / 0050 | loss 0.0516\n",
            "Train: epoch 0027 / 0100 | batch 0010 / 0050 | loss 0.0523\n",
            "Train: epoch 0027 / 0100 | batch 0011 / 0050 | loss 0.0533\n",
            "Train: epoch 0027 / 0100 | batch 0012 / 0050 | loss 0.0521\n",
            "Train: epoch 0027 / 0100 | batch 0013 / 0050 | loss 0.0519\n",
            "Train: epoch 0027 / 0100 | batch 0014 / 0050 | loss 0.0510\n",
            "Train: epoch 0027 / 0100 | batch 0015 / 0050 | loss 0.0513\n",
            "Train: epoch 0027 / 0100 | batch 0016 / 0050 | loss 0.0538\n",
            "Train: epoch 0027 / 0100 | batch 0017 / 0050 | loss 0.0543\n",
            "Train: epoch 0027 / 0100 | batch 0018 / 0050 | loss 0.0538\n",
            "Train: epoch 0027 / 0100 | batch 0019 / 0050 | loss 0.0528\n",
            "Train: epoch 0027 / 0100 | batch 0020 / 0050 | loss 0.0532\n",
            "Train: epoch 0027 / 0100 | batch 0021 / 0050 | loss 0.0532\n",
            "Train: epoch 0027 / 0100 | batch 0022 / 0050 | loss 0.0530\n",
            "Train: epoch 0027 / 0100 | batch 0023 / 0050 | loss 0.0529\n",
            "Train: epoch 0027 / 0100 | batch 0024 / 0050 | loss 0.0524\n",
            "Train: epoch 0027 / 0100 | batch 0025 / 0050 | loss 0.0528\n",
            "Train: epoch 0027 / 0100 | batch 0026 / 0050 | loss 0.0535\n",
            "Train: epoch 0027 / 0100 | batch 0027 / 0050 | loss 0.0531\n",
            "Train: epoch 0027 / 0100 | batch 0028 / 0050 | loss 0.0526\n",
            "Train: epoch 0027 / 0100 | batch 0029 / 0050 | loss 0.0522\n",
            "Train: epoch 0027 / 0100 | batch 0030 / 0050 | loss 0.0518\n",
            "Train: epoch 0027 / 0100 | batch 0031 / 0050 | loss 0.0516\n",
            "Train: epoch 0027 / 0100 | batch 0032 / 0050 | loss 0.0517\n",
            "Train: epoch 0027 / 0100 | batch 0033 / 0050 | loss 0.0516\n",
            "Train: epoch 0027 / 0100 | batch 0034 / 0050 | loss 0.0512\n",
            "Train: epoch 0027 / 0100 | batch 0035 / 0050 | loss 0.0510\n",
            "Train: epoch 0027 / 0100 | batch 0036 / 0050 | loss 0.0510\n",
            "Train: epoch 0027 / 0100 | batch 0037 / 0050 | loss 0.0509\n",
            "Train: epoch 0027 / 0100 | batch 0038 / 0050 | loss 0.0509\n",
            "Train: epoch 0027 / 0100 | batch 0039 / 0050 | loss 0.0507\n",
            "Train: epoch 0027 / 0100 | batch 0040 / 0050 | loss 0.0505\n",
            "Train: epoch 0027 / 0100 | batch 0041 / 0050 | loss 0.0502\n",
            "Train: epoch 0027 / 0100 | batch 0042 / 0050 | loss 0.0503\n",
            "Train: epoch 0027 / 0100 | batch 0043 / 0050 | loss 0.0503\n",
            "Train: epoch 0027 / 0100 | batch 0044 / 0050 | loss 0.0510\n",
            "Train: epoch 0027 / 0100 | batch 0045 / 0050 | loss 0.0513\n",
            "Train: epoch 0027 / 0100 | batch 0046 / 0050 | loss 0.0513\n",
            "Train: epoch 0027 / 0100 | batch 0047 / 0050 | loss 0.0517\n",
            "Train: epoch 0027 / 0100 | batch 0048 / 0050 | loss 0.0515\n",
            "Train: epoch 0027 / 0100 | batch 0049 / 0050 | loss 0.0514\n",
            "Val loss 0.0374\n",
            "Dice score : 0.02674645185470581\n",
            "Val loss 0.0395\n",
            "Dice score : 0.03097894974052906\n",
            "Val loss 0.0400\n",
            "Dice score : 0.024615813046693802\n",
            "Val loss 0.0446\n",
            "Dice score : 0.055321887135505676\n",
            "Val loss 0.0507\n",
            "Dice score : 0.03905168175697327\n",
            "Val loss 0.0546\n",
            "Dice score : 0.03875097632408142\n",
            "Val loss 0.0526\n",
            "Dice score : 0.05919843539595604\n",
            "Val loss 0.0506\n",
            "Dice score : 0.03795386105775833\n",
            "Val loss 0.0518\n",
            "Dice score : 0.04269571229815483\n",
            "Val loss 0.0522\n",
            "Dice score : 0.032848842442035675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 27/100 [36:16<1:37:50, 80.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0028 / 0100 | batch 0000 / 0050 | loss 0.0559\n",
            "Train: epoch 0028 / 0100 | batch 0001 / 0050 | loss 0.0533\n",
            "Train: epoch 0028 / 0100 | batch 0002 / 0050 | loss 0.0509\n",
            "Train: epoch 0028 / 0100 | batch 0003 / 0050 | loss 0.0525\n",
            "Train: epoch 0028 / 0100 | batch 0004 / 0050 | loss 0.0515\n",
            "Train: epoch 0028 / 0100 | batch 0005 / 0050 | loss 0.0505\n",
            "Train: epoch 0028 / 0100 | batch 0006 / 0050 | loss 0.0492\n",
            "Train: epoch 0028 / 0100 | batch 0007 / 0050 | loss 0.0489\n",
            "Train: epoch 0028 / 0100 | batch 0008 / 0050 | loss 0.0494\n",
            "Train: epoch 0028 / 0100 | batch 0009 / 0050 | loss 0.0498\n",
            "Train: epoch 0028 / 0100 | batch 0010 / 0050 | loss 0.0519\n",
            "Train: epoch 0028 / 0100 | batch 0011 / 0050 | loss 0.0510\n",
            "Train: epoch 0028 / 0100 | batch 0012 / 0050 | loss 0.0503\n",
            "Train: epoch 0028 / 0100 | batch 0013 / 0050 | loss 0.0504\n",
            "Train: epoch 0028 / 0100 | batch 0014 / 0050 | loss 0.0501\n",
            "Train: epoch 0028 / 0100 | batch 0015 / 0050 | loss 0.0494\n",
            "Train: epoch 0028 / 0100 | batch 0016 / 0050 | loss 0.0487\n",
            "Train: epoch 0028 / 0100 | batch 0017 / 0050 | loss 0.0484\n",
            "Train: epoch 0028 / 0100 | batch 0018 / 0050 | loss 0.0482\n",
            "Train: epoch 0028 / 0100 | batch 0019 / 0050 | loss 0.0490\n",
            "Train: epoch 0028 / 0100 | batch 0020 / 0050 | loss 0.0492\n",
            "Train: epoch 0028 / 0100 | batch 0021 / 0050 | loss 0.0490\n",
            "Train: epoch 0028 / 0100 | batch 0022 / 0050 | loss 0.0503\n",
            "Train: epoch 0028 / 0100 | batch 0023 / 0050 | loss 0.0521\n",
            "Train: epoch 0028 / 0100 | batch 0024 / 0050 | loss 0.0519\n",
            "Train: epoch 0028 / 0100 | batch 0025 / 0050 | loss 0.0517\n",
            "Train: epoch 0028 / 0100 | batch 0026 / 0050 | loss 0.0518\n",
            "Train: epoch 0028 / 0100 | batch 0027 / 0050 | loss 0.0514\n",
            "Train: epoch 0028 / 0100 | batch 0028 / 0050 | loss 0.0508\n",
            "Train: epoch 0028 / 0100 | batch 0029 / 0050 | loss 0.0517\n",
            "Train: epoch 0028 / 0100 | batch 0030 / 0050 | loss 0.0512\n",
            "Train: epoch 0028 / 0100 | batch 0031 / 0050 | loss 0.0508\n",
            "Train: epoch 0028 / 0100 | batch 0032 / 0050 | loss 0.0505\n",
            "Train: epoch 0028 / 0100 | batch 0033 / 0050 | loss 0.0500\n",
            "Train: epoch 0028 / 0100 | batch 0034 / 0050 | loss 0.0499\n",
            "Train: epoch 0028 / 0100 | batch 0035 / 0050 | loss 0.0497\n",
            "Train: epoch 0028 / 0100 | batch 0036 / 0050 | loss 0.0500\n",
            "Train: epoch 0028 / 0100 | batch 0037 / 0050 | loss 0.0500\n",
            "Train: epoch 0028 / 0100 | batch 0038 / 0050 | loss 0.0505\n",
            "Train: epoch 0028 / 0100 | batch 0039 / 0050 | loss 0.0502\n",
            "Train: epoch 0028 / 0100 | batch 0040 / 0050 | loss 0.0505\n",
            "Train: epoch 0028 / 0100 | batch 0041 / 0050 | loss 0.0511\n",
            "Train: epoch 0028 / 0100 | batch 0042 / 0050 | loss 0.0510\n",
            "Train: epoch 0028 / 0100 | batch 0043 / 0050 | loss 0.0509\n",
            "Train: epoch 0028 / 0100 | batch 0044 / 0050 | loss 0.0508\n",
            "Train: epoch 0028 / 0100 | batch 0045 / 0050 | loss 0.0510\n",
            "Train: epoch 0028 / 0100 | batch 0046 / 0050 | loss 0.0516\n",
            "Train: epoch 0028 / 0100 | batch 0047 / 0050 | loss 0.0516\n",
            "Train: epoch 0028 / 0100 | batch 0048 / 0050 | loss 0.0516\n",
            "Train: epoch 0028 / 0100 | batch 0049 / 0050 | loss 0.0519\n",
            "Val loss 0.0331\n",
            "Dice score : 0.02561328373849392\n",
            "Val loss 0.0383\n",
            "Dice score : 0.034157346934080124\n",
            "Val loss 0.0464\n",
            "Dice score : 0.03830798342823982\n",
            "Val loss 0.0536\n",
            "Dice score : 0.04065044969320297\n",
            "Val loss 0.0541\n",
            "Dice score : 0.036289773881435394\n",
            "Val loss 0.0519\n",
            "Dice score : 0.02979104034602642\n",
            "Val loss 0.0497\n",
            "Dice score : 0.020080603659152985\n",
            "Val loss 0.0521\n",
            "Dice score : 0.05667908862233162\n",
            "Val loss 0.0504\n",
            "Dice score : 0.028628801926970482\n",
            "Val loss 0.0491\n",
            "Dice score : 0.029783688485622406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 28/100 [37:36<1:36:21, 80.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0029 / 0100 | batch 0000 / 0050 | loss 0.0680\n",
            "Train: epoch 0029 / 0100 | batch 0001 / 0050 | loss 0.0581\n",
            "Train: epoch 0029 / 0100 | batch 0002 / 0050 | loss 0.0531\n",
            "Train: epoch 0029 / 0100 | batch 0003 / 0050 | loss 0.0507\n",
            "Train: epoch 0029 / 0100 | batch 0004 / 0050 | loss 0.0619\n",
            "Train: epoch 0029 / 0100 | batch 0005 / 0050 | loss 0.0605\n",
            "Train: epoch 0029 / 0100 | batch 0006 / 0050 | loss 0.0592\n",
            "Train: epoch 0029 / 0100 | batch 0007 / 0050 | loss 0.0564\n",
            "Train: epoch 0029 / 0100 | batch 0008 / 0050 | loss 0.0549\n",
            "Train: epoch 0029 / 0100 | batch 0009 / 0050 | loss 0.0593\n",
            "Train: epoch 0029 / 0100 | batch 0010 / 0050 | loss 0.0579\n",
            "Train: epoch 0029 / 0100 | batch 0011 / 0050 | loss 0.0585\n",
            "Train: epoch 0029 / 0100 | batch 0012 / 0050 | loss 0.0582\n",
            "Train: epoch 0029 / 0100 | batch 0013 / 0050 | loss 0.0565\n",
            "Train: epoch 0029 / 0100 | batch 0014 / 0050 | loss 0.0553\n",
            "Train: epoch 0029 / 0100 | batch 0015 / 0050 | loss 0.0560\n",
            "Train: epoch 0029 / 0100 | batch 0016 / 0050 | loss 0.0563\n",
            "Train: epoch 0029 / 0100 | batch 0017 / 0050 | loss 0.0556\n",
            "Train: epoch 0029 / 0100 | batch 0018 / 0050 | loss 0.0546\n",
            "Train: epoch 0029 / 0100 | batch 0019 / 0050 | loss 0.0550\n",
            "Train: epoch 0029 / 0100 | batch 0020 / 0050 | loss 0.0547\n",
            "Train: epoch 0029 / 0100 | batch 0021 / 0050 | loss 0.0539\n",
            "Train: epoch 0029 / 0100 | batch 0022 / 0050 | loss 0.0539\n",
            "Train: epoch 0029 / 0100 | batch 0023 / 0050 | loss 0.0538\n",
            "Train: epoch 0029 / 0100 | batch 0024 / 0050 | loss 0.0531\n",
            "Train: epoch 0029 / 0100 | batch 0025 / 0050 | loss 0.0523\n",
            "Train: epoch 0029 / 0100 | batch 0026 / 0050 | loss 0.0527\n",
            "Train: epoch 0029 / 0100 | batch 0027 / 0050 | loss 0.0520\n",
            "Train: epoch 0029 / 0100 | batch 0028 / 0050 | loss 0.0517\n",
            "Train: epoch 0029 / 0100 | batch 0029 / 0050 | loss 0.0516\n",
            "Train: epoch 0029 / 0100 | batch 0030 / 0050 | loss 0.0519\n",
            "Train: epoch 0029 / 0100 | batch 0031 / 0050 | loss 0.0523\n",
            "Train: epoch 0029 / 0100 | batch 0032 / 0050 | loss 0.0533\n",
            "Train: epoch 0029 / 0100 | batch 0033 / 0050 | loss 0.0527\n",
            "Train: epoch 0029 / 0100 | batch 0034 / 0050 | loss 0.0530\n",
            "Train: epoch 0029 / 0100 | batch 0035 / 0050 | loss 0.0528\n",
            "Train: epoch 0029 / 0100 | batch 0036 / 0050 | loss 0.0529\n",
            "Train: epoch 0029 / 0100 | batch 0037 / 0050 | loss 0.0529\n",
            "Train: epoch 0029 / 0100 | batch 0038 / 0050 | loss 0.0523\n",
            "Train: epoch 0029 / 0100 | batch 0039 / 0050 | loss 0.0526\n",
            "Train: epoch 0029 / 0100 | batch 0040 / 0050 | loss 0.0524\n",
            "Train: epoch 0029 / 0100 | batch 0041 / 0050 | loss 0.0522\n",
            "Train: epoch 0029 / 0100 | batch 0042 / 0050 | loss 0.0522\n",
            "Train: epoch 0029 / 0100 | batch 0043 / 0050 | loss 0.0519\n",
            "Train: epoch 0029 / 0100 | batch 0044 / 0050 | loss 0.0516\n",
            "Train: epoch 0029 / 0100 | batch 0045 / 0050 | loss 0.0517\n",
            "Train: epoch 0029 / 0100 | batch 0046 / 0050 | loss 0.0517\n",
            "Train: epoch 0029 / 0100 | batch 0047 / 0050 | loss 0.0520\n",
            "Train: epoch 0029 / 0100 | batch 0048 / 0050 | loss 0.0516\n",
            "Train: epoch 0029 / 0100 | batch 0049 / 0050 | loss 0.0514\n",
            "Val loss 0.0578\n",
            "Dice score : 0.039283160120248795\n",
            "Val loss 0.0504\n",
            "Dice score : 0.02534284070134163\n",
            "Val loss 0.0477\n",
            "Dice score : 0.024676181375980377\n",
            "Val loss 0.0457\n",
            "Dice score : 0.028381681069731712\n",
            "Val loss 0.0512\n",
            "Dice score : 0.04631375893950462\n",
            "Val loss 0.0493\n",
            "Dice score : 0.029823344200849533\n",
            "Val loss 0.0542\n",
            "Dice score : 0.03913778439164162\n",
            "Val loss 0.0526\n",
            "Dice score : 0.03647438809275627\n",
            "Val loss 0.0524\n",
            "Dice score : 0.02353152632713318\n",
            "Val loss 0.0538\n",
            "Dice score : 0.03494549170136452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 29/100 [38:54<1:34:15, 79.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0030 / 0100 | batch 0000 / 0050 | loss 0.0370\n",
            "Train: epoch 0030 / 0100 | batch 0001 / 0050 | loss 0.0525\n",
            "Train: epoch 0030 / 0100 | batch 0002 / 0050 | loss 0.0553\n",
            "Train: epoch 0030 / 0100 | batch 0003 / 0050 | loss 0.0512\n",
            "Train: epoch 0030 / 0100 | batch 0004 / 0050 | loss 0.0525\n",
            "Train: epoch 0030 / 0100 | batch 0005 / 0050 | loss 0.0500\n",
            "Train: epoch 0030 / 0100 | batch 0006 / 0050 | loss 0.0482\n",
            "Train: epoch 0030 / 0100 | batch 0007 / 0050 | loss 0.0466\n",
            "Train: epoch 0030 / 0100 | batch 0008 / 0050 | loss 0.0478\n",
            "Train: epoch 0030 / 0100 | batch 0009 / 0050 | loss 0.0503\n",
            "Train: epoch 0030 / 0100 | batch 0010 / 0050 | loss 0.0496\n",
            "Train: epoch 0030 / 0100 | batch 0011 / 0050 | loss 0.0495\n",
            "Train: epoch 0030 / 0100 | batch 0012 / 0050 | loss 0.0487\n",
            "Train: epoch 0030 / 0100 | batch 0013 / 0050 | loss 0.0481\n",
            "Train: epoch 0030 / 0100 | batch 0014 / 0050 | loss 0.0470\n",
            "Train: epoch 0030 / 0100 | batch 0015 / 0050 | loss 0.0483\n",
            "Train: epoch 0030 / 0100 | batch 0016 / 0050 | loss 0.0488\n",
            "Train: epoch 0030 / 0100 | batch 0017 / 0050 | loss 0.0482\n",
            "Train: epoch 0030 / 0100 | batch 0018 / 0050 | loss 0.0489\n",
            "Train: epoch 0030 / 0100 | batch 0019 / 0050 | loss 0.0487\n",
            "Train: epoch 0030 / 0100 | batch 0020 / 0050 | loss 0.0489\n",
            "Train: epoch 0030 / 0100 | batch 0021 / 0050 | loss 0.0494\n",
            "Train: epoch 0030 / 0100 | batch 0022 / 0050 | loss 0.0502\n",
            "Train: epoch 0030 / 0100 | batch 0023 / 0050 | loss 0.0499\n",
            "Train: epoch 0030 / 0100 | batch 0024 / 0050 | loss 0.0496\n",
            "Train: epoch 0030 / 0100 | batch 0025 / 0050 | loss 0.0491\n",
            "Train: epoch 0030 / 0100 | batch 0026 / 0050 | loss 0.0489\n",
            "Train: epoch 0030 / 0100 | batch 0027 / 0050 | loss 0.0490\n",
            "Train: epoch 0030 / 0100 | batch 0028 / 0050 | loss 0.0492\n",
            "Train: epoch 0030 / 0100 | batch 0029 / 0050 | loss 0.0494\n",
            "Train: epoch 0030 / 0100 | batch 0030 / 0050 | loss 0.0489\n",
            "Train: epoch 0030 / 0100 | batch 0031 / 0050 | loss 0.0496\n",
            "Train: epoch 0030 / 0100 | batch 0032 / 0050 | loss 0.0492\n",
            "Train: epoch 0030 / 0100 | batch 0033 / 0050 | loss 0.0495\n",
            "Train: epoch 0030 / 0100 | batch 0034 / 0050 | loss 0.0496\n",
            "Train: epoch 0030 / 0100 | batch 0035 / 0050 | loss 0.0496\n",
            "Train: epoch 0030 / 0100 | batch 0036 / 0050 | loss 0.0502\n",
            "Train: epoch 0030 / 0100 | batch 0037 / 0050 | loss 0.0498\n",
            "Train: epoch 0030 / 0100 | batch 0038 / 0050 | loss 0.0499\n",
            "Train: epoch 0030 / 0100 | batch 0039 / 0050 | loss 0.0495\n",
            "Train: epoch 0030 / 0100 | batch 0040 / 0050 | loss 0.0498\n",
            "Train: epoch 0030 / 0100 | batch 0041 / 0050 | loss 0.0506\n",
            "Train: epoch 0030 / 0100 | batch 0042 / 0050 | loss 0.0504\n",
            "Train: epoch 0030 / 0100 | batch 0043 / 0050 | loss 0.0505\n",
            "Train: epoch 0030 / 0100 | batch 0044 / 0050 | loss 0.0501\n",
            "Train: epoch 0030 / 0100 | batch 0045 / 0050 | loss 0.0504\n",
            "Train: epoch 0030 / 0100 | batch 0046 / 0050 | loss 0.0501\n",
            "Train: epoch 0030 / 0100 | batch 0047 / 0050 | loss 0.0504\n",
            "Train: epoch 0030 / 0100 | batch 0048 / 0050 | loss 0.0506\n",
            "Train: epoch 0030 / 0100 | batch 0049 / 0050 | loss 0.0507\n",
            "Val loss 0.0856\n",
            "Dice score : 0.04013621434569359\n",
            "Val loss 0.0928\n",
            "Dice score : 0.0486065112054348\n",
            "Val loss 0.0850\n",
            "Dice score : 0.029407214373350143\n",
            "Val loss 0.0806\n",
            "Dice score : 0.021426983177661896\n",
            "Val loss 0.0800\n",
            "Dice score : 0.024926161393523216\n",
            "Val loss 0.0787\n",
            "Dice score : 0.0375494658946991\n",
            "Val loss 0.0761\n",
            "Dice score : 0.03491780906915665\n",
            "Val loss 0.0743\n",
            "Dice score : 0.03762010857462883\n",
            "Val loss 0.0714\n",
            "Dice score : 0.03817087784409523\n",
            "Val loss 0.0750\n",
            "Dice score : 0.04172850400209427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 30/100 [40:13<1:32:32, 79.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0031 / 0100 | batch 0000 / 0050 | loss 0.0376\n",
            "Train: epoch 0031 / 0100 | batch 0001 / 0050 | loss 0.0446\n",
            "Train: epoch 0031 / 0100 | batch 0002 / 0050 | loss 0.0408\n",
            "Train: epoch 0031 / 0100 | batch 0003 / 0050 | loss 0.0414\n",
            "Train: epoch 0031 / 0100 | batch 0004 / 0050 | loss 0.0449\n",
            "Train: epoch 0031 / 0100 | batch 0005 / 0050 | loss 0.0478\n",
            "Train: epoch 0031 / 0100 | batch 0006 / 0050 | loss 0.0470\n",
            "Train: epoch 0031 / 0100 | batch 0007 / 0050 | loss 0.0475\n",
            "Train: epoch 0031 / 0100 | batch 0008 / 0050 | loss 0.0479\n",
            "Train: epoch 0031 / 0100 | batch 0009 / 0050 | loss 0.0469\n",
            "Train: epoch 0031 / 0100 | batch 0010 / 0050 | loss 0.0476\n",
            "Train: epoch 0031 / 0100 | batch 0011 / 0050 | loss 0.0477\n",
            "Train: epoch 0031 / 0100 | batch 0012 / 0050 | loss 0.0481\n",
            "Train: epoch 0031 / 0100 | batch 0013 / 0050 | loss 0.0489\n",
            "Train: epoch 0031 / 0100 | batch 0014 / 0050 | loss 0.0484\n",
            "Train: epoch 0031 / 0100 | batch 0015 / 0050 | loss 0.0482\n",
            "Train: epoch 0031 / 0100 | batch 0016 / 0050 | loss 0.0509\n",
            "Train: epoch 0031 / 0100 | batch 0017 / 0050 | loss 0.0520\n",
            "Train: epoch 0031 / 0100 | batch 0018 / 0050 | loss 0.0525\n",
            "Train: epoch 0031 / 0100 | batch 0019 / 0050 | loss 0.0524\n",
            "Train: epoch 0031 / 0100 | batch 0020 / 0050 | loss 0.0518\n",
            "Train: epoch 0031 / 0100 | batch 0021 / 0050 | loss 0.0515\n",
            "Train: epoch 0031 / 0100 | batch 0022 / 0050 | loss 0.0511\n",
            "Train: epoch 0031 / 0100 | batch 0023 / 0050 | loss 0.0507\n",
            "Train: epoch 0031 / 0100 | batch 0024 / 0050 | loss 0.0513\n",
            "Train: epoch 0031 / 0100 | batch 0025 / 0050 | loss 0.0509\n",
            "Train: epoch 0031 / 0100 | batch 0026 / 0050 | loss 0.0507\n",
            "Train: epoch 0031 / 0100 | batch 0027 / 0050 | loss 0.0510\n",
            "Train: epoch 0031 / 0100 | batch 0028 / 0050 | loss 0.0512\n",
            "Train: epoch 0031 / 0100 | batch 0029 / 0050 | loss 0.0509\n",
            "Train: epoch 0031 / 0100 | batch 0030 / 0050 | loss 0.0506\n",
            "Train: epoch 0031 / 0100 | batch 0031 / 0050 | loss 0.0500\n",
            "Train: epoch 0031 / 0100 | batch 0032 / 0050 | loss 0.0499\n",
            "Train: epoch 0031 / 0100 | batch 0033 / 0050 | loss 0.0495\n",
            "Train: epoch 0031 / 0100 | batch 0034 / 0050 | loss 0.0503\n",
            "Train: epoch 0031 / 0100 | batch 0035 / 0050 | loss 0.0499\n",
            "Train: epoch 0031 / 0100 | batch 0036 / 0050 | loss 0.0495\n",
            "Train: epoch 0031 / 0100 | batch 0037 / 0050 | loss 0.0492\n",
            "Train: epoch 0031 / 0100 | batch 0038 / 0050 | loss 0.0491\n",
            "Train: epoch 0031 / 0100 | batch 0039 / 0050 | loss 0.0489\n",
            "Train: epoch 0031 / 0100 | batch 0040 / 0050 | loss 0.0493\n",
            "Train: epoch 0031 / 0100 | batch 0041 / 0050 | loss 0.0501\n",
            "Train: epoch 0031 / 0100 | batch 0042 / 0050 | loss 0.0497\n",
            "Train: epoch 0031 / 0100 | batch 0043 / 0050 | loss 0.0501\n",
            "Train: epoch 0031 / 0100 | batch 0044 / 0050 | loss 0.0506\n",
            "Train: epoch 0031 / 0100 | batch 0045 / 0050 | loss 0.0508\n",
            "Train: epoch 0031 / 0100 | batch 0046 / 0050 | loss 0.0508\n",
            "Train: epoch 0031 / 0100 | batch 0047 / 0050 | loss 0.0506\n",
            "Train: epoch 0031 / 0100 | batch 0048 / 0050 | loss 0.0507\n",
            "Train: epoch 0031 / 0100 | batch 0049 / 0050 | loss 0.0508\n",
            "Val loss 0.0909\n",
            "Dice score : 0.037750329822301865\n",
            "Val loss 0.1146\n",
            "Dice score : 0.045041460543870926\n",
            "Val loss 0.0971\n",
            "Dice score : 0.05017092823982239\n",
            "Val loss 0.0895\n",
            "Dice score : 0.026922309771180153\n",
            "Val loss 0.0901\n",
            "Dice score : 0.023545440286397934\n",
            "Val loss 0.0970\n",
            "Dice score : 0.04472605139017105\n",
            "Val loss 0.0985\n",
            "Dice score : 0.047390032559633255\n",
            "Val loss 0.0942\n",
            "Dice score : 0.031713832169771194\n",
            "Val loss 0.0947\n",
            "Dice score : 0.026983313262462616\n",
            "Val loss 0.0946\n",
            "Dice score : 0.03527054935693741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 31/100 [41:35<1:32:06, 80.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0032 / 0100 | batch 0000 / 0050 | loss 0.0363\n",
            "Train: epoch 0032 / 0100 | batch 0001 / 0050 | loss 0.0344\n",
            "Train: epoch 0032 / 0100 | batch 0002 / 0050 | loss 0.0388\n",
            "Train: epoch 0032 / 0100 | batch 0003 / 0050 | loss 0.0424\n",
            "Train: epoch 0032 / 0100 | batch 0004 / 0050 | loss 0.0450\n",
            "Train: epoch 0032 / 0100 | batch 0005 / 0050 | loss 0.0468\n",
            "Train: epoch 0032 / 0100 | batch 0006 / 0050 | loss 0.0475\n",
            "Train: epoch 0032 / 0100 | batch 0007 / 0050 | loss 0.0487\n",
            "Train: epoch 0032 / 0100 | batch 0008 / 0050 | loss 0.0499\n",
            "Train: epoch 0032 / 0100 | batch 0009 / 0050 | loss 0.0499\n",
            "Train: epoch 0032 / 0100 | batch 0010 / 0050 | loss 0.0493\n",
            "Train: epoch 0032 / 0100 | batch 0011 / 0050 | loss 0.0483\n",
            "Train: epoch 0032 / 0100 | batch 0012 / 0050 | loss 0.0473\n",
            "Train: epoch 0032 / 0100 | batch 0013 / 0050 | loss 0.0468\n",
            "Train: epoch 0032 / 0100 | batch 0014 / 0050 | loss 0.0487\n",
            "Train: epoch 0032 / 0100 | batch 0015 / 0050 | loss 0.0480\n",
            "Train: epoch 0032 / 0100 | batch 0016 / 0050 | loss 0.0490\n",
            "Train: epoch 0032 / 0100 | batch 0017 / 0050 | loss 0.0483\n",
            "Train: epoch 0032 / 0100 | batch 0018 / 0050 | loss 0.0478\n",
            "Train: epoch 0032 / 0100 | batch 0019 / 0050 | loss 0.0476\n",
            "Train: epoch 0032 / 0100 | batch 0020 / 0050 | loss 0.0475\n",
            "Train: epoch 0032 / 0100 | batch 0021 / 0050 | loss 0.0486\n",
            "Train: epoch 0032 / 0100 | batch 0022 / 0050 | loss 0.0486\n",
            "Train: epoch 0032 / 0100 | batch 0023 / 0050 | loss 0.0483\n",
            "Train: epoch 0032 / 0100 | batch 0024 / 0050 | loss 0.0477\n",
            "Train: epoch 0032 / 0100 | batch 0025 / 0050 | loss 0.0485\n",
            "Train: epoch 0032 / 0100 | batch 0026 / 0050 | loss 0.0484\n",
            "Train: epoch 0032 / 0100 | batch 0027 / 0050 | loss 0.0483\n",
            "Train: epoch 0032 / 0100 | batch 0028 / 0050 | loss 0.0477\n",
            "Train: epoch 0032 / 0100 | batch 0029 / 0050 | loss 0.0484\n",
            "Train: epoch 0032 / 0100 | batch 0030 / 0050 | loss 0.0493\n",
            "Train: epoch 0032 / 0100 | batch 0031 / 0050 | loss 0.0489\n",
            "Train: epoch 0032 / 0100 | batch 0032 / 0050 | loss 0.0489\n",
            "Train: epoch 0032 / 0100 | batch 0033 / 0050 | loss 0.0486\n",
            "Train: epoch 0032 / 0100 | batch 0034 / 0050 | loss 0.0484\n",
            "Train: epoch 0032 / 0100 | batch 0035 / 0050 | loss 0.0486\n",
            "Train: epoch 0032 / 0100 | batch 0036 / 0050 | loss 0.0484\n",
            "Train: epoch 0032 / 0100 | batch 0037 / 0050 | loss 0.0481\n",
            "Train: epoch 0032 / 0100 | batch 0038 / 0050 | loss 0.0482\n",
            "Train: epoch 0032 / 0100 | batch 0039 / 0050 | loss 0.0479\n",
            "Train: epoch 0032 / 0100 | batch 0040 / 0050 | loss 0.0479\n",
            "Train: epoch 0032 / 0100 | batch 0041 / 0050 | loss 0.0487\n",
            "Train: epoch 0032 / 0100 | batch 0042 / 0050 | loss 0.0485\n",
            "Train: epoch 0032 / 0100 | batch 0043 / 0050 | loss 0.0488\n",
            "Train: epoch 0032 / 0100 | batch 0044 / 0050 | loss 0.0487\n",
            "Train: epoch 0032 / 0100 | batch 0045 / 0050 | loss 0.0487\n",
            "Train: epoch 0032 / 0100 | batch 0046 / 0050 | loss 0.0485\n",
            "Train: epoch 0032 / 0100 | batch 0047 / 0050 | loss 0.0486\n",
            "Train: epoch 0032 / 0100 | batch 0048 / 0050 | loss 0.0486\n",
            "Train: epoch 0032 / 0100 | batch 0049 / 0050 | loss 0.0484\n",
            "Val loss 0.0476\n",
            "Dice score : 0.03832899406552315\n",
            "Val loss 0.0467\n",
            "Dice score : 0.035120539367198944\n",
            "Val loss 0.0458\n",
            "Dice score : 0.034852515906095505\n",
            "Val loss 0.0461\n",
            "Dice score : 0.02664717100560665\n",
            "Val loss 0.0442\n",
            "Dice score : 0.03090130351483822\n",
            "Val loss 0.0428\n",
            "Dice score : 0.030849263072013855\n",
            "Val loss 0.0454\n",
            "Dice score : 0.057548921555280685\n",
            "Val loss 0.0470\n",
            "Dice score : 0.029324039816856384\n",
            "Val loss 0.0486\n",
            "Dice score : 0.045333825051784515\n",
            "Val loss 0.0479\n",
            "Dice score : 0.024081038311123848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 32/100 [42:55<1:30:50, 80.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0033 / 0100 | batch 0000 / 0050 | loss 0.0351\n",
            "Train: epoch 0033 / 0100 | batch 0001 / 0050 | loss 0.0407\n",
            "Train: epoch 0033 / 0100 | batch 0002 / 0050 | loss 0.0425\n",
            "Train: epoch 0033 / 0100 | batch 0003 / 0050 | loss 0.0425\n",
            "Train: epoch 0033 / 0100 | batch 0004 / 0050 | loss 0.0465\n",
            "Train: epoch 0033 / 0100 | batch 0005 / 0050 | loss 0.0451\n",
            "Train: epoch 0033 / 0100 | batch 0006 / 0050 | loss 0.0472\n",
            "Train: epoch 0033 / 0100 | batch 0007 / 0050 | loss 0.0502\n",
            "Train: epoch 0033 / 0100 | batch 0008 / 0050 | loss 0.0493\n",
            "Train: epoch 0033 / 0100 | batch 0009 / 0050 | loss 0.0479\n",
            "Train: epoch 0033 / 0100 | batch 0010 / 0050 | loss 0.0479\n",
            "Train: epoch 0033 / 0100 | batch 0011 / 0050 | loss 0.0474\n",
            "Train: epoch 0033 / 0100 | batch 0012 / 0050 | loss 0.0484\n",
            "Train: epoch 0033 / 0100 | batch 0013 / 0050 | loss 0.0486\n",
            "Train: epoch 0033 / 0100 | batch 0014 / 0050 | loss 0.0484\n",
            "Train: epoch 0033 / 0100 | batch 0015 / 0050 | loss 0.0491\n",
            "Train: epoch 0033 / 0100 | batch 0016 / 0050 | loss 0.0491\n",
            "Train: epoch 0033 / 0100 | batch 0017 / 0050 | loss 0.0486\n",
            "Train: epoch 0033 / 0100 | batch 0018 / 0050 | loss 0.0481\n",
            "Train: epoch 0033 / 0100 | batch 0019 / 0050 | loss 0.0478\n",
            "Train: epoch 0033 / 0100 | batch 0020 / 0050 | loss 0.0467\n",
            "Train: epoch 0033 / 0100 | batch 0021 / 0050 | loss 0.0478\n",
            "Train: epoch 0033 / 0100 | batch 0022 / 0050 | loss 0.0481\n",
            "Train: epoch 0033 / 0100 | batch 0023 / 0050 | loss 0.0477\n",
            "Train: epoch 0033 / 0100 | batch 0024 / 0050 | loss 0.0486\n",
            "Train: epoch 0033 / 0100 | batch 0025 / 0050 | loss 0.0487\n",
            "Train: epoch 0033 / 0100 | batch 0026 / 0050 | loss 0.0479\n",
            "Train: epoch 0033 / 0100 | batch 0027 / 0050 | loss 0.0484\n",
            "Train: epoch 0033 / 0100 | batch 0028 / 0050 | loss 0.0478\n",
            "Train: epoch 0033 / 0100 | batch 0029 / 0050 | loss 0.0476\n",
            "Train: epoch 0033 / 0100 | batch 0030 / 0050 | loss 0.0479\n",
            "Train: epoch 0033 / 0100 | batch 0031 / 0050 | loss 0.0479\n",
            "Train: epoch 0033 / 0100 | batch 0032 / 0050 | loss 0.0479\n",
            "Train: epoch 0033 / 0100 | batch 0033 / 0050 | loss 0.0475\n",
            "Train: epoch 0033 / 0100 | batch 0034 / 0050 | loss 0.0470\n",
            "Train: epoch 0033 / 0100 | batch 0035 / 0050 | loss 0.0467\n",
            "Train: epoch 0033 / 0100 | batch 0036 / 0050 | loss 0.0465\n",
            "Train: epoch 0033 / 0100 | batch 0037 / 0050 | loss 0.0463\n",
            "Train: epoch 0033 / 0100 | batch 0038 / 0050 | loss 0.0467\n",
            "Train: epoch 0033 / 0100 | batch 0039 / 0050 | loss 0.0464\n",
            "Train: epoch 0033 / 0100 | batch 0040 / 0050 | loss 0.0466\n",
            "Train: epoch 0033 / 0100 | batch 0041 / 0050 | loss 0.0467\n",
            "Train: epoch 0033 / 0100 | batch 0042 / 0050 | loss 0.0465\n",
            "Train: epoch 0033 / 0100 | batch 0043 / 0050 | loss 0.0465\n",
            "Train: epoch 0033 / 0100 | batch 0044 / 0050 | loss 0.0467\n",
            "Train: epoch 0033 / 0100 | batch 0045 / 0050 | loss 0.0466\n",
            "Train: epoch 0033 / 0100 | batch 0046 / 0050 | loss 0.0466\n",
            "Train: epoch 0033 / 0100 | batch 0047 / 0050 | loss 0.0466\n",
            "Train: epoch 0033 / 0100 | batch 0048 / 0050 | loss 0.0468\n",
            "Train: epoch 0033 / 0100 | batch 0049 / 0050 | loss 0.0466\n",
            "Val loss 0.0538\n",
            "Dice score : 0.041537169367074966\n",
            "Val loss 0.0598\n",
            "Dice score : 0.0672510415315628\n",
            "Val loss 0.0503\n",
            "Dice score : 0.038668807595968246\n",
            "Val loss 0.0498\n",
            "Dice score : 0.01893116533756256\n",
            "Val loss 0.0450\n",
            "Dice score : 0.05738796666264534\n",
            "Val loss 0.0461\n",
            "Dice score : 0.03612169623374939\n",
            "Val loss 0.0473\n",
            "Dice score : 0.044859081506729126\n",
            "Val loss 0.0463\n",
            "Dice score : 0.0416584275662899\n",
            "Val loss 0.0479\n",
            "Dice score : 0.0540849044919014\n",
            "Val loss 0.0460\n",
            "Dice score : 0.021037008613348007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 33/100 [44:18<1:30:33, 81.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0034 / 0100 | batch 0000 / 0050 | loss 0.0559\n",
            "Train: epoch 0034 / 0100 | batch 0001 / 0050 | loss 0.0491\n",
            "Train: epoch 0034 / 0100 | batch 0002 / 0050 | loss 0.0432\n",
            "Train: epoch 0034 / 0100 | batch 0003 / 0050 | loss 0.0439\n",
            "Train: epoch 0034 / 0100 | batch 0004 / 0050 | loss 0.0475\n",
            "Train: epoch 0034 / 0100 | batch 0005 / 0050 | loss 0.0503\n",
            "Train: epoch 0034 / 0100 | batch 0006 / 0050 | loss 0.0520\n",
            "Train: epoch 0034 / 0100 | batch 0007 / 0050 | loss 0.0534\n",
            "Train: epoch 0034 / 0100 | batch 0008 / 0050 | loss 0.0518\n",
            "Train: epoch 0034 / 0100 | batch 0009 / 0050 | loss 0.0519\n",
            "Train: epoch 0034 / 0100 | batch 0010 / 0050 | loss 0.0503\n",
            "Train: epoch 0034 / 0100 | batch 0011 / 0050 | loss 0.0518\n",
            "Train: epoch 0034 / 0100 | batch 0012 / 0050 | loss 0.0503\n",
            "Train: epoch 0034 / 0100 | batch 0013 / 0050 | loss 0.0497\n",
            "Train: epoch 0034 / 0100 | batch 0014 / 0050 | loss 0.0489\n",
            "Train: epoch 0034 / 0100 | batch 0015 / 0050 | loss 0.0485\n",
            "Train: epoch 0034 / 0100 | batch 0016 / 0050 | loss 0.0479\n",
            "Train: epoch 0034 / 0100 | batch 0017 / 0050 | loss 0.0478\n",
            "Train: epoch 0034 / 0100 | batch 0018 / 0050 | loss 0.0470\n",
            "Train: epoch 0034 / 0100 | batch 0019 / 0050 | loss 0.0464\n",
            "Train: epoch 0034 / 0100 | batch 0020 / 0050 | loss 0.0460\n",
            "Train: epoch 0034 / 0100 | batch 0021 / 0050 | loss 0.0454\n",
            "Train: epoch 0034 / 0100 | batch 0022 / 0050 | loss 0.0463\n",
            "Train: epoch 0034 / 0100 | batch 0023 / 0050 | loss 0.0458\n",
            "Train: epoch 0034 / 0100 | batch 0024 / 0050 | loss 0.0456\n",
            "Train: epoch 0034 / 0100 | batch 0025 / 0050 | loss 0.0452\n",
            "Train: epoch 0034 / 0100 | batch 0026 / 0050 | loss 0.0446\n",
            "Train: epoch 0034 / 0100 | batch 0027 / 0050 | loss 0.0456\n",
            "Train: epoch 0034 / 0100 | batch 0028 / 0050 | loss 0.0451\n",
            "Train: epoch 0034 / 0100 | batch 0029 / 0050 | loss 0.0452\n",
            "Train: epoch 0034 / 0100 | batch 0030 / 0050 | loss 0.0455\n",
            "Train: epoch 0034 / 0100 | batch 0031 / 0050 | loss 0.0459\n",
            "Train: epoch 0034 / 0100 | batch 0032 / 0050 | loss 0.0464\n",
            "Train: epoch 0034 / 0100 | batch 0033 / 0050 | loss 0.0463\n",
            "Train: epoch 0034 / 0100 | batch 0034 / 0050 | loss 0.0463\n",
            "Train: epoch 0034 / 0100 | batch 0035 / 0050 | loss 0.0459\n",
            "Train: epoch 0034 / 0100 | batch 0036 / 0050 | loss 0.0456\n",
            "Train: epoch 0034 / 0100 | batch 0037 / 0050 | loss 0.0453\n",
            "Train: epoch 0034 / 0100 | batch 0038 / 0050 | loss 0.0454\n",
            "Train: epoch 0034 / 0100 | batch 0039 / 0050 | loss 0.0455\n",
            "Train: epoch 0034 / 0100 | batch 0040 / 0050 | loss 0.0452\n",
            "Train: epoch 0034 / 0100 | batch 0041 / 0050 | loss 0.0450\n",
            "Train: epoch 0034 / 0100 | batch 0042 / 0050 | loss 0.0451\n",
            "Train: epoch 0034 / 0100 | batch 0043 / 0050 | loss 0.0457\n",
            "Train: epoch 0034 / 0100 | batch 0044 / 0050 | loss 0.0457\n",
            "Train: epoch 0034 / 0100 | batch 0045 / 0050 | loss 0.0455\n",
            "Train: epoch 0034 / 0100 | batch 0046 / 0050 | loss 0.0453\n",
            "Train: epoch 0034 / 0100 | batch 0047 / 0050 | loss 0.0450\n",
            "Train: epoch 0034 / 0100 | batch 0048 / 0050 | loss 0.0447\n",
            "Train: epoch 0034 / 0100 | batch 0049 / 0050 | loss 0.0454\n",
            "Val loss 0.0658\n",
            "Dice score : 0.06846659630537033\n",
            "Val loss 0.0504\n",
            "Dice score : 0.04000576213002205\n",
            "Val loss 0.0433\n",
            "Dice score : 0.0481431744992733\n",
            "Val loss 0.0446\n",
            "Dice score : 0.017643729224801064\n",
            "Val loss 0.0441\n",
            "Dice score : 0.034054916352033615\n",
            "Val loss 0.0443\n",
            "Dice score : 0.03373650088906288\n",
            "Val loss 0.0458\n",
            "Dice score : 0.04531153291463852\n",
            "Val loss 0.0443\n",
            "Dice score : 0.02982226014137268\n",
            "Val loss 0.0432\n",
            "Dice score : 0.028104031458497047\n",
            "Val loss 0.0456\n",
            "Dice score : 0.04204043745994568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 34/100 [45:39<1:29:10, 81.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0035 / 0100 | batch 0000 / 0050 | loss 0.0357\n",
            "Train: epoch 0035 / 0100 | batch 0001 / 0050 | loss 0.0336\n",
            "Train: epoch 0035 / 0100 | batch 0002 / 0050 | loss 0.0387\n",
            "Train: epoch 0035 / 0100 | batch 0003 / 0050 | loss 0.0375\n",
            "Train: epoch 0035 / 0100 | batch 0004 / 0050 | loss 0.0450\n",
            "Train: epoch 0035 / 0100 | batch 0005 / 0050 | loss 0.0444\n",
            "Train: epoch 0035 / 0100 | batch 0006 / 0050 | loss 0.0423\n",
            "Train: epoch 0035 / 0100 | batch 0007 / 0050 | loss 0.0428\n",
            "Train: epoch 0035 / 0100 | batch 0008 / 0050 | loss 0.0416\n",
            "Train: epoch 0035 / 0100 | batch 0009 / 0050 | loss 0.0411\n",
            "Train: epoch 0035 / 0100 | batch 0010 / 0050 | loss 0.0416\n",
            "Train: epoch 0035 / 0100 | batch 0011 / 0050 | loss 0.0412\n",
            "Train: epoch 0035 / 0100 | batch 0012 / 0050 | loss 0.0444\n",
            "Train: epoch 0035 / 0100 | batch 0013 / 0050 | loss 0.0463\n",
            "Train: epoch 0035 / 0100 | batch 0014 / 0050 | loss 0.0461\n",
            "Train: epoch 0035 / 0100 | batch 0015 / 0050 | loss 0.0472\n",
            "Train: epoch 0035 / 0100 | batch 0016 / 0050 | loss 0.0476\n",
            "Train: epoch 0035 / 0100 | batch 0017 / 0050 | loss 0.0472\n",
            "Train: epoch 0035 / 0100 | batch 0018 / 0050 | loss 0.0466\n",
            "Train: epoch 0035 / 0100 | batch 0019 / 0050 | loss 0.0460\n",
            "Train: epoch 0035 / 0100 | batch 0020 / 0050 | loss 0.0463\n",
            "Train: epoch 0035 / 0100 | batch 0021 / 0050 | loss 0.0466\n",
            "Train: epoch 0035 / 0100 | batch 0022 / 0050 | loss 0.0465\n",
            "Train: epoch 0035 / 0100 | batch 0023 / 0050 | loss 0.0460\n",
            "Train: epoch 0035 / 0100 | batch 0024 / 0050 | loss 0.0455\n",
            "Train: epoch 0035 / 0100 | batch 0025 / 0050 | loss 0.0460\n",
            "Train: epoch 0035 / 0100 | batch 0026 / 0050 | loss 0.0457\n",
            "Train: epoch 0035 / 0100 | batch 0027 / 0050 | loss 0.0455\n",
            "Train: epoch 0035 / 0100 | batch 0028 / 0050 | loss 0.0450\n",
            "Train: epoch 0035 / 0100 | batch 0029 / 0050 | loss 0.0451\n",
            "Train: epoch 0035 / 0100 | batch 0030 / 0050 | loss 0.0450\n",
            "Train: epoch 0035 / 0100 | batch 0031 / 0050 | loss 0.0446\n",
            "Train: epoch 0035 / 0100 | batch 0032 / 0050 | loss 0.0443\n",
            "Train: epoch 0035 / 0100 | batch 0033 / 0050 | loss 0.0441\n",
            "Train: epoch 0035 / 0100 | batch 0034 / 0050 | loss 0.0443\n",
            "Train: epoch 0035 / 0100 | batch 0035 / 0050 | loss 0.0441\n",
            "Train: epoch 0035 / 0100 | batch 0036 / 0050 | loss 0.0438\n",
            "Train: epoch 0035 / 0100 | batch 0037 / 0050 | loss 0.0434\n",
            "Train: epoch 0035 / 0100 | batch 0038 / 0050 | loss 0.0436\n",
            "Train: epoch 0035 / 0100 | batch 0039 / 0050 | loss 0.0435\n",
            "Train: epoch 0035 / 0100 | batch 0040 / 0050 | loss 0.0438\n",
            "Train: epoch 0035 / 0100 | batch 0041 / 0050 | loss 0.0440\n",
            "Train: epoch 0035 / 0100 | batch 0042 / 0050 | loss 0.0438\n",
            "Train: epoch 0035 / 0100 | batch 0043 / 0050 | loss 0.0443\n",
            "Train: epoch 0035 / 0100 | batch 0044 / 0050 | loss 0.0445\n",
            "Train: epoch 0035 / 0100 | batch 0045 / 0050 | loss 0.0442\n",
            "Train: epoch 0035 / 0100 | batch 0046 / 0050 | loss 0.0442\n",
            "Train: epoch 0035 / 0100 | batch 0047 / 0050 | loss 0.0442\n",
            "Train: epoch 0035 / 0100 | batch 0048 / 0050 | loss 0.0445\n",
            "Train: epoch 0035 / 0100 | batch 0049 / 0050 | loss 0.0448\n",
            "Val loss 0.0898\n",
            "Dice score : 0.07271725684404373\n",
            "Val loss 0.0606\n",
            "Dice score : 0.030603837221860886\n",
            "Val loss 0.0543\n",
            "Dice score : 0.04190083593130112\n",
            "Val loss 0.0506\n",
            "Dice score : 0.03746713697910309\n",
            "Val loss 0.0505\n",
            "Dice score : 0.04206404834985733\n",
            "Val loss 0.0496\n",
            "Dice score : 0.0368756465613842\n",
            "Val loss 0.0471\n",
            "Dice score : 0.04053335636854172\n",
            "Val loss 0.0458\n",
            "Dice score : 0.03712354600429535\n",
            "Val loss 0.0444\n",
            "Dice score : 0.04243859648704529\n",
            "Val loss 0.0425\n",
            "Dice score : 0.034239720553159714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 35/100 [46:59<1:27:25, 80.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0036 / 0100 | batch 0000 / 0050 | loss 0.0502\n",
            "Train: epoch 0036 / 0100 | batch 0001 / 0050 | loss 0.0426\n",
            "Train: epoch 0036 / 0100 | batch 0002 / 0050 | loss 0.0382\n",
            "Train: epoch 0036 / 0100 | batch 0003 / 0050 | loss 0.0405\n",
            "Train: epoch 0036 / 0100 | batch 0004 / 0050 | loss 0.0413\n",
            "Train: epoch 0036 / 0100 | batch 0005 / 0050 | loss 0.0414\n",
            "Train: epoch 0036 / 0100 | batch 0006 / 0050 | loss 0.0409\n",
            "Train: epoch 0036 / 0100 | batch 0007 / 0050 | loss 0.0398\n",
            "Train: epoch 0036 / 0100 | batch 0008 / 0050 | loss 0.0396\n",
            "Train: epoch 0036 / 0100 | batch 0009 / 0050 | loss 0.0406\n",
            "Train: epoch 0036 / 0100 | batch 0010 / 0050 | loss 0.0394\n",
            "Train: epoch 0036 / 0100 | batch 0011 / 0050 | loss 0.0412\n",
            "Train: epoch 0036 / 0100 | batch 0012 / 0050 | loss 0.0409\n",
            "Train: epoch 0036 / 0100 | batch 0013 / 0050 | loss 0.0418\n",
            "Train: epoch 0036 / 0100 | batch 0014 / 0050 | loss 0.0427\n",
            "Train: epoch 0036 / 0100 | batch 0015 / 0050 | loss 0.0421\n",
            "Train: epoch 0036 / 0100 | batch 0016 / 0050 | loss 0.0417\n",
            "Train: epoch 0036 / 0100 | batch 0017 / 0050 | loss 0.0419\n",
            "Train: epoch 0036 / 0100 | batch 0018 / 0050 | loss 0.0418\n",
            "Train: epoch 0036 / 0100 | batch 0019 / 0050 | loss 0.0434\n",
            "Train: epoch 0036 / 0100 | batch 0020 / 0050 | loss 0.0443\n",
            "Train: epoch 0036 / 0100 | batch 0021 / 0050 | loss 0.0445\n",
            "Train: epoch 0036 / 0100 | batch 0022 / 0050 | loss 0.0444\n",
            "Train: epoch 0036 / 0100 | batch 0023 / 0050 | loss 0.0450\n",
            "Train: epoch 0036 / 0100 | batch 0024 / 0050 | loss 0.0452\n",
            "Train: epoch 0036 / 0100 | batch 0025 / 0050 | loss 0.0452\n",
            "Train: epoch 0036 / 0100 | batch 0026 / 0050 | loss 0.0459\n",
            "Train: epoch 0036 / 0100 | batch 0027 / 0050 | loss 0.0455\n",
            "Train: epoch 0036 / 0100 | batch 0028 / 0050 | loss 0.0451\n",
            "Train: epoch 0036 / 0100 | batch 0029 / 0050 | loss 0.0457\n",
            "Train: epoch 0036 / 0100 | batch 0030 / 0050 | loss 0.0454\n",
            "Train: epoch 0036 / 0100 | batch 0031 / 0050 | loss 0.0456\n",
            "Train: epoch 0036 / 0100 | batch 0032 / 0050 | loss 0.0453\n",
            "Train: epoch 0036 / 0100 | batch 0033 / 0050 | loss 0.0452\n",
            "Train: epoch 0036 / 0100 | batch 0034 / 0050 | loss 0.0450\n",
            "Train: epoch 0036 / 0100 | batch 0035 / 0050 | loss 0.0451\n",
            "Train: epoch 0036 / 0100 | batch 0036 / 0050 | loss 0.0448\n",
            "Train: epoch 0036 / 0100 | batch 0037 / 0050 | loss 0.0446\n",
            "Train: epoch 0036 / 0100 | batch 0038 / 0050 | loss 0.0446\n",
            "Train: epoch 0036 / 0100 | batch 0039 / 0050 | loss 0.0443\n",
            "Train: epoch 0036 / 0100 | batch 0040 / 0050 | loss 0.0440\n",
            "Train: epoch 0036 / 0100 | batch 0041 / 0050 | loss 0.0436\n",
            "Train: epoch 0036 / 0100 | batch 0042 / 0050 | loss 0.0440\n",
            "Train: epoch 0036 / 0100 | batch 0043 / 0050 | loss 0.0441\n",
            "Train: epoch 0036 / 0100 | batch 0044 / 0050 | loss 0.0440\n",
            "Train: epoch 0036 / 0100 | batch 0045 / 0050 | loss 0.0440\n",
            "Train: epoch 0036 / 0100 | batch 0046 / 0050 | loss 0.0438\n",
            "Train: epoch 0036 / 0100 | batch 0047 / 0050 | loss 0.0439\n",
            "Train: epoch 0036 / 0100 | batch 0048 / 0050 | loss 0.0438\n",
            "Train: epoch 0036 / 0100 | batch 0049 / 0050 | loss 0.0438\n",
            "Val loss 0.0320\n",
            "Dice score : 0.03267931193113327\n",
            "Val loss 0.0313\n",
            "Dice score : 0.031221279874444008\n",
            "Val loss 0.0318\n",
            "Dice score : 0.03147585690021515\n",
            "Val loss 0.0366\n",
            "Dice score : 0.06888186931610107\n",
            "Val loss 0.0392\n",
            "Dice score : 0.07385782152414322\n",
            "Val loss 0.0389\n",
            "Dice score : 0.03475889563560486\n",
            "Val loss 0.0387\n",
            "Dice score : 0.06293189525604248\n",
            "Val loss 0.0382\n",
            "Dice score : 0.026628568768501282\n",
            "Val loss 0.0389\n",
            "Dice score : 0.042824216187000275\n",
            "Val loss 0.0424\n",
            "Dice score : 0.03927965834736824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 36/100 [48:20<1:26:00, 80.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0037 / 0100 | batch 0000 / 0050 | loss 0.0458\n",
            "Train: epoch 0037 / 0100 | batch 0001 / 0050 | loss 0.0466\n",
            "Train: epoch 0037 / 0100 | batch 0002 / 0050 | loss 0.0476\n",
            "Train: epoch 0037 / 0100 | batch 0003 / 0050 | loss 0.0513\n",
            "Train: epoch 0037 / 0100 | batch 0004 / 0050 | loss 0.0494\n",
            "Train: epoch 0037 / 0100 | batch 0005 / 0050 | loss 0.0459\n",
            "Train: epoch 0037 / 0100 | batch 0006 / 0050 | loss 0.0460\n",
            "Train: epoch 0037 / 0100 | batch 0007 / 0050 | loss 0.0460\n",
            "Train: epoch 0037 / 0100 | batch 0008 / 0050 | loss 0.0460\n",
            "Train: epoch 0037 / 0100 | batch 0009 / 0050 | loss 0.0446\n",
            "Train: epoch 0037 / 0100 | batch 0010 / 0050 | loss 0.0437\n",
            "Train: epoch 0037 / 0100 | batch 0011 / 0050 | loss 0.0440\n",
            "Train: epoch 0037 / 0100 | batch 0012 / 0050 | loss 0.0428\n",
            "Train: epoch 0037 / 0100 | batch 0013 / 0050 | loss 0.0428\n",
            "Train: epoch 0037 / 0100 | batch 0014 / 0050 | loss 0.0421\n",
            "Train: epoch 0037 / 0100 | batch 0015 / 0050 | loss 0.0427\n",
            "Train: epoch 0037 / 0100 | batch 0016 / 0050 | loss 0.0427\n",
            "Train: epoch 0037 / 0100 | batch 0017 / 0050 | loss 0.0425\n",
            "Train: epoch 0037 / 0100 | batch 0018 / 0050 | loss 0.0422\n",
            "Train: epoch 0037 / 0100 | batch 0019 / 0050 | loss 0.0436\n",
            "Train: epoch 0037 / 0100 | batch 0020 / 0050 | loss 0.0440\n",
            "Train: epoch 0037 / 0100 | batch 0021 / 0050 | loss 0.0436\n",
            "Train: epoch 0037 / 0100 | batch 0022 / 0050 | loss 0.0433\n",
            "Train: epoch 0037 / 0100 | batch 0023 / 0050 | loss 0.0429\n",
            "Train: epoch 0037 / 0100 | batch 0024 / 0050 | loss 0.0425\n",
            "Train: epoch 0037 / 0100 | batch 0025 / 0050 | loss 0.0427\n",
            "Train: epoch 0037 / 0100 | batch 0026 / 0050 | loss 0.0443\n",
            "Train: epoch 0037 / 0100 | batch 0027 / 0050 | loss 0.0442\n",
            "Train: epoch 0037 / 0100 | batch 0028 / 0050 | loss 0.0452\n",
            "Train: epoch 0037 / 0100 | batch 0029 / 0050 | loss 0.0447\n",
            "Train: epoch 0037 / 0100 | batch 0030 / 0050 | loss 0.0449\n",
            "Train: epoch 0037 / 0100 | batch 0031 / 0050 | loss 0.0444\n",
            "Train: epoch 0037 / 0100 | batch 0032 / 0050 | loss 0.0443\n",
            "Train: epoch 0037 / 0100 | batch 0033 / 0050 | loss 0.0439\n",
            "Train: epoch 0037 / 0100 | batch 0034 / 0050 | loss 0.0436\n",
            "Train: epoch 0037 / 0100 | batch 0035 / 0050 | loss 0.0438\n",
            "Train: epoch 0037 / 0100 | batch 0036 / 0050 | loss 0.0434\n",
            "Train: epoch 0037 / 0100 | batch 0037 / 0050 | loss 0.0433\n",
            "Train: epoch 0037 / 0100 | batch 0038 / 0050 | loss 0.0432\n",
            "Train: epoch 0037 / 0100 | batch 0039 / 0050 | loss 0.0430\n",
            "Train: epoch 0037 / 0100 | batch 0040 / 0050 | loss 0.0431\n",
            "Train: epoch 0037 / 0100 | batch 0041 / 0050 | loss 0.0431\n",
            "Train: epoch 0037 / 0100 | batch 0042 / 0050 | loss 0.0432\n",
            "Train: epoch 0037 / 0100 | batch 0043 / 0050 | loss 0.0434\n",
            "Train: epoch 0037 / 0100 | batch 0044 / 0050 | loss 0.0437\n",
            "Train: epoch 0037 / 0100 | batch 0045 / 0050 | loss 0.0435\n",
            "Train: epoch 0037 / 0100 | batch 0046 / 0050 | loss 0.0432\n",
            "Train: epoch 0037 / 0100 | batch 0047 / 0050 | loss 0.0435\n",
            "Train: epoch 0037 / 0100 | batch 0048 / 0050 | loss 0.0432\n",
            "Train: epoch 0037 / 0100 | batch 0049 / 0050 | loss 0.0432\n",
            "Val loss 0.0370\n",
            "Dice score : 0.03795711696147919\n",
            "Val loss 0.0397\n",
            "Dice score : 0.04518604651093483\n",
            "Val loss 0.0422\n",
            "Dice score : 0.046148911118507385\n",
            "Val loss 0.0445\n",
            "Dice score : 0.032855186611413956\n",
            "Val loss 0.0438\n",
            "Dice score : 0.041363149881362915\n",
            "Val loss 0.0488\n",
            "Dice score : 0.052062954753637314\n",
            "Val loss 0.0518\n",
            "Dice score : 0.050306808203458786\n",
            "Val loss 0.0521\n",
            "Dice score : 0.053393442183732986\n",
            "Val loss 0.0532\n",
            "Dice score : 0.04011279717087746\n",
            "Val loss 0.0526\n",
            "Dice score : 0.04127887636423111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 37/100 [49:41<1:24:58, 80.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0038 / 0100 | batch 0000 / 0050 | loss 0.0586\n",
            "Train: epoch 0038 / 0100 | batch 0001 / 0050 | loss 0.0500\n",
            "Train: epoch 0038 / 0100 | batch 0002 / 0050 | loss 0.0546\n",
            "Train: epoch 0038 / 0100 | batch 0003 / 0050 | loss 0.0533\n",
            "Train: epoch 0038 / 0100 | batch 0004 / 0050 | loss 0.0508\n",
            "Train: epoch 0038 / 0100 | batch 0005 / 0050 | loss 0.0475\n",
            "Train: epoch 0038 / 0100 | batch 0006 / 0050 | loss 0.0447\n",
            "Train: epoch 0038 / 0100 | batch 0007 / 0050 | loss 0.0442\n",
            "Train: epoch 0038 / 0100 | batch 0008 / 0050 | loss 0.0424\n",
            "Train: epoch 0038 / 0100 | batch 0009 / 0050 | loss 0.0421\n",
            "Train: epoch 0038 / 0100 | batch 0010 / 0050 | loss 0.0433\n",
            "Train: epoch 0038 / 0100 | batch 0011 / 0050 | loss 0.0428\n",
            "Train: epoch 0038 / 0100 | batch 0012 / 0050 | loss 0.0424\n",
            "Train: epoch 0038 / 0100 | batch 0013 / 0050 | loss 0.0425\n",
            "Train: epoch 0038 / 0100 | batch 0014 / 0050 | loss 0.0427\n",
            "Train: epoch 0038 / 0100 | batch 0015 / 0050 | loss 0.0418\n",
            "Train: epoch 0038 / 0100 | batch 0016 / 0050 | loss 0.0413\n",
            "Train: epoch 0038 / 0100 | batch 0017 / 0050 | loss 0.0422\n",
            "Train: epoch 0038 / 0100 | batch 0018 / 0050 | loss 0.0419\n",
            "Train: epoch 0038 / 0100 | batch 0019 / 0050 | loss 0.0423\n",
            "Train: epoch 0038 / 0100 | batch 0020 / 0050 | loss 0.0419\n",
            "Train: epoch 0038 / 0100 | batch 0021 / 0050 | loss 0.0423\n",
            "Train: epoch 0038 / 0100 | batch 0022 / 0050 | loss 0.0426\n",
            "Train: epoch 0038 / 0100 | batch 0023 / 0050 | loss 0.0422\n",
            "Train: epoch 0038 / 0100 | batch 0024 / 0050 | loss 0.0419\n",
            "Train: epoch 0038 / 0100 | batch 0025 / 0050 | loss 0.0417\n",
            "Train: epoch 0038 / 0100 | batch 0026 / 0050 | loss 0.0416\n",
            "Train: epoch 0038 / 0100 | batch 0027 / 0050 | loss 0.0414\n",
            "Train: epoch 0038 / 0100 | batch 0028 / 0050 | loss 0.0413\n",
            "Train: epoch 0038 / 0100 | batch 0029 / 0050 | loss 0.0409\n",
            "Train: epoch 0038 / 0100 | batch 0030 / 0050 | loss 0.0408\n",
            "Train: epoch 0038 / 0100 | batch 0031 / 0050 | loss 0.0408\n",
            "Train: epoch 0038 / 0100 | batch 0032 / 0050 | loss 0.0407\n",
            "Train: epoch 0038 / 0100 | batch 0033 / 0050 | loss 0.0411\n",
            "Train: epoch 0038 / 0100 | batch 0034 / 0050 | loss 0.0418\n",
            "Train: epoch 0038 / 0100 | batch 0035 / 0050 | loss 0.0414\n",
            "Train: epoch 0038 / 0100 | batch 0036 / 0050 | loss 0.0417\n",
            "Train: epoch 0038 / 0100 | batch 0037 / 0050 | loss 0.0419\n",
            "Train: epoch 0038 / 0100 | batch 0038 / 0050 | loss 0.0416\n",
            "Train: epoch 0038 / 0100 | batch 0039 / 0050 | loss 0.0417\n",
            "Train: epoch 0038 / 0100 | batch 0040 / 0050 | loss 0.0415\n",
            "Train: epoch 0038 / 0100 | batch 0041 / 0050 | loss 0.0422\n",
            "Train: epoch 0038 / 0100 | batch 0042 / 0050 | loss 0.0423\n",
            "Train: epoch 0038 / 0100 | batch 0043 / 0050 | loss 0.0421\n",
            "Train: epoch 0038 / 0100 | batch 0044 / 0050 | loss 0.0419\n",
            "Train: epoch 0038 / 0100 | batch 0045 / 0050 | loss 0.0420\n",
            "Train: epoch 0038 / 0100 | batch 0046 / 0050 | loss 0.0420\n",
            "Train: epoch 0038 / 0100 | batch 0047 / 0050 | loss 0.0421\n",
            "Train: epoch 0038 / 0100 | batch 0048 / 0050 | loss 0.0421\n",
            "Train: epoch 0038 / 0100 | batch 0049 / 0050 | loss 0.0424\n",
            "Val loss 0.0371\n",
            "Dice score : 0.030077993869781494\n",
            "Val loss 0.0467\n",
            "Dice score : 0.06464945524930954\n",
            "Val loss 0.0405\n",
            "Dice score : 0.03873547539114952\n",
            "Val loss 0.0403\n",
            "Dice score : 0.03985126316547394\n",
            "Val loss 0.0397\n",
            "Dice score : 0.03865521401166916\n",
            "Val loss 0.0376\n",
            "Dice score : 0.06184353679418564\n",
            "Val loss 0.0405\n",
            "Dice score : 0.05926366522908211\n",
            "Val loss 0.0392\n",
            "Dice score : 0.03865288570523262\n",
            "Val loss 0.0402\n",
            "Dice score : 0.04572875797748566\n",
            "Val loss 0.0404\n",
            "Dice score : 0.03083486109972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 38/100 [51:00<1:23:00, 80.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0039 / 0100 | batch 0000 / 0050 | loss 0.0348\n",
            "Train: epoch 0039 / 0100 | batch 0001 / 0050 | loss 0.0379\n",
            "Train: epoch 0039 / 0100 | batch 0002 / 0050 | loss 0.0357\n",
            "Train: epoch 0039 / 0100 | batch 0003 / 0050 | loss 0.0350\n",
            "Train: epoch 0039 / 0100 | batch 0004 / 0050 | loss 0.0338\n",
            "Train: epoch 0039 / 0100 | batch 0005 / 0050 | loss 0.0358\n",
            "Train: epoch 0039 / 0100 | batch 0006 / 0050 | loss 0.0347\n",
            "Train: epoch 0039 / 0100 | batch 0007 / 0050 | loss 0.0343\n",
            "Train: epoch 0039 / 0100 | batch 0008 / 0050 | loss 0.0339\n",
            "Train: epoch 0039 / 0100 | batch 0009 / 0050 | loss 0.0357\n",
            "Train: epoch 0039 / 0100 | batch 0010 / 0050 | loss 0.0357\n",
            "Train: epoch 0039 / 0100 | batch 0011 / 0050 | loss 0.0363\n",
            "Train: epoch 0039 / 0100 | batch 0012 / 0050 | loss 0.0374\n",
            "Train: epoch 0039 / 0100 | batch 0013 / 0050 | loss 0.0380\n",
            "Train: epoch 0039 / 0100 | batch 0014 / 0050 | loss 0.0372\n",
            "Train: epoch 0039 / 0100 | batch 0015 / 0050 | loss 0.0391\n",
            "Train: epoch 0039 / 0100 | batch 0016 / 0050 | loss 0.0399\n",
            "Train: epoch 0039 / 0100 | batch 0017 / 0050 | loss 0.0403\n",
            "Train: epoch 0039 / 0100 | batch 0018 / 0050 | loss 0.0397\n",
            "Train: epoch 0039 / 0100 | batch 0019 / 0050 | loss 0.0409\n",
            "Train: epoch 0039 / 0100 | batch 0020 / 0050 | loss 0.0408\n",
            "Train: epoch 0039 / 0100 | batch 0021 / 0050 | loss 0.0404\n",
            "Train: epoch 0039 / 0100 | batch 0022 / 0050 | loss 0.0409\n",
            "Train: epoch 0039 / 0100 | batch 0023 / 0050 | loss 0.0412\n",
            "Train: epoch 0039 / 0100 | batch 0024 / 0050 | loss 0.0412\n",
            "Train: epoch 0039 / 0100 | batch 0025 / 0050 | loss 0.0415\n",
            "Train: epoch 0039 / 0100 | batch 0026 / 0050 | loss 0.0416\n",
            "Train: epoch 0039 / 0100 | batch 0027 / 0050 | loss 0.0414\n",
            "Train: epoch 0039 / 0100 | batch 0028 / 0050 | loss 0.0423\n",
            "Train: epoch 0039 / 0100 | batch 0029 / 0050 | loss 0.0419\n",
            "Train: epoch 0039 / 0100 | batch 0030 / 0050 | loss 0.0414\n",
            "Train: epoch 0039 / 0100 | batch 0031 / 0050 | loss 0.0411\n",
            "Train: epoch 0039 / 0100 | batch 0032 / 0050 | loss 0.0420\n",
            "Train: epoch 0039 / 0100 | batch 0033 / 0050 | loss 0.0417\n",
            "Train: epoch 0039 / 0100 | batch 0034 / 0050 | loss 0.0417\n",
            "Train: epoch 0039 / 0100 | batch 0035 / 0050 | loss 0.0414\n",
            "Train: epoch 0039 / 0100 | batch 0036 / 0050 | loss 0.0425\n",
            "Train: epoch 0039 / 0100 | batch 0037 / 0050 | loss 0.0424\n",
            "Train: epoch 0039 / 0100 | batch 0038 / 0050 | loss 0.0423\n",
            "Train: epoch 0039 / 0100 | batch 0039 / 0050 | loss 0.0419\n",
            "Train: epoch 0039 / 0100 | batch 0040 / 0050 | loss 0.0416\n",
            "Train: epoch 0039 / 0100 | batch 0041 / 0050 | loss 0.0415\n",
            "Train: epoch 0039 / 0100 | batch 0042 / 0050 | loss 0.0413\n",
            "Train: epoch 0039 / 0100 | batch 0043 / 0050 | loss 0.0412\n",
            "Train: epoch 0039 / 0100 | batch 0044 / 0050 | loss 0.0412\n",
            "Train: epoch 0039 / 0100 | batch 0045 / 0050 | loss 0.0415\n",
            "Train: epoch 0039 / 0100 | batch 0046 / 0050 | loss 0.0415\n",
            "Train: epoch 0039 / 0100 | batch 0047 / 0050 | loss 0.0421\n",
            "Train: epoch 0039 / 0100 | batch 0048 / 0050 | loss 0.0419\n",
            "Train: epoch 0039 / 0100 | batch 0049 / 0050 | loss 0.0422\n",
            "Val loss 0.0367\n",
            "Dice score : 0.029780162498354912\n",
            "Val loss 0.0390\n",
            "Dice score : 0.02507321909070015\n",
            "Val loss 0.0505\n",
            "Dice score : 0.04290478676557541\n",
            "Val loss 0.0463\n",
            "Dice score : 0.025461189448833466\n",
            "Val loss 0.0438\n",
            "Dice score : 0.026686696335673332\n",
            "Val loss 0.0470\n",
            "Dice score : 0.0401826910674572\n",
            "Val loss 0.0475\n",
            "Dice score : 0.037962835282087326\n",
            "Val loss 0.0496\n",
            "Dice score : 0.046581562608480453\n",
            "Val loss 0.0486\n",
            "Dice score : 0.03001018986105919\n",
            "Val loss 0.0485\n",
            "Dice score : 0.033292610198259354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 39/100 [52:20<1:21:33, 80.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0040 / 0100 | batch 0000 / 0050 | loss 0.0564\n",
            "Train: epoch 0040 / 0100 | batch 0001 / 0050 | loss 0.0430\n",
            "Train: epoch 0040 / 0100 | batch 0002 / 0050 | loss 0.0510\n",
            "Train: epoch 0040 / 0100 | batch 0003 / 0050 | loss 0.0456\n",
            "Train: epoch 0040 / 0100 | batch 0004 / 0050 | loss 0.0456\n",
            "Train: epoch 0040 / 0100 | batch 0005 / 0050 | loss 0.0443\n",
            "Train: epoch 0040 / 0100 | batch 0006 / 0050 | loss 0.0420\n",
            "Train: epoch 0040 / 0100 | batch 0007 / 0050 | loss 0.0442\n",
            "Train: epoch 0040 / 0100 | batch 0008 / 0050 | loss 0.0446\n",
            "Train: epoch 0040 / 0100 | batch 0009 / 0050 | loss 0.0454\n",
            "Train: epoch 0040 / 0100 | batch 0010 / 0050 | loss 0.0451\n",
            "Train: epoch 0040 / 0100 | batch 0011 / 0050 | loss 0.0443\n",
            "Train: epoch 0040 / 0100 | batch 0012 / 0050 | loss 0.0439\n",
            "Train: epoch 0040 / 0100 | batch 0013 / 0050 | loss 0.0440\n",
            "Train: epoch 0040 / 0100 | batch 0014 / 0050 | loss 0.0444\n",
            "Train: epoch 0040 / 0100 | batch 0015 / 0050 | loss 0.0442\n",
            "Train: epoch 0040 / 0100 | batch 0016 / 0050 | loss 0.0436\n",
            "Train: epoch 0040 / 0100 | batch 0017 / 0050 | loss 0.0428\n",
            "Train: epoch 0040 / 0100 | batch 0018 / 0050 | loss 0.0431\n",
            "Train: epoch 0040 / 0100 | batch 0019 / 0050 | loss 0.0429\n",
            "Train: epoch 0040 / 0100 | batch 0020 / 0050 | loss 0.0433\n",
            "Train: epoch 0040 / 0100 | batch 0021 / 0050 | loss 0.0428\n",
            "Train: epoch 0040 / 0100 | batch 0022 / 0050 | loss 0.0428\n",
            "Train: epoch 0040 / 0100 | batch 0023 / 0050 | loss 0.0434\n",
            "Train: epoch 0040 / 0100 | batch 0024 / 0050 | loss 0.0440\n",
            "Train: epoch 0040 / 0100 | batch 0025 / 0050 | loss 0.0436\n",
            "Train: epoch 0040 / 0100 | batch 0026 / 0050 | loss 0.0431\n",
            "Train: epoch 0040 / 0100 | batch 0027 / 0050 | loss 0.0427\n",
            "Train: epoch 0040 / 0100 | batch 0028 / 0050 | loss 0.0426\n",
            "Train: epoch 0040 / 0100 | batch 0029 / 0050 | loss 0.0422\n",
            "Train: epoch 0040 / 0100 | batch 0030 / 0050 | loss 0.0420\n",
            "Train: epoch 0040 / 0100 | batch 0031 / 0050 | loss 0.0425\n",
            "Train: epoch 0040 / 0100 | batch 0032 / 0050 | loss 0.0423\n",
            "Train: epoch 0040 / 0100 | batch 0033 / 0050 | loss 0.0421\n",
            "Train: epoch 0040 / 0100 | batch 0034 / 0050 | loss 0.0423\n",
            "Train: epoch 0040 / 0100 | batch 0035 / 0050 | loss 0.0430\n",
            "Train: epoch 0040 / 0100 | batch 0036 / 0050 | loss 0.0430\n",
            "Train: epoch 0040 / 0100 | batch 0037 / 0050 | loss 0.0428\n",
            "Train: epoch 0040 / 0100 | batch 0038 / 0050 | loss 0.0427\n",
            "Train: epoch 0040 / 0100 | batch 0039 / 0050 | loss 0.0429\n",
            "Train: epoch 0040 / 0100 | batch 0040 / 0050 | loss 0.0426\n",
            "Train: epoch 0040 / 0100 | batch 0041 / 0050 | loss 0.0423\n",
            "Train: epoch 0040 / 0100 | batch 0042 / 0050 | loss 0.0422\n",
            "Train: epoch 0040 / 0100 | batch 0043 / 0050 | loss 0.0424\n",
            "Train: epoch 0040 / 0100 | batch 0044 / 0050 | loss 0.0429\n",
            "Train: epoch 0040 / 0100 | batch 0045 / 0050 | loss 0.0432\n",
            "Train: epoch 0040 / 0100 | batch 0046 / 0050 | loss 0.0429\n",
            "Train: epoch 0040 / 0100 | batch 0047 / 0050 | loss 0.0427\n",
            "Train: epoch 0040 / 0100 | batch 0048 / 0050 | loss 0.0424\n",
            "Train: epoch 0040 / 0100 | batch 0049 / 0050 | loss 0.0421\n",
            "Val loss 0.0647\n",
            "Dice score : 0.06601119041442871\n",
            "Val loss 0.0470\n",
            "Dice score : 0.04410901665687561\n",
            "Val loss 0.0415\n",
            "Dice score : 0.04646209254860878\n",
            "Val loss 0.0427\n",
            "Dice score : 0.03746028617024422\n",
            "Val loss 0.0445\n",
            "Dice score : 0.03629584610462189\n",
            "Val loss 0.0450\n",
            "Dice score : 0.05849494785070419\n",
            "Val loss 0.0425\n",
            "Dice score : 0.03917790949344635\n",
            "Val loss 0.0406\n",
            "Dice score : 0.04847760498523712\n",
            "Val loss 0.0395\n",
            "Dice score : 0.055354923009872437\n",
            "Val loss 0.0388\n",
            "Dice score : 0.060623303055763245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 40/100 [53:39<1:19:56, 79.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0041 / 0100 | batch 0000 / 0050 | loss 0.0544\n",
            "Train: epoch 0041 / 0100 | batch 0001 / 0050 | loss 0.0520\n",
            "Train: epoch 0041 / 0100 | batch 0002 / 0050 | loss 0.0460\n",
            "Train: epoch 0041 / 0100 | batch 0003 / 0050 | loss 0.0429\n",
            "Train: epoch 0041 / 0100 | batch 0004 / 0050 | loss 0.0403\n",
            "Train: epoch 0041 / 0100 | batch 0005 / 0050 | loss 0.0396\n",
            "Train: epoch 0041 / 0100 | batch 0006 / 0050 | loss 0.0398\n",
            "Train: epoch 0041 / 0100 | batch 0007 / 0050 | loss 0.0404\n",
            "Train: epoch 0041 / 0100 | batch 0008 / 0050 | loss 0.0394\n",
            "Train: epoch 0041 / 0100 | batch 0009 / 0050 | loss 0.0382\n",
            "Train: epoch 0041 / 0100 | batch 0010 / 0050 | loss 0.0381\n",
            "Train: epoch 0041 / 0100 | batch 0011 / 0050 | loss 0.0395\n",
            "Train: epoch 0041 / 0100 | batch 0012 / 0050 | loss 0.0399\n",
            "Train: epoch 0041 / 0100 | batch 0013 / 0050 | loss 0.0404\n",
            "Train: epoch 0041 / 0100 | batch 0014 / 0050 | loss 0.0410\n",
            "Train: epoch 0041 / 0100 | batch 0015 / 0050 | loss 0.0410\n",
            "Train: epoch 0041 / 0100 | batch 0016 / 0050 | loss 0.0414\n",
            "Train: epoch 0041 / 0100 | batch 0017 / 0050 | loss 0.0434\n",
            "Train: epoch 0041 / 0100 | batch 0018 / 0050 | loss 0.0429\n",
            "Train: epoch 0041 / 0100 | batch 0019 / 0050 | loss 0.0429\n",
            "Train: epoch 0041 / 0100 | batch 0020 / 0050 | loss 0.0425\n",
            "Train: epoch 0041 / 0100 | batch 0021 / 0050 | loss 0.0424\n",
            "Train: epoch 0041 / 0100 | batch 0022 / 0050 | loss 0.0425\n",
            "Train: epoch 0041 / 0100 | batch 0023 / 0050 | loss 0.0424\n",
            "Train: epoch 0041 / 0100 | batch 0024 / 0050 | loss 0.0422\n",
            "Train: epoch 0041 / 0100 | batch 0025 / 0050 | loss 0.0418\n",
            "Train: epoch 0041 / 0100 | batch 0026 / 0050 | loss 0.0416\n",
            "Train: epoch 0041 / 0100 | batch 0027 / 0050 | loss 0.0414\n",
            "Train: epoch 0041 / 0100 | batch 0028 / 0050 | loss 0.0410\n",
            "Train: epoch 0041 / 0100 | batch 0029 / 0050 | loss 0.0406\n",
            "Train: epoch 0041 / 0100 | batch 0030 / 0050 | loss 0.0415\n",
            "Train: epoch 0041 / 0100 | batch 0031 / 0050 | loss 0.0411\n",
            "Train: epoch 0041 / 0100 | batch 0032 / 0050 | loss 0.0413\n",
            "Train: epoch 0041 / 0100 | batch 0033 / 0050 | loss 0.0415\n",
            "Train: epoch 0041 / 0100 | batch 0034 / 0050 | loss 0.0414\n",
            "Train: epoch 0041 / 0100 | batch 0035 / 0050 | loss 0.0417\n",
            "Train: epoch 0041 / 0100 | batch 0036 / 0050 | loss 0.0423\n",
            "Train: epoch 0041 / 0100 | batch 0037 / 0050 | loss 0.0423\n",
            "Train: epoch 0041 / 0100 | batch 0038 / 0050 | loss 0.0425\n",
            "Train: epoch 0041 / 0100 | batch 0039 / 0050 | loss 0.0429\n",
            "Train: epoch 0041 / 0100 | batch 0040 / 0050 | loss 0.0431\n",
            "Train: epoch 0041 / 0100 | batch 0041 / 0050 | loss 0.0428\n",
            "Train: epoch 0041 / 0100 | batch 0042 / 0050 | loss 0.0428\n",
            "Train: epoch 0041 / 0100 | batch 0043 / 0050 | loss 0.0427\n",
            "Train: epoch 0041 / 0100 | batch 0044 / 0050 | loss 0.0429\n",
            "Train: epoch 0041 / 0100 | batch 0045 / 0050 | loss 0.0426\n",
            "Train: epoch 0041 / 0100 | batch 0046 / 0050 | loss 0.0424\n",
            "Train: epoch 0041 / 0100 | batch 0047 / 0050 | loss 0.0421\n",
            "Train: epoch 0041 / 0100 | batch 0048 / 0050 | loss 0.0419\n",
            "Train: epoch 0041 / 0100 | batch 0049 / 0050 | loss 0.0419\n",
            "Val loss 0.0359\n",
            "Dice score : 0.04309387877583504\n",
            "Val loss 0.0364\n",
            "Dice score : 0.042521800845861435\n",
            "Val loss 0.0333\n",
            "Dice score : 0.04902689903974533\n",
            "Val loss 0.0400\n",
            "Dice score : 0.048075489699840546\n",
            "Val loss 0.0392\n",
            "Dice score : 0.056176960468292236\n",
            "Val loss 0.0403\n",
            "Dice score : 0.05449170619249344\n",
            "Val loss 0.0424\n",
            "Dice score : 0.06390664726495743\n",
            "Val loss 0.0417\n",
            "Dice score : 0.03644261509180069\n",
            "Val loss 0.0417\n",
            "Dice score : 0.03925349935889244\n",
            "Val loss 0.0417\n",
            "Dice score : 0.04036236181855202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 41/100 [54:58<1:18:14, 79.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0042 / 0100 | batch 0000 / 0050 | loss 0.0412\n",
            "Train: epoch 0042 / 0100 | batch 0001 / 0050 | loss 0.0437\n",
            "Train: epoch 0042 / 0100 | batch 0002 / 0050 | loss 0.0397\n",
            "Train: epoch 0042 / 0100 | batch 0003 / 0050 | loss 0.0376\n",
            "Train: epoch 0042 / 0100 | batch 0004 / 0050 | loss 0.0363\n",
            "Train: epoch 0042 / 0100 | batch 0005 / 0050 | loss 0.0388\n",
            "Train: epoch 0042 / 0100 | batch 0006 / 0050 | loss 0.0370\n",
            "Train: epoch 0042 / 0100 | batch 0007 / 0050 | loss 0.0359\n",
            "Train: epoch 0042 / 0100 | batch 0008 / 0050 | loss 0.0355\n",
            "Train: epoch 0042 / 0100 | batch 0009 / 0050 | loss 0.0364\n",
            "Train: epoch 0042 / 0100 | batch 0010 / 0050 | loss 0.0385\n",
            "Train: epoch 0042 / 0100 | batch 0011 / 0050 | loss 0.0384\n",
            "Train: epoch 0042 / 0100 | batch 0012 / 0050 | loss 0.0380\n",
            "Train: epoch 0042 / 0100 | batch 0013 / 0050 | loss 0.0384\n",
            "Train: epoch 0042 / 0100 | batch 0014 / 0050 | loss 0.0389\n",
            "Train: epoch 0042 / 0100 | batch 0015 / 0050 | loss 0.0398\n",
            "Train: epoch 0042 / 0100 | batch 0016 / 0050 | loss 0.0392\n",
            "Train: epoch 0042 / 0100 | batch 0017 / 0050 | loss 0.0404\n",
            "Train: epoch 0042 / 0100 | batch 0018 / 0050 | loss 0.0400\n",
            "Train: epoch 0042 / 0100 | batch 0019 / 0050 | loss 0.0398\n",
            "Train: epoch 0042 / 0100 | batch 0020 / 0050 | loss 0.0393\n",
            "Train: epoch 0042 / 0100 | batch 0021 / 0050 | loss 0.0403\n",
            "Train: epoch 0042 / 0100 | batch 0022 / 0050 | loss 0.0401\n",
            "Train: epoch 0042 / 0100 | batch 0023 / 0050 | loss 0.0404\n",
            "Train: epoch 0042 / 0100 | batch 0024 / 0050 | loss 0.0410\n",
            "Train: epoch 0042 / 0100 | batch 0025 / 0050 | loss 0.0409\n",
            "Train: epoch 0042 / 0100 | batch 0026 / 0050 | loss 0.0410\n",
            "Train: epoch 0042 / 0100 | batch 0027 / 0050 | loss 0.0411\n",
            "Train: epoch 0042 / 0100 | batch 0028 / 0050 | loss 0.0427\n",
            "Train: epoch 0042 / 0100 | batch 0029 / 0050 | loss 0.0427\n",
            "Train: epoch 0042 / 0100 | batch 0030 / 0050 | loss 0.0429\n",
            "Train: epoch 0042 / 0100 | batch 0031 / 0050 | loss 0.0424\n",
            "Train: epoch 0042 / 0100 | batch 0032 / 0050 | loss 0.0422\n",
            "Train: epoch 0042 / 0100 | batch 0033 / 0050 | loss 0.0418\n",
            "Train: epoch 0042 / 0100 | batch 0034 / 0050 | loss 0.0418\n",
            "Train: epoch 0042 / 0100 | batch 0035 / 0050 | loss 0.0415\n",
            "Train: epoch 0042 / 0100 | batch 0036 / 0050 | loss 0.0414\n",
            "Train: epoch 0042 / 0100 | batch 0037 / 0050 | loss 0.0413\n",
            "Train: epoch 0042 / 0100 | batch 0038 / 0050 | loss 0.0416\n",
            "Train: epoch 0042 / 0100 | batch 0039 / 0050 | loss 0.0415\n",
            "Train: epoch 0042 / 0100 | batch 0040 / 0050 | loss 0.0412\n",
            "Train: epoch 0042 / 0100 | batch 0041 / 0050 | loss 0.0414\n",
            "Train: epoch 0042 / 0100 | batch 0042 / 0050 | loss 0.0412\n",
            "Train: epoch 0042 / 0100 | batch 0043 / 0050 | loss 0.0413\n",
            "Train: epoch 0042 / 0100 | batch 0044 / 0050 | loss 0.0411\n",
            "Train: epoch 0042 / 0100 | batch 0045 / 0050 | loss 0.0413\n",
            "Train: epoch 0042 / 0100 | batch 0046 / 0050 | loss 0.0413\n",
            "Train: epoch 0042 / 0100 | batch 0047 / 0050 | loss 0.0416\n",
            "Train: epoch 0042 / 0100 | batch 0048 / 0050 | loss 0.0412\n",
            "Train: epoch 0042 / 0100 | batch 0049 / 0050 | loss 0.0415\n",
            "Val loss 0.0560\n",
            "Dice score : 0.04779411107301712\n",
            "Val loss 0.0413\n",
            "Dice score : 0.03206969052553177\n",
            "Val loss 0.0447\n",
            "Dice score : 0.057054150849580765\n",
            "Val loss 0.0444\n",
            "Dice score : 0.0503881499171257\n",
            "Val loss 0.0428\n",
            "Dice score : 0.0395490936934948\n",
            "Val loss 0.0412\n",
            "Dice score : 0.021994952112436295\n",
            "Val loss 0.0399\n",
            "Dice score : 0.03576146066188812\n",
            "Val loss 0.0407\n",
            "Dice score : 0.033119603991508484\n",
            "Val loss 0.0394\n",
            "Dice score : 0.04129909351468086\n",
            "Val loss 0.0397\n",
            "Dice score : 0.05748317763209343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 42/100 [56:16<1:16:27, 79.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0043 / 0100 | batch 0000 / 0050 | loss 0.0451\n",
            "Train: epoch 0043 / 0100 | batch 0001 / 0050 | loss 0.0372\n",
            "Train: epoch 0043 / 0100 | batch 0002 / 0050 | loss 0.0354\n",
            "Train: epoch 0043 / 0100 | batch 0003 / 0050 | loss 0.0385\n",
            "Train: epoch 0043 / 0100 | batch 0004 / 0050 | loss 0.0371\n",
            "Train: epoch 0043 / 0100 | batch 0005 / 0050 | loss 0.0387\n",
            "Train: epoch 0043 / 0100 | batch 0006 / 0050 | loss 0.0409\n",
            "Train: epoch 0043 / 0100 | batch 0007 / 0050 | loss 0.0391\n",
            "Train: epoch 0043 / 0100 | batch 0008 / 0050 | loss 0.0381\n",
            "Train: epoch 0043 / 0100 | batch 0009 / 0050 | loss 0.0377\n",
            "Train: epoch 0043 / 0100 | batch 0010 / 0050 | loss 0.0370\n",
            "Train: epoch 0043 / 0100 | batch 0011 / 0050 | loss 0.0371\n",
            "Train: epoch 0043 / 0100 | batch 0012 / 0050 | loss 0.0371\n",
            "Train: epoch 0043 / 0100 | batch 0013 / 0050 | loss 0.0370\n",
            "Train: epoch 0043 / 0100 | batch 0014 / 0050 | loss 0.0372\n",
            "Train: epoch 0043 / 0100 | batch 0015 / 0050 | loss 0.0372\n",
            "Train: epoch 0043 / 0100 | batch 0016 / 0050 | loss 0.0377\n",
            "Train: epoch 0043 / 0100 | batch 0017 / 0050 | loss 0.0375\n",
            "Train: epoch 0043 / 0100 | batch 0018 / 0050 | loss 0.0374\n",
            "Train: epoch 0043 / 0100 | batch 0019 / 0050 | loss 0.0371\n",
            "Train: epoch 0043 / 0100 | batch 0020 / 0050 | loss 0.0383\n",
            "Train: epoch 0043 / 0100 | batch 0021 / 0050 | loss 0.0386\n",
            "Train: epoch 0043 / 0100 | batch 0022 / 0050 | loss 0.0387\n",
            "Train: epoch 0043 / 0100 | batch 0023 / 0050 | loss 0.0395\n",
            "Train: epoch 0043 / 0100 | batch 0024 / 0050 | loss 0.0405\n",
            "Train: epoch 0043 / 0100 | batch 0025 / 0050 | loss 0.0408\n",
            "Train: epoch 0043 / 0100 | batch 0026 / 0050 | loss 0.0404\n",
            "Train: epoch 0043 / 0100 | batch 0027 / 0050 | loss 0.0401\n",
            "Train: epoch 0043 / 0100 | batch 0028 / 0050 | loss 0.0402\n",
            "Train: epoch 0043 / 0100 | batch 0029 / 0050 | loss 0.0400\n",
            "Train: epoch 0043 / 0100 | batch 0030 / 0050 | loss 0.0400\n",
            "Train: epoch 0043 / 0100 | batch 0031 / 0050 | loss 0.0404\n",
            "Train: epoch 0043 / 0100 | batch 0032 / 0050 | loss 0.0408\n",
            "Train: epoch 0043 / 0100 | batch 0033 / 0050 | loss 0.0408\n",
            "Train: epoch 0043 / 0100 | batch 0034 / 0050 | loss 0.0405\n",
            "Train: epoch 0043 / 0100 | batch 0035 / 0050 | loss 0.0401\n",
            "Train: epoch 0043 / 0100 | batch 0036 / 0050 | loss 0.0400\n",
            "Train: epoch 0043 / 0100 | batch 0037 / 0050 | loss 0.0405\n",
            "Train: epoch 0043 / 0100 | batch 0038 / 0050 | loss 0.0408\n",
            "Train: epoch 0043 / 0100 | batch 0039 / 0050 | loss 0.0414\n",
            "Train: epoch 0043 / 0100 | batch 0040 / 0050 | loss 0.0415\n",
            "Train: epoch 0043 / 0100 | batch 0041 / 0050 | loss 0.0416\n",
            "Train: epoch 0043 / 0100 | batch 0042 / 0050 | loss 0.0413\n",
            "Train: epoch 0043 / 0100 | batch 0043 / 0050 | loss 0.0412\n",
            "Train: epoch 0043 / 0100 | batch 0044 / 0050 | loss 0.0413\n",
            "Train: epoch 0043 / 0100 | batch 0045 / 0050 | loss 0.0413\n",
            "Train: epoch 0043 / 0100 | batch 0046 / 0050 | loss 0.0411\n",
            "Train: epoch 0043 / 0100 | batch 0047 / 0050 | loss 0.0410\n",
            "Train: epoch 0043 / 0100 | batch 0048 / 0050 | loss 0.0412\n",
            "Train: epoch 0043 / 0100 | batch 0049 / 0050 | loss 0.0412\n",
            "Val loss 0.0425\n",
            "Dice score : 0.030581099912524223\n",
            "Val loss 0.0462\n",
            "Dice score : 0.028165247291326523\n",
            "Val loss 0.0572\n",
            "Dice score : 0.06312243640422821\n",
            "Val loss 0.0530\n",
            "Dice score : 0.03411688283085823\n",
            "Val loss 0.0506\n",
            "Dice score : 0.040204644203186035\n",
            "Val loss 0.0550\n",
            "Dice score : 0.043960437178611755\n",
            "Val loss 0.0527\n",
            "Dice score : 0.03724728524684906\n",
            "Val loss 0.0519\n",
            "Dice score : 0.0238882414996624\n",
            "Val loss 0.0517\n",
            "Dice score : 0.04180723801255226\n",
            "Val loss 0.0513\n",
            "Dice score : 0.0337073914706707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 43/100 [57:36<1:15:28, 79.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0044 / 0100 | batch 0000 / 0050 | loss 0.0279\n",
            "Train: epoch 0044 / 0100 | batch 0001 / 0050 | loss 0.0325\n",
            "Train: epoch 0044 / 0100 | batch 0002 / 0050 | loss 0.0334\n",
            "Train: epoch 0044 / 0100 | batch 0003 / 0050 | loss 0.0395\n",
            "Train: epoch 0044 / 0100 | batch 0004 / 0050 | loss 0.0429\n",
            "Train: epoch 0044 / 0100 | batch 0005 / 0050 | loss 0.0467\n",
            "Train: epoch 0044 / 0100 | batch 0006 / 0050 | loss 0.0463\n",
            "Train: epoch 0044 / 0100 | batch 0007 / 0050 | loss 0.0443\n",
            "Train: epoch 0044 / 0100 | batch 0008 / 0050 | loss 0.0423\n",
            "Train: epoch 0044 / 0100 | batch 0009 / 0050 | loss 0.0424\n",
            "Train: epoch 0044 / 0100 | batch 0010 / 0050 | loss 0.0414\n",
            "Train: epoch 0044 / 0100 | batch 0011 / 0050 | loss 0.0411\n",
            "Train: epoch 0044 / 0100 | batch 0012 / 0050 | loss 0.0408\n",
            "Train: epoch 0044 / 0100 | batch 0013 / 0050 | loss 0.0401\n",
            "Train: epoch 0044 / 0100 | batch 0014 / 0050 | loss 0.0403\n",
            "Train: epoch 0044 / 0100 | batch 0015 / 0050 | loss 0.0397\n",
            "Train: epoch 0044 / 0100 | batch 0016 / 0050 | loss 0.0408\n",
            "Train: epoch 0044 / 0100 | batch 0017 / 0050 | loss 0.0409\n",
            "Train: epoch 0044 / 0100 | batch 0018 / 0050 | loss 0.0402\n",
            "Train: epoch 0044 / 0100 | batch 0019 / 0050 | loss 0.0396\n",
            "Train: epoch 0044 / 0100 | batch 0020 / 0050 | loss 0.0393\n",
            "Train: epoch 0044 / 0100 | batch 0021 / 0050 | loss 0.0393\n",
            "Train: epoch 0044 / 0100 | batch 0022 / 0050 | loss 0.0395\n",
            "Train: epoch 0044 / 0100 | batch 0023 / 0050 | loss 0.0399\n",
            "Train: epoch 0044 / 0100 | batch 0024 / 0050 | loss 0.0397\n",
            "Train: epoch 0044 / 0100 | batch 0025 / 0050 | loss 0.0392\n",
            "Train: epoch 0044 / 0100 | batch 0026 / 0050 | loss 0.0393\n",
            "Train: epoch 0044 / 0100 | batch 0027 / 0050 | loss 0.0395\n",
            "Train: epoch 0044 / 0100 | batch 0028 / 0050 | loss 0.0398\n",
            "Train: epoch 0044 / 0100 | batch 0029 / 0050 | loss 0.0402\n",
            "Train: epoch 0044 / 0100 | batch 0030 / 0050 | loss 0.0398\n",
            "Train: epoch 0044 / 0100 | batch 0031 / 0050 | loss 0.0396\n",
            "Train: epoch 0044 / 0100 | batch 0032 / 0050 | loss 0.0392\n",
            "Train: epoch 0044 / 0100 | batch 0033 / 0050 | loss 0.0389\n",
            "Train: epoch 0044 / 0100 | batch 0034 / 0050 | loss 0.0389\n",
            "Train: epoch 0044 / 0100 | batch 0035 / 0050 | loss 0.0392\n",
            "Train: epoch 0044 / 0100 | batch 0036 / 0050 | loss 0.0389\n",
            "Train: epoch 0044 / 0100 | batch 0037 / 0050 | loss 0.0389\n",
            "Train: epoch 0044 / 0100 | batch 0038 / 0050 | loss 0.0391\n",
            "Train: epoch 0044 / 0100 | batch 0039 / 0050 | loss 0.0392\n",
            "Train: epoch 0044 / 0100 | batch 0040 / 0050 | loss 0.0394\n",
            "Train: epoch 0044 / 0100 | batch 0041 / 0050 | loss 0.0396\n",
            "Train: epoch 0044 / 0100 | batch 0042 / 0050 | loss 0.0394\n",
            "Train: epoch 0044 / 0100 | batch 0043 / 0050 | loss 0.0396\n",
            "Train: epoch 0044 / 0100 | batch 0044 / 0050 | loss 0.0404\n",
            "Train: epoch 0044 / 0100 | batch 0045 / 0050 | loss 0.0409\n",
            "Train: epoch 0044 / 0100 | batch 0046 / 0050 | loss 0.0407\n",
            "Train: epoch 0044 / 0100 | batch 0047 / 0050 | loss 0.0409\n",
            "Train: epoch 0044 / 0100 | batch 0048 / 0050 | loss 0.0412\n",
            "Train: epoch 0044 / 0100 | batch 0049 / 0050 | loss 0.0411\n",
            "Val loss 0.0452\n",
            "Dice score : 0.047593556344509125\n",
            "Val loss 0.0510\n",
            "Dice score : 0.0678202286362648\n",
            "Val loss 0.0464\n",
            "Dice score : 0.03592268005013466\n",
            "Val loss 0.0435\n",
            "Dice score : 0.04519972577691078\n",
            "Val loss 0.0430\n",
            "Dice score : 0.054144661873579025\n",
            "Val loss 0.0428\n",
            "Dice score : 0.029499731957912445\n",
            "Val loss 0.0453\n",
            "Dice score : 0.051565371453762054\n",
            "Val loss 0.0441\n",
            "Dice score : 0.018796730786561966\n",
            "Val loss 0.0419\n",
            "Dice score : 0.03709183260798454\n",
            "Val loss 0.0410\n",
            "Dice score : 0.04551703855395317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 44/100 [58:57<1:14:35, 79.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0045 / 0100 | batch 0000 / 0050 | loss 0.0443\n",
            "Train: epoch 0045 / 0100 | batch 0001 / 0050 | loss 0.0450\n",
            "Train: epoch 0045 / 0100 | batch 0002 / 0050 | loss 0.0483\n",
            "Train: epoch 0045 / 0100 | batch 0003 / 0050 | loss 0.0442\n",
            "Train: epoch 0045 / 0100 | batch 0004 / 0050 | loss 0.0420\n",
            "Train: epoch 0045 / 0100 | batch 0005 / 0050 | loss 0.0423\n",
            "Train: epoch 0045 / 0100 | batch 0006 / 0050 | loss 0.0430\n",
            "Train: epoch 0045 / 0100 | batch 0007 / 0050 | loss 0.0436\n",
            "Train: epoch 0045 / 0100 | batch 0008 / 0050 | loss 0.0432\n",
            "Train: epoch 0045 / 0100 | batch 0009 / 0050 | loss 0.0448\n",
            "Train: epoch 0045 / 0100 | batch 0010 / 0050 | loss 0.0438\n",
            "Train: epoch 0045 / 0100 | batch 0011 / 0050 | loss 0.0427\n",
            "Train: epoch 0045 / 0100 | batch 0012 / 0050 | loss 0.0421\n",
            "Train: epoch 0045 / 0100 | batch 0013 / 0050 | loss 0.0418\n",
            "Train: epoch 0045 / 0100 | batch 0014 / 0050 | loss 0.0425\n",
            "Train: epoch 0045 / 0100 | batch 0015 / 0050 | loss 0.0435\n",
            "Train: epoch 0045 / 0100 | batch 0016 / 0050 | loss 0.0426\n",
            "Train: epoch 0045 / 0100 | batch 0017 / 0050 | loss 0.0427\n",
            "Train: epoch 0045 / 0100 | batch 0018 / 0050 | loss 0.0425\n",
            "Train: epoch 0045 / 0100 | batch 0019 / 0050 | loss 0.0416\n",
            "Train: epoch 0045 / 0100 | batch 0020 / 0050 | loss 0.0409\n",
            "Train: epoch 0045 / 0100 | batch 0021 / 0050 | loss 0.0404\n",
            "Train: epoch 0045 / 0100 | batch 0022 / 0050 | loss 0.0410\n",
            "Train: epoch 0045 / 0100 | batch 0023 / 0050 | loss 0.0412\n",
            "Train: epoch 0045 / 0100 | batch 0024 / 0050 | loss 0.0417\n",
            "Train: epoch 0045 / 0100 | batch 0025 / 0050 | loss 0.0410\n",
            "Train: epoch 0045 / 0100 | batch 0026 / 0050 | loss 0.0407\n",
            "Train: epoch 0045 / 0100 | batch 0027 / 0050 | loss 0.0409\n",
            "Train: epoch 0045 / 0100 | batch 0028 / 0050 | loss 0.0420\n",
            "Train: epoch 0045 / 0100 | batch 0029 / 0050 | loss 0.0421\n",
            "Train: epoch 0045 / 0100 | batch 0030 / 0050 | loss 0.0418\n",
            "Train: epoch 0045 / 0100 | batch 0031 / 0050 | loss 0.0416\n",
            "Train: epoch 0045 / 0100 | batch 0032 / 0050 | loss 0.0412\n",
            "Train: epoch 0045 / 0100 | batch 0033 / 0050 | loss 0.0409\n",
            "Train: epoch 0045 / 0100 | batch 0034 / 0050 | loss 0.0406\n",
            "Train: epoch 0045 / 0100 | batch 0035 / 0050 | loss 0.0406\n",
            "Train: epoch 0045 / 0100 | batch 0036 / 0050 | loss 0.0406\n",
            "Train: epoch 0045 / 0100 | batch 0037 / 0050 | loss 0.0413\n",
            "Train: epoch 0045 / 0100 | batch 0038 / 0050 | loss 0.0413\n",
            "Train: epoch 0045 / 0100 | batch 0039 / 0050 | loss 0.0410\n",
            "Train: epoch 0045 / 0100 | batch 0040 / 0050 | loss 0.0408\n",
            "Train: epoch 0045 / 0100 | batch 0041 / 0050 | loss 0.0407\n",
            "Train: epoch 0045 / 0100 | batch 0042 / 0050 | loss 0.0407\n",
            "Train: epoch 0045 / 0100 | batch 0043 / 0050 | loss 0.0405\n",
            "Train: epoch 0045 / 0100 | batch 0044 / 0050 | loss 0.0405\n",
            "Train: epoch 0045 / 0100 | batch 0045 / 0050 | loss 0.0408\n",
            "Train: epoch 0045 / 0100 | batch 0046 / 0050 | loss 0.0409\n",
            "Train: epoch 0045 / 0100 | batch 0047 / 0050 | loss 0.0410\n",
            "Train: epoch 0045 / 0100 | batch 0048 / 0050 | loss 0.0411\n",
            "Train: epoch 0045 / 0100 | batch 0049 / 0050 | loss 0.0408\n",
            "Val loss 0.0329\n",
            "Dice score : 0.04539112001657486\n",
            "Val loss 0.0304\n",
            "Dice score : 0.03186308220028877\n",
            "Val loss 0.0308\n",
            "Dice score : 0.041125331073999405\n",
            "Val loss 0.0301\n",
            "Dice score : 0.056338317692279816\n",
            "Val loss 0.0318\n",
            "Dice score : 0.045611586421728134\n",
            "Val loss 0.0367\n",
            "Dice score : 0.05145548656582832\n",
            "Val loss 0.0369\n",
            "Dice score : 0.03991518169641495\n",
            "Val loss 0.0390\n",
            "Dice score : 0.047662120312452316\n",
            "Val loss 0.0403\n",
            "Dice score : 0.05817081779241562\n",
            "Val loss 0.0400\n",
            "Dice score : 0.036980438977479935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 45/100 [1:00:17<1:13:11, 79.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0046 / 0100 | batch 0000 / 0050 | loss 0.0367\n",
            "Train: epoch 0046 / 0100 | batch 0001 / 0050 | loss 0.0308\n",
            "Train: epoch 0046 / 0100 | batch 0002 / 0050 | loss 0.0356\n",
            "Train: epoch 0046 / 0100 | batch 0003 / 0050 | loss 0.0344\n",
            "Train: epoch 0046 / 0100 | batch 0004 / 0050 | loss 0.0330\n",
            "Train: epoch 0046 / 0100 | batch 0005 / 0050 | loss 0.0322\n",
            "Train: epoch 0046 / 0100 | batch 0006 / 0050 | loss 0.0318\n",
            "Train: epoch 0046 / 0100 | batch 0007 / 0050 | loss 0.0328\n",
            "Train: epoch 0046 / 0100 | batch 0008 / 0050 | loss 0.0357\n",
            "Train: epoch 0046 / 0100 | batch 0009 / 0050 | loss 0.0354\n",
            "Train: epoch 0046 / 0100 | batch 0010 / 0050 | loss 0.0359\n",
            "Train: epoch 0046 / 0100 | batch 0011 / 0050 | loss 0.0363\n",
            "Train: epoch 0046 / 0100 | batch 0012 / 0050 | loss 0.0363\n",
            "Train: epoch 0046 / 0100 | batch 0013 / 0050 | loss 0.0369\n",
            "Train: epoch 0046 / 0100 | batch 0014 / 0050 | loss 0.0372\n",
            "Train: epoch 0046 / 0100 | batch 0015 / 0050 | loss 0.0365\n",
            "Train: epoch 0046 / 0100 | batch 0016 / 0050 | loss 0.0376\n",
            "Train: epoch 0046 / 0100 | batch 0017 / 0050 | loss 0.0378\n",
            "Train: epoch 0046 / 0100 | batch 0018 / 0050 | loss 0.0374\n",
            "Train: epoch 0046 / 0100 | batch 0019 / 0050 | loss 0.0387\n",
            "Train: epoch 0046 / 0100 | batch 0020 / 0050 | loss 0.0383\n",
            "Train: epoch 0046 / 0100 | batch 0021 / 0050 | loss 0.0379\n",
            "Train: epoch 0046 / 0100 | batch 0022 / 0050 | loss 0.0374\n",
            "Train: epoch 0046 / 0100 | batch 0023 / 0050 | loss 0.0383\n",
            "Train: epoch 0046 / 0100 | batch 0024 / 0050 | loss 0.0389\n",
            "Train: epoch 0046 / 0100 | batch 0025 / 0050 | loss 0.0390\n",
            "Train: epoch 0046 / 0100 | batch 0026 / 0050 | loss 0.0386\n",
            "Train: epoch 0046 / 0100 | batch 0027 / 0050 | loss 0.0393\n",
            "Train: epoch 0046 / 0100 | batch 0028 / 0050 | loss 0.0389\n",
            "Train: epoch 0046 / 0100 | batch 0029 / 0050 | loss 0.0388\n",
            "Train: epoch 0046 / 0100 | batch 0030 / 0050 | loss 0.0390\n",
            "Train: epoch 0046 / 0100 | batch 0031 / 0050 | loss 0.0405\n",
            "Train: epoch 0046 / 0100 | batch 0032 / 0050 | loss 0.0406\n",
            "Train: epoch 0046 / 0100 | batch 0033 / 0050 | loss 0.0413\n",
            "Train: epoch 0046 / 0100 | batch 0034 / 0050 | loss 0.0412\n",
            "Train: epoch 0046 / 0100 | batch 0035 / 0050 | loss 0.0410\n",
            "Train: epoch 0046 / 0100 | batch 0036 / 0050 | loss 0.0410\n",
            "Train: epoch 0046 / 0100 | batch 0037 / 0050 | loss 0.0412\n",
            "Train: epoch 0046 / 0100 | batch 0038 / 0050 | loss 0.0411\n",
            "Train: epoch 0046 / 0100 | batch 0039 / 0050 | loss 0.0410\n",
            "Train: epoch 0046 / 0100 | batch 0040 / 0050 | loss 0.0412\n",
            "Train: epoch 0046 / 0100 | batch 0041 / 0050 | loss 0.0409\n",
            "Train: epoch 0046 / 0100 | batch 0042 / 0050 | loss 0.0411\n",
            "Train: epoch 0046 / 0100 | batch 0043 / 0050 | loss 0.0409\n",
            "Train: epoch 0046 / 0100 | batch 0044 / 0050 | loss 0.0407\n",
            "Train: epoch 0046 / 0100 | batch 0045 / 0050 | loss 0.0407\n",
            "Train: epoch 0046 / 0100 | batch 0046 / 0050 | loss 0.0405\n",
            "Train: epoch 0046 / 0100 | batch 0047 / 0050 | loss 0.0404\n",
            "Train: epoch 0046 / 0100 | batch 0048 / 0050 | loss 0.0403\n",
            "Train: epoch 0046 / 0100 | batch 0049 / 0050 | loss 0.0400\n",
            "Val loss 0.0587\n",
            "Dice score : 0.03986376151442528\n",
            "Val loss 0.0573\n",
            "Dice score : 0.05199657008051872\n",
            "Val loss 0.0509\n",
            "Dice score : 0.0392787791788578\n",
            "Val loss 0.0475\n",
            "Dice score : 0.0439690425992012\n",
            "Val loss 0.0505\n",
            "Dice score : 0.05276703089475632\n",
            "Val loss 0.0470\n",
            "Dice score : 0.03617379441857338\n",
            "Val loss 0.0467\n",
            "Dice score : 0.045326653867959976\n",
            "Val loss 0.0454\n",
            "Dice score : 0.02922717295587063\n",
            "Val loss 0.0437\n",
            "Dice score : 0.04209195822477341\n",
            "Val loss 0.0426\n",
            "Dice score : 0.04069088026881218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 46/100 [1:01:38<1:12:12, 80.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0047 / 0100 | batch 0000 / 0050 | loss 0.0300\n",
            "Train: epoch 0047 / 0100 | batch 0001 / 0050 | loss 0.0330\n",
            "Train: epoch 0047 / 0100 | batch 0002 / 0050 | loss 0.0418\n",
            "Train: epoch 0047 / 0100 | batch 0003 / 0050 | loss 0.0427\n",
            "Train: epoch 0047 / 0100 | batch 0004 / 0050 | loss 0.0395\n",
            "Train: epoch 0047 / 0100 | batch 0005 / 0050 | loss 0.0375\n",
            "Train: epoch 0047 / 0100 | batch 0006 / 0050 | loss 0.0400\n",
            "Train: epoch 0047 / 0100 | batch 0007 / 0050 | loss 0.0387\n",
            "Train: epoch 0047 / 0100 | batch 0008 / 0050 | loss 0.0390\n",
            "Train: epoch 0047 / 0100 | batch 0009 / 0050 | loss 0.0389\n",
            "Train: epoch 0047 / 0100 | batch 0010 / 0050 | loss 0.0401\n",
            "Train: epoch 0047 / 0100 | batch 0011 / 0050 | loss 0.0391\n",
            "Train: epoch 0047 / 0100 | batch 0012 / 0050 | loss 0.0388\n",
            "Train: epoch 0047 / 0100 | batch 0013 / 0050 | loss 0.0389\n",
            "Train: epoch 0047 / 0100 | batch 0014 / 0050 | loss 0.0383\n",
            "Train: epoch 0047 / 0100 | batch 0015 / 0050 | loss 0.0387\n",
            "Train: epoch 0047 / 0100 | batch 0016 / 0050 | loss 0.0381\n",
            "Train: epoch 0047 / 0100 | batch 0017 / 0050 | loss 0.0379\n",
            "Train: epoch 0047 / 0100 | batch 0018 / 0050 | loss 0.0375\n",
            "Train: epoch 0047 / 0100 | batch 0019 / 0050 | loss 0.0381\n",
            "Train: epoch 0047 / 0100 | batch 0020 / 0050 | loss 0.0394\n",
            "Train: epoch 0047 / 0100 | batch 0021 / 0050 | loss 0.0390\n",
            "Train: epoch 0047 / 0100 | batch 0022 / 0050 | loss 0.0388\n",
            "Train: epoch 0047 / 0100 | batch 0023 / 0050 | loss 0.0394\n",
            "Train: epoch 0047 / 0100 | batch 0024 / 0050 | loss 0.0391\n",
            "Train: epoch 0047 / 0100 | batch 0025 / 0050 | loss 0.0388\n",
            "Train: epoch 0047 / 0100 | batch 0026 / 0050 | loss 0.0393\n",
            "Train: epoch 0047 / 0100 | batch 0027 / 0050 | loss 0.0396\n",
            "Train: epoch 0047 / 0100 | batch 0028 / 0050 | loss 0.0396\n",
            "Train: epoch 0047 / 0100 | batch 0029 / 0050 | loss 0.0395\n",
            "Train: epoch 0047 / 0100 | batch 0030 / 0050 | loss 0.0397\n",
            "Train: epoch 0047 / 0100 | batch 0031 / 0050 | loss 0.0396\n",
            "Train: epoch 0047 / 0100 | batch 0032 / 0050 | loss 0.0395\n",
            "Train: epoch 0047 / 0100 | batch 0033 / 0050 | loss 0.0393\n",
            "Train: epoch 0047 / 0100 | batch 0034 / 0050 | loss 0.0390\n",
            "Train: epoch 0047 / 0100 | batch 0035 / 0050 | loss 0.0393\n",
            "Train: epoch 0047 / 0100 | batch 0036 / 0050 | loss 0.0397\n",
            "Train: epoch 0047 / 0100 | batch 0037 / 0050 | loss 0.0398\n",
            "Train: epoch 0047 / 0100 | batch 0038 / 0050 | loss 0.0399\n",
            "Train: epoch 0047 / 0100 | batch 0039 / 0050 | loss 0.0402\n",
            "Train: epoch 0047 / 0100 | batch 0040 / 0050 | loss 0.0400\n",
            "Train: epoch 0047 / 0100 | batch 0041 / 0050 | loss 0.0396\n",
            "Train: epoch 0047 / 0100 | batch 0042 / 0050 | loss 0.0397\n",
            "Train: epoch 0047 / 0100 | batch 0043 / 0050 | loss 0.0401\n",
            "Train: epoch 0047 / 0100 | batch 0044 / 0050 | loss 0.0405\n",
            "Train: epoch 0047 / 0100 | batch 0045 / 0050 | loss 0.0403\n",
            "Train: epoch 0047 / 0100 | batch 0046 / 0050 | loss 0.0406\n",
            "Train: epoch 0047 / 0100 | batch 0047 / 0050 | loss 0.0403\n",
            "Train: epoch 0047 / 0100 | batch 0048 / 0050 | loss 0.0405\n",
            "Train: epoch 0047 / 0100 | batch 0049 / 0050 | loss 0.0403\n",
            "Val loss 0.0406\n",
            "Dice score : 0.05567947402596474\n",
            "Val loss 0.0510\n",
            "Dice score : 0.038085877895355225\n",
            "Val loss 0.0509\n",
            "Dice score : 0.025637920945882797\n",
            "Val loss 0.0444\n",
            "Dice score : 0.052477333694696426\n",
            "Val loss 0.0466\n",
            "Dice score : 0.037364762276411057\n",
            "Val loss 0.0475\n",
            "Dice score : 0.07138974219560623\n",
            "Val loss 0.0502\n",
            "Dice score : 0.05131051316857338\n",
            "Val loss 0.0489\n",
            "Dice score : 0.038424331694841385\n",
            "Val loss 0.0506\n",
            "Dice score : 0.04778354614973068\n",
            "Val loss 0.0483\n",
            "Dice score : 0.04906713217496872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 47/100 [1:02:58<1:10:46, 80.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0048 / 0100 | batch 0000 / 0050 | loss 0.0295\n",
            "Train: epoch 0048 / 0100 | batch 0001 / 0050 | loss 0.0450\n",
            "Train: epoch 0048 / 0100 | batch 0002 / 0050 | loss 0.0418\n",
            "Train: epoch 0048 / 0100 | batch 0003 / 0050 | loss 0.0381\n",
            "Train: epoch 0048 / 0100 | batch 0004 / 0050 | loss 0.0373\n",
            "Train: epoch 0048 / 0100 | batch 0005 / 0050 | loss 0.0393\n",
            "Train: epoch 0048 / 0100 | batch 0006 / 0050 | loss 0.0407\n",
            "Train: epoch 0048 / 0100 | batch 0007 / 0050 | loss 0.0417\n",
            "Train: epoch 0048 / 0100 | batch 0008 / 0050 | loss 0.0405\n",
            "Train: epoch 0048 / 0100 | batch 0009 / 0050 | loss 0.0402\n",
            "Train: epoch 0048 / 0100 | batch 0010 / 0050 | loss 0.0414\n",
            "Train: epoch 0048 / 0100 | batch 0011 / 0050 | loss 0.0412\n",
            "Train: epoch 0048 / 0100 | batch 0012 / 0050 | loss 0.0412\n",
            "Train: epoch 0048 / 0100 | batch 0013 / 0050 | loss 0.0409\n",
            "Train: epoch 0048 / 0100 | batch 0014 / 0050 | loss 0.0401\n",
            "Train: epoch 0048 / 0100 | batch 0015 / 0050 | loss 0.0402\n",
            "Train: epoch 0048 / 0100 | batch 0016 / 0050 | loss 0.0398\n",
            "Train: epoch 0048 / 0100 | batch 0017 / 0050 | loss 0.0395\n",
            "Train: epoch 0048 / 0100 | batch 0018 / 0050 | loss 0.0395\n",
            "Train: epoch 0048 / 0100 | batch 0019 / 0050 | loss 0.0397\n",
            "Train: epoch 0048 / 0100 | batch 0020 / 0050 | loss 0.0399\n",
            "Train: epoch 0048 / 0100 | batch 0021 / 0050 | loss 0.0407\n",
            "Train: epoch 0048 / 0100 | batch 0022 / 0050 | loss 0.0408\n",
            "Train: epoch 0048 / 0100 | batch 0023 / 0050 | loss 0.0407\n",
            "Train: epoch 0048 / 0100 | batch 0024 / 0050 | loss 0.0401\n",
            "Train: epoch 0048 / 0100 | batch 0025 / 0050 | loss 0.0402\n",
            "Train: epoch 0048 / 0100 | batch 0026 / 0050 | loss 0.0399\n",
            "Train: epoch 0048 / 0100 | batch 0027 / 0050 | loss 0.0397\n",
            "Train: epoch 0048 / 0100 | batch 0028 / 0050 | loss 0.0394\n",
            "Train: epoch 0048 / 0100 | batch 0029 / 0050 | loss 0.0397\n",
            "Train: epoch 0048 / 0100 | batch 0030 / 0050 | loss 0.0400\n",
            "Train: epoch 0048 / 0100 | batch 0031 / 0050 | loss 0.0398\n",
            "Train: epoch 0048 / 0100 | batch 0032 / 0050 | loss 0.0400\n",
            "Train: epoch 0048 / 0100 | batch 0033 / 0050 | loss 0.0398\n",
            "Train: epoch 0048 / 0100 | batch 0034 / 0050 | loss 0.0405\n",
            "Train: epoch 0048 / 0100 | batch 0035 / 0050 | loss 0.0400\n",
            "Train: epoch 0048 / 0100 | batch 0036 / 0050 | loss 0.0399\n",
            "Train: epoch 0048 / 0100 | batch 0037 / 0050 | loss 0.0399\n",
            "Train: epoch 0048 / 0100 | batch 0038 / 0050 | loss 0.0395\n",
            "Train: epoch 0048 / 0100 | batch 0039 / 0050 | loss 0.0396\n",
            "Train: epoch 0048 / 0100 | batch 0040 / 0050 | loss 0.0396\n",
            "Train: epoch 0048 / 0100 | batch 0041 / 0050 | loss 0.0398\n",
            "Train: epoch 0048 / 0100 | batch 0042 / 0050 | loss 0.0400\n",
            "Train: epoch 0048 / 0100 | batch 0043 / 0050 | loss 0.0399\n",
            "Train: epoch 0048 / 0100 | batch 0044 / 0050 | loss 0.0397\n",
            "Train: epoch 0048 / 0100 | batch 0045 / 0050 | loss 0.0401\n",
            "Train: epoch 0048 / 0100 | batch 0046 / 0050 | loss 0.0406\n",
            "Train: epoch 0048 / 0100 | batch 0047 / 0050 | loss 0.0404\n",
            "Train: epoch 0048 / 0100 | batch 0048 / 0050 | loss 0.0403\n",
            "Train: epoch 0048 / 0100 | batch 0049 / 0050 | loss 0.0401\n",
            "Val loss 0.0576\n",
            "Dice score : 0.02073865942656994\n",
            "Val loss 0.0503\n",
            "Dice score : 0.03823414444923401\n",
            "Val loss 0.0536\n",
            "Dice score : 0.05068023502826691\n",
            "Val loss 0.0543\n",
            "Dice score : 0.1128377690911293\n",
            "Val loss 0.0509\n",
            "Dice score : 0.032705191522836685\n",
            "Val loss 0.0573\n",
            "Dice score : 0.06506210565567017\n",
            "Val loss 0.0551\n",
            "Dice score : 0.030914366245269775\n",
            "Val loss 0.0549\n",
            "Dice score : 0.036348309367895126\n",
            "Val loss 0.0533\n",
            "Dice score : 0.026854868978261948\n",
            "Val loss 0.0529\n",
            "Dice score : 0.03791167959570885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 48/100 [1:04:17<1:09:04, 79.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0049 / 0100 | batch 0000 / 0050 | loss 0.0321\n",
            "Train: epoch 0049 / 0100 | batch 0001 / 0050 | loss 0.0339\n",
            "Train: epoch 0049 / 0100 | batch 0002 / 0050 | loss 0.0322\n",
            "Train: epoch 0049 / 0100 | batch 0003 / 0050 | loss 0.0430\n",
            "Train: epoch 0049 / 0100 | batch 0004 / 0050 | loss 0.0451\n",
            "Train: epoch 0049 / 0100 | batch 0005 / 0050 | loss 0.0430\n",
            "Train: epoch 0049 / 0100 | batch 0006 / 0050 | loss 0.0442\n",
            "Train: epoch 0049 / 0100 | batch 0007 / 0050 | loss 0.0437\n",
            "Train: epoch 0049 / 0100 | batch 0008 / 0050 | loss 0.0426\n",
            "Train: epoch 0049 / 0100 | batch 0009 / 0050 | loss 0.0413\n",
            "Train: epoch 0049 / 0100 | batch 0010 / 0050 | loss 0.0417\n",
            "Train: epoch 0049 / 0100 | batch 0011 / 0050 | loss 0.0419\n",
            "Train: epoch 0049 / 0100 | batch 0012 / 0050 | loss 0.0411\n",
            "Train: epoch 0049 / 0100 | batch 0013 / 0050 | loss 0.0413\n",
            "Train: epoch 0049 / 0100 | batch 0014 / 0050 | loss 0.0415\n",
            "Train: epoch 0049 / 0100 | batch 0015 / 0050 | loss 0.0409\n",
            "Train: epoch 0049 / 0100 | batch 0016 / 0050 | loss 0.0405\n",
            "Train: epoch 0049 / 0100 | batch 0017 / 0050 | loss 0.0398\n",
            "Train: epoch 0049 / 0100 | batch 0018 / 0050 | loss 0.0397\n",
            "Train: epoch 0049 / 0100 | batch 0019 / 0050 | loss 0.0406\n",
            "Train: epoch 0049 / 0100 | batch 0020 / 0050 | loss 0.0401\n",
            "Train: epoch 0049 / 0100 | batch 0021 / 0050 | loss 0.0398\n",
            "Train: epoch 0049 / 0100 | batch 0022 / 0050 | loss 0.0402\n",
            "Train: epoch 0049 / 0100 | batch 0023 / 0050 | loss 0.0405\n",
            "Train: epoch 0049 / 0100 | batch 0024 / 0050 | loss 0.0423\n",
            "Train: epoch 0049 / 0100 | batch 0025 / 0050 | loss 0.0426\n",
            "Train: epoch 0049 / 0100 | batch 0026 / 0050 | loss 0.0422\n",
            "Train: epoch 0049 / 0100 | batch 0027 / 0050 | loss 0.0423\n",
            "Train: epoch 0049 / 0100 | batch 0028 / 0050 | loss 0.0427\n",
            "Train: epoch 0049 / 0100 | batch 0029 / 0050 | loss 0.0426\n",
            "Train: epoch 0049 / 0100 | batch 0030 / 0050 | loss 0.0427\n",
            "Train: epoch 0049 / 0100 | batch 0031 / 0050 | loss 0.0427\n",
            "Train: epoch 0049 / 0100 | batch 0032 / 0050 | loss 0.0423\n",
            "Train: epoch 0049 / 0100 | batch 0033 / 0050 | loss 0.0422\n",
            "Train: epoch 0049 / 0100 | batch 0034 / 0050 | loss 0.0422\n",
            "Train: epoch 0049 / 0100 | batch 0035 / 0050 | loss 0.0419\n",
            "Train: epoch 0049 / 0100 | batch 0036 / 0050 | loss 0.0420\n",
            "Train: epoch 0049 / 0100 | batch 0037 / 0050 | loss 0.0416\n",
            "Train: epoch 0049 / 0100 | batch 0038 / 0050 | loss 0.0414\n",
            "Train: epoch 0049 / 0100 | batch 0039 / 0050 | loss 0.0412\n",
            "Train: epoch 0049 / 0100 | batch 0040 / 0050 | loss 0.0410\n",
            "Train: epoch 0049 / 0100 | batch 0041 / 0050 | loss 0.0409\n",
            "Train: epoch 0049 / 0100 | batch 0042 / 0050 | loss 0.0408\n",
            "Train: epoch 0049 / 0100 | batch 0043 / 0050 | loss 0.0404\n",
            "Train: epoch 0049 / 0100 | batch 0044 / 0050 | loss 0.0409\n",
            "Train: epoch 0049 / 0100 | batch 0045 / 0050 | loss 0.0408\n",
            "Train: epoch 0049 / 0100 | batch 0046 / 0050 | loss 0.0407\n",
            "Train: epoch 0049 / 0100 | batch 0047 / 0050 | loss 0.0407\n",
            "Train: epoch 0049 / 0100 | batch 0048 / 0050 | loss 0.0404\n",
            "Train: epoch 0049 / 0100 | batch 0049 / 0050 | loss 0.0402\n",
            "Val loss 0.0534\n",
            "Dice score : 0.05274169519543648\n",
            "Val loss 0.0585\n",
            "Dice score : 0.043011024594306946\n",
            "Val loss 0.0518\n",
            "Dice score : 0.027358435094356537\n",
            "Val loss 0.0448\n",
            "Dice score : 0.04530245438218117\n",
            "Val loss 0.0434\n",
            "Dice score : 0.04747464507818222\n",
            "Val loss 0.0412\n",
            "Dice score : 0.03400195762515068\n",
            "Val loss 0.0416\n",
            "Dice score : 0.043363433331251144\n",
            "Val loss 0.0409\n",
            "Dice score : 0.03984817489981651\n",
            "Val loss 0.0406\n",
            "Dice score : 0.03418176993727684\n",
            "Val loss 0.0398\n",
            "Dice score : 0.0439850278198719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 49/100 [1:05:38<1:08:02, 80.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0050 / 0100 | batch 0000 / 0050 | loss 0.0420\n",
            "Train: epoch 0050 / 0100 | batch 0001 / 0050 | loss 0.0436\n",
            "Train: epoch 0050 / 0100 | batch 0002 / 0050 | loss 0.0436\n",
            "Train: epoch 0050 / 0100 | batch 0003 / 0050 | loss 0.0411\n",
            "Train: epoch 0050 / 0100 | batch 0004 / 0050 | loss 0.0433\n",
            "Train: epoch 0050 / 0100 | batch 0005 / 0050 | loss 0.0451\n",
            "Train: epoch 0050 / 0100 | batch 0006 / 0050 | loss 0.0471\n",
            "Train: epoch 0050 / 0100 | batch 0007 / 0050 | loss 0.0446\n",
            "Train: epoch 0050 / 0100 | batch 0008 / 0050 | loss 0.0435\n",
            "Train: epoch 0050 / 0100 | batch 0009 / 0050 | loss 0.0437\n",
            "Train: epoch 0050 / 0100 | batch 0010 / 0050 | loss 0.0425\n",
            "Train: epoch 0050 / 0100 | batch 0011 / 0050 | loss 0.0415\n",
            "Train: epoch 0050 / 0100 | batch 0012 / 0050 | loss 0.0430\n",
            "Train: epoch 0050 / 0100 | batch 0013 / 0050 | loss 0.0421\n",
            "Train: epoch 0050 / 0100 | batch 0014 / 0050 | loss 0.0413\n",
            "Train: epoch 0050 / 0100 | batch 0015 / 0050 | loss 0.0407\n",
            "Train: epoch 0050 / 0100 | batch 0016 / 0050 | loss 0.0405\n",
            "Train: epoch 0050 / 0100 | batch 0017 / 0050 | loss 0.0404\n",
            "Train: epoch 0050 / 0100 | batch 0018 / 0050 | loss 0.0414\n",
            "Train: epoch 0050 / 0100 | batch 0019 / 0050 | loss 0.0421\n",
            "Train: epoch 0050 / 0100 | batch 0020 / 0050 | loss 0.0428\n",
            "Train: epoch 0050 / 0100 | batch 0021 / 0050 | loss 0.0421\n",
            "Train: epoch 0050 / 0100 | batch 0022 / 0050 | loss 0.0420\n",
            "Train: epoch 0050 / 0100 | batch 0023 / 0050 | loss 0.0415\n",
            "Train: epoch 0050 / 0100 | batch 0024 / 0050 | loss 0.0413\n",
            "Train: epoch 0050 / 0100 | batch 0025 / 0050 | loss 0.0407\n",
            "Train: epoch 0050 / 0100 | batch 0026 / 0050 | loss 0.0403\n",
            "Train: epoch 0050 / 0100 | batch 0027 / 0050 | loss 0.0406\n",
            "Train: epoch 0050 / 0100 | batch 0028 / 0050 | loss 0.0405\n",
            "Train: epoch 0050 / 0100 | batch 0029 / 0050 | loss 0.0403\n",
            "Train: epoch 0050 / 0100 | batch 0030 / 0050 | loss 0.0404\n",
            "Train: epoch 0050 / 0100 | batch 0031 / 0050 | loss 0.0405\n",
            "Train: epoch 0050 / 0100 | batch 0032 / 0050 | loss 0.0402\n",
            "Train: epoch 0050 / 0100 | batch 0033 / 0050 | loss 0.0398\n",
            "Train: epoch 0050 / 0100 | batch 0034 / 0050 | loss 0.0401\n",
            "Train: epoch 0050 / 0100 | batch 0035 / 0050 | loss 0.0402\n",
            "Train: epoch 0050 / 0100 | batch 0036 / 0050 | loss 0.0400\n",
            "Train: epoch 0050 / 0100 | batch 0037 / 0050 | loss 0.0400\n",
            "Train: epoch 0050 / 0100 | batch 0038 / 0050 | loss 0.0402\n",
            "Train: epoch 0050 / 0100 | batch 0039 / 0050 | loss 0.0400\n",
            "Train: epoch 0050 / 0100 | batch 0040 / 0050 | loss 0.0401\n",
            "Train: epoch 0050 / 0100 | batch 0041 / 0050 | loss 0.0401\n",
            "Train: epoch 0050 / 0100 | batch 0042 / 0050 | loss 0.0402\n",
            "Train: epoch 0050 / 0100 | batch 0043 / 0050 | loss 0.0399\n",
            "Train: epoch 0050 / 0100 | batch 0044 / 0050 | loss 0.0401\n",
            "Train: epoch 0050 / 0100 | batch 0045 / 0050 | loss 0.0401\n",
            "Train: epoch 0050 / 0100 | batch 0046 / 0050 | loss 0.0399\n",
            "Train: epoch 0050 / 0100 | batch 0047 / 0050 | loss 0.0398\n",
            "Train: epoch 0050 / 0100 | batch 0048 / 0050 | loss 0.0394\n",
            "Train: epoch 0050 / 0100 | batch 0049 / 0050 | loss 0.0396\n",
            "Val loss 0.0251\n",
            "Dice score : 0.04675498604774475\n",
            "Val loss 0.0290\n",
            "Dice score : 0.04629790782928467\n",
            "Val loss 0.0280\n",
            "Dice score : 0.03431368246674538\n",
            "Val loss 0.0264\n",
            "Dice score : 0.044187527149915695\n",
            "Val loss 0.0340\n",
            "Dice score : 0.07795097678899765\n",
            "Val loss 0.0362\n",
            "Dice score : 0.03194955736398697\n",
            "Val loss 0.0350\n",
            "Dice score : 0.03497885540127754\n",
            "Val loss 0.0361\n",
            "Dice score : 0.06197052076458931\n",
            "Val loss 0.0366\n",
            "Dice score : 0.04511498659849167\n",
            "Val loss 0.0377\n",
            "Dice score : 0.04620753973722458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 50/100 [1:07:00<1:07:14, 80.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0051 / 0100 | batch 0000 / 0050 | loss 0.0254\n",
            "Train: epoch 0051 / 0100 | batch 0001 / 0050 | loss 0.0325\n",
            "Train: epoch 0051 / 0100 | batch 0002 / 0050 | loss 0.0361\n",
            "Train: epoch 0051 / 0100 | batch 0003 / 0050 | loss 0.0391\n",
            "Train: epoch 0051 / 0100 | batch 0004 / 0050 | loss 0.0373\n",
            "Train: epoch 0051 / 0100 | batch 0005 / 0050 | loss 0.0405\n",
            "Train: epoch 0051 / 0100 | batch 0006 / 0050 | loss 0.0414\n",
            "Train: epoch 0051 / 0100 | batch 0007 / 0050 | loss 0.0395\n",
            "Train: epoch 0051 / 0100 | batch 0008 / 0050 | loss 0.0400\n",
            "Train: epoch 0051 / 0100 | batch 0009 / 0050 | loss 0.0418\n",
            "Train: epoch 0051 / 0100 | batch 0010 / 0050 | loss 0.0411\n",
            "Train: epoch 0051 / 0100 | batch 0011 / 0050 | loss 0.0410\n",
            "Train: epoch 0051 / 0100 | batch 0012 / 0050 | loss 0.0398\n",
            "Train: epoch 0051 / 0100 | batch 0013 / 0050 | loss 0.0399\n",
            "Train: epoch 0051 / 0100 | batch 0014 / 0050 | loss 0.0398\n",
            "Train: epoch 0051 / 0100 | batch 0015 / 0050 | loss 0.0394\n",
            "Train: epoch 0051 / 0100 | batch 0016 / 0050 | loss 0.0393\n",
            "Train: epoch 0051 / 0100 | batch 0017 / 0050 | loss 0.0396\n",
            "Train: epoch 0051 / 0100 | batch 0018 / 0050 | loss 0.0393\n",
            "Train: epoch 0051 / 0100 | batch 0019 / 0050 | loss 0.0394\n",
            "Train: epoch 0051 / 0100 | batch 0020 / 0050 | loss 0.0389\n",
            "Train: epoch 0051 / 0100 | batch 0021 / 0050 | loss 0.0384\n",
            "Train: epoch 0051 / 0100 | batch 0022 / 0050 | loss 0.0390\n",
            "Train: epoch 0051 / 0100 | batch 0023 / 0050 | loss 0.0384\n",
            "Train: epoch 0051 / 0100 | batch 0024 / 0050 | loss 0.0382\n",
            "Train: epoch 0051 / 0100 | batch 0025 / 0050 | loss 0.0379\n",
            "Train: epoch 0051 / 0100 | batch 0026 / 0050 | loss 0.0381\n",
            "Train: epoch 0051 / 0100 | batch 0027 / 0050 | loss 0.0398\n",
            "Train: epoch 0051 / 0100 | batch 0028 / 0050 | loss 0.0401\n",
            "Train: epoch 0051 / 0100 | batch 0029 / 0050 | loss 0.0399\n",
            "Train: epoch 0051 / 0100 | batch 0030 / 0050 | loss 0.0395\n",
            "Train: epoch 0051 / 0100 | batch 0031 / 0050 | loss 0.0395\n",
            "Train: epoch 0051 / 0100 | batch 0032 / 0050 | loss 0.0398\n",
            "Train: epoch 0051 / 0100 | batch 0033 / 0050 | loss 0.0400\n",
            "Train: epoch 0051 / 0100 | batch 0034 / 0050 | loss 0.0397\n",
            "Train: epoch 0051 / 0100 | batch 0035 / 0050 | loss 0.0398\n",
            "Train: epoch 0051 / 0100 | batch 0036 / 0050 | loss 0.0395\n",
            "Train: epoch 0051 / 0100 | batch 0037 / 0050 | loss 0.0395\n",
            "Train: epoch 0051 / 0100 | batch 0038 / 0050 | loss 0.0395\n",
            "Train: epoch 0051 / 0100 | batch 0039 / 0050 | loss 0.0392\n",
            "Train: epoch 0051 / 0100 | batch 0040 / 0050 | loss 0.0389\n",
            "Train: epoch 0051 / 0100 | batch 0041 / 0050 | loss 0.0392\n",
            "Train: epoch 0051 / 0100 | batch 0042 / 0050 | loss 0.0389\n",
            "Train: epoch 0051 / 0100 | batch 0043 / 0050 | loss 0.0396\n",
            "Train: epoch 0051 / 0100 | batch 0044 / 0050 | loss 0.0394\n",
            "Train: epoch 0051 / 0100 | batch 0045 / 0050 | loss 0.0395\n",
            "Train: epoch 0051 / 0100 | batch 0046 / 0050 | loss 0.0397\n",
            "Train: epoch 0051 / 0100 | batch 0047 / 0050 | loss 0.0394\n",
            "Train: epoch 0051 / 0100 | batch 0048 / 0050 | loss 0.0395\n",
            "Train: epoch 0051 / 0100 | batch 0049 / 0050 | loss 0.0393\n",
            "Val loss 0.0243\n",
            "Dice score : 0.04854632914066315\n",
            "Val loss 0.0276\n",
            "Dice score : 0.04740403592586517\n",
            "Val loss 0.0289\n",
            "Dice score : 0.06863118708133698\n",
            "Val loss 0.0315\n",
            "Dice score : 0.05741838365793228\n",
            "Val loss 0.0298\n",
            "Dice score : 0.043037693947553635\n",
            "Val loss 0.0320\n",
            "Dice score : 0.03429458662867546\n",
            "Val loss 0.0331\n",
            "Dice score : 0.06528833508491516\n",
            "Val loss 0.0332\n",
            "Dice score : 0.04556490480899811\n",
            "Val loss 0.0350\n",
            "Dice score : 0.05957527086138725\n",
            "Val loss 0.0374\n",
            "Dice score : 0.05513937026262283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 51/100 [1:08:23<1:06:28, 81.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0052 / 0100 | batch 0000 / 0050 | loss 0.0440\n",
            "Train: epoch 0052 / 0100 | batch 0001 / 0050 | loss 0.0358\n",
            "Train: epoch 0052 / 0100 | batch 0002 / 0050 | loss 0.0345\n",
            "Train: epoch 0052 / 0100 | batch 0003 / 0050 | loss 0.0371\n",
            "Train: epoch 0052 / 0100 | batch 0004 / 0050 | loss 0.0347\n",
            "Train: epoch 0052 / 0100 | batch 0005 / 0050 | loss 0.0338\n",
            "Train: epoch 0052 / 0100 | batch 0006 / 0050 | loss 0.0365\n",
            "Train: epoch 0052 / 0100 | batch 0007 / 0050 | loss 0.0353\n",
            "Train: epoch 0052 / 0100 | batch 0008 / 0050 | loss 0.0384\n",
            "Train: epoch 0052 / 0100 | batch 0009 / 0050 | loss 0.0376\n",
            "Train: epoch 0052 / 0100 | batch 0010 / 0050 | loss 0.0371\n",
            "Train: epoch 0052 / 0100 | batch 0011 / 0050 | loss 0.0366\n",
            "Train: epoch 0052 / 0100 | batch 0012 / 0050 | loss 0.0357\n",
            "Train: epoch 0052 / 0100 | batch 0013 / 0050 | loss 0.0355\n",
            "Train: epoch 0052 / 0100 | batch 0014 / 0050 | loss 0.0348\n",
            "Train: epoch 0052 / 0100 | batch 0015 / 0050 | loss 0.0350\n",
            "Train: epoch 0052 / 0100 | batch 0016 / 0050 | loss 0.0354\n",
            "Train: epoch 0052 / 0100 | batch 0017 / 0050 | loss 0.0350\n",
            "Train: epoch 0052 / 0100 | batch 0018 / 0050 | loss 0.0348\n",
            "Train: epoch 0052 / 0100 | batch 0019 / 0050 | loss 0.0352\n",
            "Train: epoch 0052 / 0100 | batch 0020 / 0050 | loss 0.0350\n",
            "Train: epoch 0052 / 0100 | batch 0021 / 0050 | loss 0.0350\n",
            "Train: epoch 0052 / 0100 | batch 0022 / 0050 | loss 0.0347\n",
            "Train: epoch 0052 / 0100 | batch 0023 / 0050 | loss 0.0367\n",
            "Train: epoch 0052 / 0100 | batch 0024 / 0050 | loss 0.0363\n",
            "Train: epoch 0052 / 0100 | batch 0025 / 0050 | loss 0.0361\n",
            "Train: epoch 0052 / 0100 | batch 0026 / 0050 | loss 0.0364\n",
            "Train: epoch 0052 / 0100 | batch 0027 / 0050 | loss 0.0363\n",
            "Train: epoch 0052 / 0100 | batch 0028 / 0050 | loss 0.0365\n",
            "Train: epoch 0052 / 0100 | batch 0029 / 0050 | loss 0.0362\n",
            "Train: epoch 0052 / 0100 | batch 0030 / 0050 | loss 0.0363\n",
            "Train: epoch 0052 / 0100 | batch 0031 / 0050 | loss 0.0367\n",
            "Train: epoch 0052 / 0100 | batch 0032 / 0050 | loss 0.0369\n",
            "Train: epoch 0052 / 0100 | batch 0033 / 0050 | loss 0.0371\n",
            "Train: epoch 0052 / 0100 | batch 0034 / 0050 | loss 0.0370\n",
            "Train: epoch 0052 / 0100 | batch 0035 / 0050 | loss 0.0373\n",
            "Train: epoch 0052 / 0100 | batch 0036 / 0050 | loss 0.0376\n",
            "Train: epoch 0052 / 0100 | batch 0037 / 0050 | loss 0.0372\n",
            "Train: epoch 0052 / 0100 | batch 0038 / 0050 | loss 0.0370\n",
            "Train: epoch 0052 / 0100 | batch 0039 / 0050 | loss 0.0375\n",
            "Train: epoch 0052 / 0100 | batch 0040 / 0050 | loss 0.0374\n",
            "Train: epoch 0052 / 0100 | batch 0041 / 0050 | loss 0.0375\n",
            "Train: epoch 0052 / 0100 | batch 0042 / 0050 | loss 0.0377\n",
            "Train: epoch 0052 / 0100 | batch 0043 / 0050 | loss 0.0379\n",
            "Train: epoch 0052 / 0100 | batch 0044 / 0050 | loss 0.0379\n",
            "Train: epoch 0052 / 0100 | batch 0045 / 0050 | loss 0.0382\n",
            "Train: epoch 0052 / 0100 | batch 0046 / 0050 | loss 0.0384\n",
            "Train: epoch 0052 / 0100 | batch 0047 / 0050 | loss 0.0386\n",
            "Train: epoch 0052 / 0100 | batch 0048 / 0050 | loss 0.0385\n",
            "Train: epoch 0052 / 0100 | batch 0049 / 0050 | loss 0.0385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 51/100 [1:09:31<1:06:48, 81.80s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-01d8978e175b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mSpine_segment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer_Spine_segment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpine_segment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer_Spine_segment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-d5b143b6aaeb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, validation_loader)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalidation_loader\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d5b143b6aaeb>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, data_loader)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;31m#forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Spine_segment = UNET()\n",
        "Spine_segment.cuda()\n",
        "trainer_Spine_segment = trainer(Spine_segment, train_loader,\"Adam\", epoch_size=100, learning_rate=0.001)\n",
        "trainer_Spine_segment.train(val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZLqogj5KeDM",
        "outputId": "ae4eacf4-bb0e-493c-f788-d18e7de5b0cc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0001 / 0100 | batch 0000 / 0050 | loss 0.7131\n",
            "Train: epoch 0001 / 0100 | batch 0001 / 0050 | loss 0.6970\n",
            "Train: epoch 0001 / 0100 | batch 0002 / 0050 | loss 0.6779\n",
            "Train: epoch 0001 / 0100 | batch 0003 / 0050 | loss 0.6553\n",
            "Train: epoch 0001 / 0100 | batch 0004 / 0050 | loss 0.6379\n",
            "Train: epoch 0001 / 0100 | batch 0005 / 0050 | loss 0.6231\n",
            "Train: epoch 0001 / 0100 | batch 0006 / 0050 | loss 0.6093\n",
            "Train: epoch 0001 / 0100 | batch 0007 / 0050 | loss 0.5973\n",
            "Train: epoch 0001 / 0100 | batch 0008 / 0050 | loss 0.5869\n",
            "Train: epoch 0001 / 0100 | batch 0009 / 0050 | loss 0.5779\n",
            "Train: epoch 0001 / 0100 | batch 0010 / 0050 | loss 0.5701\n",
            "Train: epoch 0001 / 0100 | batch 0011 / 0050 | loss 0.5625\n",
            "Train: epoch 0001 / 0100 | batch 0012 / 0050 | loss 0.5555\n",
            "Train: epoch 0001 / 0100 | batch 0013 / 0050 | loss 0.5487\n",
            "Train: epoch 0001 / 0100 | batch 0014 / 0050 | loss 0.5419\n",
            "Train: epoch 0001 / 0100 | batch 0015 / 0050 | loss 0.5357\n",
            "Train: epoch 0001 / 0100 | batch 0016 / 0050 | loss 0.5295\n",
            "Train: epoch 0001 / 0100 | batch 0017 / 0050 | loss 0.5238\n",
            "Train: epoch 0001 / 0100 | batch 0018 / 0050 | loss 0.5183\n",
            "Train: epoch 0001 / 0100 | batch 0019 / 0050 | loss 0.5126\n",
            "Train: epoch 0001 / 0100 | batch 0020 / 0050 | loss 0.5070\n",
            "Train: epoch 0001 / 0100 | batch 0021 / 0050 | loss 0.5017\n",
            "Train: epoch 0001 / 0100 | batch 0022 / 0050 | loss 0.4964\n",
            "Train: epoch 0001 / 0100 | batch 0023 / 0050 | loss 0.4911\n",
            "Train: epoch 0001 / 0100 | batch 0024 / 0050 | loss 0.4863\n",
            "Train: epoch 0001 / 0100 | batch 0025 / 0050 | loss 0.4821\n",
            "Train: epoch 0001 / 0100 | batch 0026 / 0050 | loss 0.4773\n",
            "Train: epoch 0001 / 0100 | batch 0027 / 0050 | loss 0.4728\n",
            "Train: epoch 0001 / 0100 | batch 0028 / 0050 | loss 0.4683\n",
            "Train: epoch 0001 / 0100 | batch 0029 / 0050 | loss 0.4636\n",
            "Train: epoch 0001 / 0100 | batch 0030 / 0050 | loss 0.4591\n",
            "Train: epoch 0001 / 0100 | batch 0031 / 0050 | loss 0.4547\n",
            "Train: epoch 0001 / 0100 | batch 0032 / 0050 | loss 0.4504\n",
            "Train: epoch 0001 / 0100 | batch 0033 / 0050 | loss 0.4461\n",
            "Train: epoch 0001 / 0100 | batch 0034 / 0050 | loss 0.4420\n",
            "Train: epoch 0001 / 0100 | batch 0035 / 0050 | loss 0.4380\n",
            "Train: epoch 0001 / 0100 | batch 0036 / 0050 | loss 0.4343\n",
            "Train: epoch 0001 / 0100 | batch 0037 / 0050 | loss 0.4303\n",
            "Train: epoch 0001 / 0100 | batch 0038 / 0050 | loss 0.4265\n",
            "Train: epoch 0001 / 0100 | batch 0039 / 0050 | loss 0.4227\n",
            "Train: epoch 0001 / 0100 | batch 0040 / 0050 | loss 0.4191\n",
            "Train: epoch 0001 / 0100 | batch 0041 / 0050 | loss 0.4153\n",
            "Train: epoch 0001 / 0100 | batch 0042 / 0050 | loss 0.4117\n",
            "Train: epoch 0001 / 0100 | batch 0043 / 0050 | loss 0.4080\n",
            "Train: epoch 0001 / 0100 | batch 0044 / 0050 | loss 0.4046\n",
            "Train: epoch 0001 / 0100 | batch 0045 / 0050 | loss 0.4012\n",
            "Train: epoch 0001 / 0100 | batch 0046 / 0050 | loss 0.3978\n",
            "Train: epoch 0001 / 0100 | batch 0047 / 0050 | loss 0.3945\n",
            "Train: epoch 0001 / 0100 | batch 0048 / 0050 | loss 0.3911\n",
            "Train: epoch 0001 / 0100 | batch 0049 / 0050 | loss 0.3879\n",
            "Val loss 0.2417\n",
            "Dice score : 0.006738502997905016\n",
            "Val loss 0.2448\n",
            "Dice score : 0.00047332042595371604\n",
            "Val loss 0.2456\n",
            "Dice score : 0.04894097149372101\n",
            "Val loss 0.2424\n",
            "Dice score : 0.0385219007730484\n",
            "Val loss 0.2423\n",
            "Dice score : 0.01751265674829483\n",
            "Val loss 0.2435\n",
            "Dice score : 0.05193041265010834\n",
            "Val loss 0.2423\n",
            "Dice score : 0.0005938202375546098\n",
            "Val loss 0.2427\n",
            "Dice score : 0.032675568014383316\n",
            "Val loss 0.2426\n",
            "Dice score : 0.010816001333296299\n",
            "Val loss 0.2430\n",
            "Dice score : 0.0122770881280303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 1/100 [01:24<2:18:40, 84.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0002 / 0100 | batch 0000 / 0050 | loss 0.2302\n",
            "Train: epoch 0002 / 0100 | batch 0001 / 0050 | loss 0.2211\n",
            "Train: epoch 0002 / 0100 | batch 0002 / 0050 | loss 0.2223\n",
            "Train: epoch 0002 / 0100 | batch 0003 / 0050 | loss 0.2211\n",
            "Train: epoch 0002 / 0100 | batch 0004 / 0050 | loss 0.2175\n",
            "Train: epoch 0002 / 0100 | batch 0005 / 0050 | loss 0.2159\n",
            "Train: epoch 0002 / 0100 | batch 0006 / 0050 | loss 0.2131\n",
            "Train: epoch 0002 / 0100 | batch 0007 / 0050 | loss 0.2104\n",
            "Train: epoch 0002 / 0100 | batch 0008 / 0050 | loss 0.2084\n",
            "Train: epoch 0002 / 0100 | batch 0009 / 0050 | loss 0.2072\n",
            "Train: epoch 0002 / 0100 | batch 0010 / 0050 | loss 0.2052\n",
            "Train: epoch 0002 / 0100 | batch 0011 / 0050 | loss 0.2036\n",
            "Train: epoch 0002 / 0100 | batch 0012 / 0050 | loss 0.2016\n",
            "Train: epoch 0002 / 0100 | batch 0013 / 0050 | loss 0.2001\n",
            "Train: epoch 0002 / 0100 | batch 0014 / 0050 | loss 0.1990\n",
            "Train: epoch 0002 / 0100 | batch 0015 / 0050 | loss 0.1981\n",
            "Train: epoch 0002 / 0100 | batch 0016 / 0050 | loss 0.1968\n",
            "Train: epoch 0002 / 0100 | batch 0017 / 0050 | loss 0.1950\n",
            "Train: epoch 0002 / 0100 | batch 0018 / 0050 | loss 0.1936\n",
            "Train: epoch 0002 / 0100 | batch 0019 / 0050 | loss 0.1916\n",
            "Train: epoch 0002 / 0100 | batch 0020 / 0050 | loss 0.1901\n",
            "Train: epoch 0002 / 0100 | batch 0021 / 0050 | loss 0.1884\n",
            "Train: epoch 0002 / 0100 | batch 0022 / 0050 | loss 0.1871\n",
            "Train: epoch 0002 / 0100 | batch 0023 / 0050 | loss 0.1860\n",
            "Train: epoch 0002 / 0100 | batch 0024 / 0050 | loss 0.1844\n",
            "Train: epoch 0002 / 0100 | batch 0025 / 0050 | loss 0.1834\n",
            "Train: epoch 0002 / 0100 | batch 0026 / 0050 | loss 0.1819\n",
            "Train: epoch 0002 / 0100 | batch 0027 / 0050 | loss 0.1805\n",
            "Train: epoch 0002 / 0100 | batch 0028 / 0050 | loss 0.1790\n",
            "Train: epoch 0002 / 0100 | batch 0029 / 0050 | loss 0.1774\n",
            "Train: epoch 0002 / 0100 | batch 0030 / 0050 | loss 0.1765\n",
            "Train: epoch 0002 / 0100 | batch 0031 / 0050 | loss 0.1750\n",
            "Train: epoch 0002 / 0100 | batch 0032 / 0050 | loss 0.1736\n",
            "Train: epoch 0002 / 0100 | batch 0033 / 0050 | loss 0.1724\n",
            "Train: epoch 0002 / 0100 | batch 0034 / 0050 | loss 0.1712\n",
            "Train: epoch 0002 / 0100 | batch 0035 / 0050 | loss 0.1701\n",
            "Train: epoch 0002 / 0100 | batch 0036 / 0050 | loss 0.1687\n",
            "Train: epoch 0002 / 0100 | batch 0037 / 0050 | loss 0.1677\n",
            "Train: epoch 0002 / 0100 | batch 0038 / 0050 | loss 0.1665\n",
            "Train: epoch 0002 / 0100 | batch 0039 / 0050 | loss 0.1651\n",
            "Train: epoch 0002 / 0100 | batch 0040 / 0050 | loss 0.1639\n",
            "Train: epoch 0002 / 0100 | batch 0041 / 0050 | loss 0.1632\n",
            "Train: epoch 0002 / 0100 | batch 0042 / 0050 | loss 0.1621\n",
            "Train: epoch 0002 / 0100 | batch 0043 / 0050 | loss 0.1611\n",
            "Train: epoch 0002 / 0100 | batch 0044 / 0050 | loss 0.1600\n",
            "Train: epoch 0002 / 0100 | batch 0045 / 0050 | loss 0.1593\n",
            "Train: epoch 0002 / 0100 | batch 0046 / 0050 | loss 0.1583\n",
            "Train: epoch 0002 / 0100 | batch 0047 / 0050 | loss 0.1574\n",
            "Train: epoch 0002 / 0100 | batch 0048 / 0050 | loss 0.1563\n",
            "Train: epoch 0002 / 0100 | batch 0049 / 0050 | loss 0.1555\n",
            "Val loss 0.1255\n",
            "Dice score : 0.03597349673509598\n",
            "Val loss 0.1220\n",
            "Dice score : 0.027423273772001266\n",
            "Val loss 0.1245\n",
            "Dice score : 0.013349278829991817\n",
            "Val loss 0.1237\n",
            "Dice score : 0.033661384135484695\n",
            "Val loss 0.1241\n",
            "Dice score : 0.014057579450309277\n",
            "Val loss 0.1219\n",
            "Dice score : 0.03047291748225689\n",
            "Val loss 0.1245\n",
            "Dice score : 0.01397989597171545\n",
            "Val loss 0.1241\n",
            "Dice score : 0.03282500058412552\n",
            "Val loss 0.1234\n",
            "Dice score : 0.05767519772052765\n",
            "Val loss 0.1229\n",
            "Dice score : 0.05508570373058319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/100 [02:37<2:07:28, 78.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0003 / 0100 | batch 0000 / 0050 | loss 0.1062\n",
            "Train: epoch 0003 / 0100 | batch 0001 / 0050 | loss 0.1047\n",
            "Train: epoch 0003 / 0100 | batch 0002 / 0050 | loss 0.1016\n",
            "Train: epoch 0003 / 0100 | batch 0003 / 0050 | loss 0.1002\n",
            "Train: epoch 0003 / 0100 | batch 0004 / 0050 | loss 0.1001\n",
            "Train: epoch 0003 / 0100 | batch 0005 / 0050 | loss 0.0991\n",
            "Train: epoch 0003 / 0100 | batch 0006 / 0050 | loss 0.0986\n",
            "Train: epoch 0003 / 0100 | batch 0007 / 0050 | loss 0.0994\n",
            "Train: epoch 0003 / 0100 | batch 0008 / 0050 | loss 0.0981\n",
            "Train: epoch 0003 / 0100 | batch 0009 / 0050 | loss 0.0972\n",
            "Train: epoch 0003 / 0100 | batch 0010 / 0050 | loss 0.0967\n",
            "Train: epoch 0003 / 0100 | batch 0011 / 0050 | loss 0.0967\n",
            "Train: epoch 0003 / 0100 | batch 0012 / 0050 | loss 0.0967\n",
            "Train: epoch 0003 / 0100 | batch 0013 / 0050 | loss 0.0965\n",
            "Train: epoch 0003 / 0100 | batch 0014 / 0050 | loss 0.0963\n",
            "Train: epoch 0003 / 0100 | batch 0015 / 0050 | loss 0.0954\n",
            "Train: epoch 0003 / 0100 | batch 0016 / 0050 | loss 0.0949\n",
            "Train: epoch 0003 / 0100 | batch 0017 / 0050 | loss 0.0949\n",
            "Train: epoch 0003 / 0100 | batch 0018 / 0050 | loss 0.0953\n",
            "Train: epoch 0003 / 0100 | batch 0019 / 0050 | loss 0.0945\n",
            "Train: epoch 0003 / 0100 | batch 0020 / 0050 | loss 0.0938\n",
            "Train: epoch 0003 / 0100 | batch 0021 / 0050 | loss 0.0933\n",
            "Train: epoch 0003 / 0100 | batch 0022 / 0050 | loss 0.0928\n",
            "Train: epoch 0003 / 0100 | batch 0023 / 0050 | loss 0.0943\n",
            "Train: epoch 0003 / 0100 | batch 0024 / 0050 | loss 0.0935\n",
            "Train: epoch 0003 / 0100 | batch 0025 / 0050 | loss 0.0934\n",
            "Train: epoch 0003 / 0100 | batch 0026 / 0050 | loss 0.0934\n",
            "Train: epoch 0003 / 0100 | batch 0027 / 0050 | loss 0.0934\n",
            "Train: epoch 0003 / 0100 | batch 0028 / 0050 | loss 0.0930\n",
            "Train: epoch 0003 / 0100 | batch 0029 / 0050 | loss 0.0924\n",
            "Train: epoch 0003 / 0100 | batch 0030 / 0050 | loss 0.0926\n",
            "Train: epoch 0003 / 0100 | batch 0031 / 0050 | loss 0.0924\n",
            "Train: epoch 0003 / 0100 | batch 0032 / 0050 | loss 0.0918\n",
            "Train: epoch 0003 / 0100 | batch 0033 / 0050 | loss 0.0920\n",
            "Train: epoch 0003 / 0100 | batch 0034 / 0050 | loss 0.0915\n",
            "Train: epoch 0003 / 0100 | batch 0035 / 0050 | loss 0.0910\n",
            "Train: epoch 0003 / 0100 | batch 0036 / 0050 | loss 0.0905\n",
            "Train: epoch 0003 / 0100 | batch 0037 / 0050 | loss 0.0903\n",
            "Train: epoch 0003 / 0100 | batch 0038 / 0050 | loss 0.0902\n",
            "Train: epoch 0003 / 0100 | batch 0039 / 0050 | loss 0.0898\n",
            "Train: epoch 0003 / 0100 | batch 0040 / 0050 | loss 0.0897\n",
            "Train: epoch 0003 / 0100 | batch 0041 / 0050 | loss 0.0896\n",
            "Train: epoch 0003 / 0100 | batch 0042 / 0050 | loss 0.0896\n",
            "Train: epoch 0003 / 0100 | batch 0043 / 0050 | loss 0.0892\n",
            "Train: epoch 0003 / 0100 | batch 0044 / 0050 | loss 0.0887\n",
            "Train: epoch 0003 / 0100 | batch 0045 / 0050 | loss 0.0887\n",
            "Train: epoch 0003 / 0100 | batch 0046 / 0050 | loss 0.0889\n",
            "Train: epoch 0003 / 0100 | batch 0047 / 0050 | loss 0.0887\n",
            "Train: epoch 0003 / 0100 | batch 0048 / 0050 | loss 0.0888\n",
            "Train: epoch 0003 / 0100 | batch 0049 / 0050 | loss 0.0883\n",
            "Val loss 0.0878\n",
            "Dice score : 0.0425383597612381\n",
            "Val loss 0.0802\n",
            "Dice score : 0.03906210511922836\n",
            "Val loss 0.0777\n",
            "Dice score : 0.03108770027756691\n",
            "Val loss 0.0794\n",
            "Dice score : 0.035476699471473694\n",
            "Val loss 0.0772\n",
            "Dice score : 0.0198256466537714\n",
            "Val loss 0.0774\n",
            "Dice score : 0.0471799299120903\n",
            "Val loss 0.0798\n",
            "Dice score : 0.019769908860325813\n",
            "Val loss 0.0804\n",
            "Dice score : 0.050619613379240036\n",
            "Val loss 0.0789\n",
            "Dice score : 0.02481367066502571\n",
            "Val loss 0.0802\n",
            "Dice score : 0.03968697041273117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 3/100 [03:49<2:01:26, 75.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0004 / 0100 | batch 0000 / 0050 | loss 0.0824\n",
            "Train: epoch 0004 / 0100 | batch 0001 / 0050 | loss 0.0779\n",
            "Train: epoch 0004 / 0100 | batch 0002 / 0050 | loss 0.0821\n",
            "Train: epoch 0004 / 0100 | batch 0003 / 0050 | loss 0.0797\n",
            "Train: epoch 0004 / 0100 | batch 0004 / 0050 | loss 0.0768\n",
            "Train: epoch 0004 / 0100 | batch 0005 / 0050 | loss 0.0763\n",
            "Train: epoch 0004 / 0100 | batch 0006 / 0050 | loss 0.0754\n",
            "Train: epoch 0004 / 0100 | batch 0007 / 0050 | loss 0.0753\n",
            "Train: epoch 0004 / 0100 | batch 0008 / 0050 | loss 0.0745\n",
            "Train: epoch 0004 / 0100 | batch 0009 / 0050 | loss 0.0742\n",
            "Train: epoch 0004 / 0100 | batch 0010 / 0050 | loss 0.0735\n",
            "Train: epoch 0004 / 0100 | batch 0011 / 0050 | loss 0.0730\n",
            "Train: epoch 0004 / 0100 | batch 0012 / 0050 | loss 0.0733\n",
            "Train: epoch 0004 / 0100 | batch 0013 / 0050 | loss 0.0726\n",
            "Train: epoch 0004 / 0100 | batch 0014 / 0050 | loss 0.0726\n",
            "Train: epoch 0004 / 0100 | batch 0015 / 0050 | loss 0.0741\n",
            "Train: epoch 0004 / 0100 | batch 0016 / 0050 | loss 0.0738\n",
            "Train: epoch 0004 / 0100 | batch 0017 / 0050 | loss 0.0738\n",
            "Train: epoch 0004 / 0100 | batch 0018 / 0050 | loss 0.0733\n",
            "Train: epoch 0004 / 0100 | batch 0019 / 0050 | loss 0.0728\n",
            "Train: epoch 0004 / 0100 | batch 0020 / 0050 | loss 0.0726\n",
            "Train: epoch 0004 / 0100 | batch 0021 / 0050 | loss 0.0725\n",
            "Train: epoch 0004 / 0100 | batch 0022 / 0050 | loss 0.0721\n",
            "Train: epoch 0004 / 0100 | batch 0023 / 0050 | loss 0.0717\n",
            "Train: epoch 0004 / 0100 | batch 0024 / 0050 | loss 0.0721\n",
            "Train: epoch 0004 / 0100 | batch 0025 / 0050 | loss 0.0719\n",
            "Train: epoch 0004 / 0100 | batch 0026 / 0050 | loss 0.0715\n",
            "Train: epoch 0004 / 0100 | batch 0027 / 0050 | loss 0.0717\n",
            "Train: epoch 0004 / 0100 | batch 0028 / 0050 | loss 0.0715\n",
            "Train: epoch 0004 / 0100 | batch 0029 / 0050 | loss 0.0710\n",
            "Train: epoch 0004 / 0100 | batch 0030 / 0050 | loss 0.0711\n",
            "Train: epoch 0004 / 0100 | batch 0031 / 0050 | loss 0.0707\n",
            "Train: epoch 0004 / 0100 | batch 0032 / 0050 | loss 0.0705\n",
            "Train: epoch 0004 / 0100 | batch 0033 / 0050 | loss 0.0707\n",
            "Train: epoch 0004 / 0100 | batch 0034 / 0050 | loss 0.0708\n",
            "Train: epoch 0004 / 0100 | batch 0035 / 0050 | loss 0.0714\n",
            "Train: epoch 0004 / 0100 | batch 0036 / 0050 | loss 0.0715\n",
            "Train: epoch 0004 / 0100 | batch 0037 / 0050 | loss 0.0712\n",
            "Train: epoch 0004 / 0100 | batch 0038 / 0050 | loss 0.0711\n",
            "Train: epoch 0004 / 0100 | batch 0039 / 0050 | loss 0.0711\n",
            "Train: epoch 0004 / 0100 | batch 0040 / 0050 | loss 0.0708\n",
            "Train: epoch 0004 / 0100 | batch 0041 / 0050 | loss 0.0707\n",
            "Train: epoch 0004 / 0100 | batch 0042 / 0050 | loss 0.0706\n",
            "Train: epoch 0004 / 0100 | batch 0043 / 0050 | loss 0.0705\n",
            "Train: epoch 0004 / 0100 | batch 0044 / 0050 | loss 0.0707\n",
            "Train: epoch 0004 / 0100 | batch 0045 / 0050 | loss 0.0703\n",
            "Train: epoch 0004 / 0100 | batch 0046 / 0050 | loss 0.0699\n",
            "Train: epoch 0004 / 0100 | batch 0047 / 0050 | loss 0.0698\n",
            "Train: epoch 0004 / 0100 | batch 0048 / 0050 | loss 0.0697\n",
            "Train: epoch 0004 / 0100 | batch 0049 / 0050 | loss 0.0696\n",
            "Val loss 0.0820\n",
            "Dice score : 0.05091672018170357\n",
            "Val loss 0.0697\n",
            "Dice score : 0.040121253579854965\n",
            "Val loss 0.0663\n",
            "Dice score : 0.04641236364841461\n",
            "Val loss 0.0620\n",
            "Dice score : 0.019060390070080757\n",
            "Val loss 0.0651\n",
            "Dice score : 0.039857424795627594\n",
            "Val loss 0.0688\n",
            "Dice score : 0.05301564559340477\n",
            "Val loss 0.0685\n",
            "Dice score : 0.056030016392469406\n",
            "Val loss 0.0668\n",
            "Dice score : 0.04143797606229782\n",
            "Val loss 0.0687\n",
            "Dice score : 0.015885651111602783\n",
            "Val loss 0.0674\n",
            "Dice score : 0.02570609748363495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 4/100 [05:02<1:58:49, 74.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0005 / 0100 | batch 0000 / 0050 | loss 0.0600\n",
            "Train: epoch 0005 / 0100 | batch 0001 / 0050 | loss 0.0667\n",
            "Train: epoch 0005 / 0100 | batch 0002 / 0050 | loss 0.0684\n",
            "Train: epoch 0005 / 0100 | batch 0003 / 0050 | loss 0.0707\n",
            "Train: epoch 0005 / 0100 | batch 0004 / 0050 | loss 0.0671\n",
            "Train: epoch 0005 / 0100 | batch 0005 / 0050 | loss 0.0688\n",
            "Train: epoch 0005 / 0100 | batch 0006 / 0050 | loss 0.0681\n",
            "Train: epoch 0005 / 0100 | batch 0007 / 0050 | loss 0.0701\n",
            "Train: epoch 0005 / 0100 | batch 0008 / 0050 | loss 0.0689\n",
            "Train: epoch 0005 / 0100 | batch 0009 / 0050 | loss 0.0670\n",
            "Train: epoch 0005 / 0100 | batch 0010 / 0050 | loss 0.0662\n",
            "Train: epoch 0005 / 0100 | batch 0011 / 0050 | loss 0.0656\n",
            "Train: epoch 0005 / 0100 | batch 0012 / 0050 | loss 0.0657\n",
            "Train: epoch 0005 / 0100 | batch 0013 / 0050 | loss 0.0657\n",
            "Train: epoch 0005 / 0100 | batch 0014 / 0050 | loss 0.0659\n",
            "Train: epoch 0005 / 0100 | batch 0015 / 0050 | loss 0.0665\n",
            "Train: epoch 0005 / 0100 | batch 0016 / 0050 | loss 0.0665\n",
            "Train: epoch 0005 / 0100 | batch 0017 / 0050 | loss 0.0659\n",
            "Train: epoch 0005 / 0100 | batch 0018 / 0050 | loss 0.0661\n",
            "Train: epoch 0005 / 0100 | batch 0019 / 0050 | loss 0.0655\n",
            "Train: epoch 0005 / 0100 | batch 0020 / 0050 | loss 0.0653\n",
            "Train: epoch 0005 / 0100 | batch 0021 / 0050 | loss 0.0649\n",
            "Train: epoch 0005 / 0100 | batch 0022 / 0050 | loss 0.0644\n",
            "Train: epoch 0005 / 0100 | batch 0023 / 0050 | loss 0.0641\n",
            "Train: epoch 0005 / 0100 | batch 0024 / 0050 | loss 0.0650\n",
            "Train: epoch 0005 / 0100 | batch 0025 / 0050 | loss 0.0650\n",
            "Train: epoch 0005 / 0100 | batch 0026 / 0050 | loss 0.0646\n",
            "Train: epoch 0005 / 0100 | batch 0027 / 0050 | loss 0.0642\n",
            "Train: epoch 0005 / 0100 | batch 0028 / 0050 | loss 0.0643\n",
            "Train: epoch 0005 / 0100 | batch 0029 / 0050 | loss 0.0639\n",
            "Train: epoch 0005 / 0100 | batch 0030 / 0050 | loss 0.0637\n",
            "Train: epoch 0005 / 0100 | batch 0031 / 0050 | loss 0.0639\n",
            "Train: epoch 0005 / 0100 | batch 0032 / 0050 | loss 0.0639\n",
            "Train: epoch 0005 / 0100 | batch 0033 / 0050 | loss 0.0640\n",
            "Train: epoch 0005 / 0100 | batch 0034 / 0050 | loss 0.0637\n",
            "Train: epoch 0005 / 0100 | batch 0035 / 0050 | loss 0.0632\n",
            "Train: epoch 0005 / 0100 | batch 0036 / 0050 | loss 0.0633\n",
            "Train: epoch 0005 / 0100 | batch 0037 / 0050 | loss 0.0630\n",
            "Train: epoch 0005 / 0100 | batch 0038 / 0050 | loss 0.0629\n",
            "Train: epoch 0005 / 0100 | batch 0039 / 0050 | loss 0.0628\n",
            "Train: epoch 0005 / 0100 | batch 0040 / 0050 | loss 0.0630\n",
            "Train: epoch 0005 / 0100 | batch 0041 / 0050 | loss 0.0630\n",
            "Train: epoch 0005 / 0100 | batch 0042 / 0050 | loss 0.0627\n",
            "Train: epoch 0005 / 0100 | batch 0043 / 0050 | loss 0.0624\n",
            "Train: epoch 0005 / 0100 | batch 0044 / 0050 | loss 0.0630\n",
            "Train: epoch 0005 / 0100 | batch 0045 / 0050 | loss 0.0630\n",
            "Train: epoch 0005 / 0100 | batch 0046 / 0050 | loss 0.0629\n",
            "Train: epoch 0005 / 0100 | batch 0047 / 0050 | loss 0.0628\n",
            "Train: epoch 0005 / 0100 | batch 0048 / 0050 | loss 0.0626\n",
            "Train: epoch 0005 / 0100 | batch 0049 / 0050 | loss 0.0626\n",
            "Val loss 0.0637\n",
            "Dice score : 0.016776137053966522\n",
            "Val loss 0.0629\n",
            "Dice score : 0.023151017725467682\n",
            "Val loss 0.0600\n",
            "Dice score : 0.03616395220160484\n",
            "Val loss 0.0591\n",
            "Dice score : 0.036899566650390625\n",
            "Val loss 0.0608\n",
            "Dice score : 0.04622913897037506\n",
            "Val loss 0.0591\n",
            "Dice score : 0.03732064738869667\n",
            "Val loss 0.0616\n",
            "Dice score : 0.06973807513713837\n",
            "Val loss 0.0625\n",
            "Dice score : 0.026534846052527428\n",
            "Val loss 0.0643\n",
            "Dice score : 0.052457939833402634\n",
            "Val loss 0.0626\n",
            "Dice score : 0.03714931383728981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 5/100 [06:15<1:56:54, 73.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0006 / 0100 | batch 0000 / 0050 | loss 0.0886\n",
            "Train: epoch 0006 / 0100 | batch 0001 / 0050 | loss 0.0691\n",
            "Train: epoch 0006 / 0100 | batch 0002 / 0050 | loss 0.0617\n",
            "Train: epoch 0006 / 0100 | batch 0003 / 0050 | loss 0.0645\n",
            "Train: epoch 0006 / 0100 | batch 0004 / 0050 | loss 0.0645\n",
            "Train: epoch 0006 / 0100 | batch 0005 / 0050 | loss 0.0626\n",
            "Train: epoch 0006 / 0100 | batch 0006 / 0050 | loss 0.0609\n",
            "Train: epoch 0006 / 0100 | batch 0007 / 0050 | loss 0.0594\n",
            "Train: epoch 0006 / 0100 | batch 0008 / 0050 | loss 0.0589\n",
            "Train: epoch 0006 / 0100 | batch 0009 / 0050 | loss 0.0597\n",
            "Train: epoch 0006 / 0100 | batch 0010 / 0050 | loss 0.0602\n",
            "Train: epoch 0006 / 0100 | batch 0011 / 0050 | loss 0.0601\n",
            "Train: epoch 0006 / 0100 | batch 0012 / 0050 | loss 0.0595\n",
            "Train: epoch 0006 / 0100 | batch 0013 / 0050 | loss 0.0592\n",
            "Train: epoch 0006 / 0100 | batch 0014 / 0050 | loss 0.0591\n",
            "Train: epoch 0006 / 0100 | batch 0015 / 0050 | loss 0.0604\n",
            "Train: epoch 0006 / 0100 | batch 0016 / 0050 | loss 0.0610\n",
            "Train: epoch 0006 / 0100 | batch 0017 / 0050 | loss 0.0610\n",
            "Train: epoch 0006 / 0100 | batch 0018 / 0050 | loss 0.0608\n",
            "Train: epoch 0006 / 0100 | batch 0019 / 0050 | loss 0.0616\n",
            "Train: epoch 0006 / 0100 | batch 0020 / 0050 | loss 0.0636\n",
            "Train: epoch 0006 / 0100 | batch 0021 / 0050 | loss 0.0641\n",
            "Train: epoch 0006 / 0100 | batch 0022 / 0050 | loss 0.0637\n",
            "Train: epoch 0006 / 0100 | batch 0023 / 0050 | loss 0.0643\n",
            "Train: epoch 0006 / 0100 | batch 0024 / 0050 | loss 0.0638\n",
            "Train: epoch 0006 / 0100 | batch 0025 / 0050 | loss 0.0635\n",
            "Train: epoch 0006 / 0100 | batch 0026 / 0050 | loss 0.0631\n",
            "Train: epoch 0006 / 0100 | batch 0027 / 0050 | loss 0.0633\n",
            "Train: epoch 0006 / 0100 | batch 0028 / 0050 | loss 0.0632\n",
            "Train: epoch 0006 / 0100 | batch 0029 / 0050 | loss 0.0626\n",
            "Train: epoch 0006 / 0100 | batch 0030 / 0050 | loss 0.0620\n",
            "Train: epoch 0006 / 0100 | batch 0031 / 0050 | loss 0.0619\n",
            "Train: epoch 0006 / 0100 | batch 0032 / 0050 | loss 0.0619\n",
            "Train: epoch 0006 / 0100 | batch 0033 / 0050 | loss 0.0614\n",
            "Train: epoch 0006 / 0100 | batch 0034 / 0050 | loss 0.0611\n",
            "Train: epoch 0006 / 0100 | batch 0035 / 0050 | loss 0.0611\n",
            "Train: epoch 0006 / 0100 | batch 0036 / 0050 | loss 0.0609\n",
            "Train: epoch 0006 / 0100 | batch 0037 / 0050 | loss 0.0608\n",
            "Train: epoch 0006 / 0100 | batch 0038 / 0050 | loss 0.0604\n",
            "Train: epoch 0006 / 0100 | batch 0039 / 0050 | loss 0.0606\n",
            "Train: epoch 0006 / 0100 | batch 0040 / 0050 | loss 0.0606\n",
            "Train: epoch 0006 / 0100 | batch 0041 / 0050 | loss 0.0607\n",
            "Train: epoch 0006 / 0100 | batch 0042 / 0050 | loss 0.0609\n",
            "Train: epoch 0006 / 0100 | batch 0043 / 0050 | loss 0.0605\n",
            "Train: epoch 0006 / 0100 | batch 0044 / 0050 | loss 0.0602\n",
            "Train: epoch 0006 / 0100 | batch 0045 / 0050 | loss 0.0605\n",
            "Train: epoch 0006 / 0100 | batch 0046 / 0050 | loss 0.0605\n",
            "Train: epoch 0006 / 0100 | batch 0047 / 0050 | loss 0.0606\n",
            "Train: epoch 0006 / 0100 | batch 0048 / 0050 | loss 0.0603\n",
            "Train: epoch 0006 / 0100 | batch 0049 / 0050 | loss 0.0601\n",
            "Val loss 0.0778\n",
            "Dice score : 0.050540994852781296\n",
            "Val loss 0.0628\n",
            "Dice score : 0.021970022469758987\n",
            "Val loss 0.0578\n",
            "Dice score : 0.03821629658341408\n",
            "Val loss 0.0562\n",
            "Dice score : 0.039210475981235504\n",
            "Val loss 0.0575\n",
            "Dice score : 0.027799125760793686\n",
            "Val loss 0.0588\n",
            "Dice score : 0.042353786528110504\n",
            "Val loss 0.0614\n",
            "Dice score : 0.028266271576285362\n",
            "Val loss 0.0605\n",
            "Dice score : 0.04263544827699661\n",
            "Val loss 0.0621\n",
            "Dice score : 0.053224533796310425\n",
            "Val loss 0.0610\n",
            "Dice score : 0.04463791847229004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 6/100 [07:28<1:54:58, 73.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0007 / 0100 | batch 0000 / 0050 | loss 0.0517\n",
            "Train: epoch 0007 / 0100 | batch 0001 / 0050 | loss 0.0502\n",
            "Train: epoch 0007 / 0100 | batch 0002 / 0050 | loss 0.0502\n",
            "Train: epoch 0007 / 0100 | batch 0003 / 0050 | loss 0.0595\n",
            "Train: epoch 0007 / 0100 | batch 0004 / 0050 | loss 0.0562\n",
            "Train: epoch 0007 / 0100 | batch 0005 / 0050 | loss 0.0567\n",
            "Train: epoch 0007 / 0100 | batch 0006 / 0050 | loss 0.0568\n",
            "Train: epoch 0007 / 0100 | batch 0007 / 0050 | loss 0.0587\n",
            "Train: epoch 0007 / 0100 | batch 0008 / 0050 | loss 0.0578\n",
            "Train: epoch 0007 / 0100 | batch 0009 / 0050 | loss 0.0574\n",
            "Train: epoch 0007 / 0100 | batch 0010 / 0050 | loss 0.0568\n",
            "Train: epoch 0007 / 0100 | batch 0011 / 0050 | loss 0.0564\n",
            "Train: epoch 0007 / 0100 | batch 0012 / 0050 | loss 0.0555\n",
            "Train: epoch 0007 / 0100 | batch 0013 / 0050 | loss 0.0557\n",
            "Train: epoch 0007 / 0100 | batch 0014 / 0050 | loss 0.0548\n",
            "Train: epoch 0007 / 0100 | batch 0015 / 0050 | loss 0.0560\n",
            "Train: epoch 0007 / 0100 | batch 0016 / 0050 | loss 0.0557\n",
            "Train: epoch 0007 / 0100 | batch 0017 / 0050 | loss 0.0557\n",
            "Train: epoch 0007 / 0100 | batch 0018 / 0050 | loss 0.0562\n",
            "Train: epoch 0007 / 0100 | batch 0019 / 0050 | loss 0.0581\n",
            "Train: epoch 0007 / 0100 | batch 0020 / 0050 | loss 0.0586\n",
            "Train: epoch 0007 / 0100 | batch 0021 / 0050 | loss 0.0584\n",
            "Train: epoch 0007 / 0100 | batch 0022 / 0050 | loss 0.0582\n",
            "Train: epoch 0007 / 0100 | batch 0023 / 0050 | loss 0.0585\n",
            "Train: epoch 0007 / 0100 | batch 0024 / 0050 | loss 0.0586\n",
            "Train: epoch 0007 / 0100 | batch 0025 / 0050 | loss 0.0585\n",
            "Train: epoch 0007 / 0100 | batch 0026 / 0050 | loss 0.0587\n",
            "Train: epoch 0007 / 0100 | batch 0027 / 0050 | loss 0.0584\n",
            "Train: epoch 0007 / 0100 | batch 0028 / 0050 | loss 0.0581\n",
            "Train: epoch 0007 / 0100 | batch 0029 / 0050 | loss 0.0583\n",
            "Train: epoch 0007 / 0100 | batch 0030 / 0050 | loss 0.0586\n",
            "Train: epoch 0007 / 0100 | batch 0031 / 0050 | loss 0.0585\n",
            "Train: epoch 0007 / 0100 | batch 0032 / 0050 | loss 0.0591\n",
            "Train: epoch 0007 / 0100 | batch 0033 / 0050 | loss 0.0591\n",
            "Train: epoch 0007 / 0100 | batch 0034 / 0050 | loss 0.0592\n",
            "Train: epoch 0007 / 0100 | batch 0035 / 0050 | loss 0.0588\n",
            "Train: epoch 0007 / 0100 | batch 0036 / 0050 | loss 0.0587\n",
            "Train: epoch 0007 / 0100 | batch 0037 / 0050 | loss 0.0585\n",
            "Train: epoch 0007 / 0100 | batch 0038 / 0050 | loss 0.0584\n",
            "Train: epoch 0007 / 0100 | batch 0039 / 0050 | loss 0.0590\n",
            "Train: epoch 0007 / 0100 | batch 0040 / 0050 | loss 0.0586\n",
            "Train: epoch 0007 / 0100 | batch 0041 / 0050 | loss 0.0582\n",
            "Train: epoch 0007 / 0100 | batch 0042 / 0050 | loss 0.0579\n",
            "Train: epoch 0007 / 0100 | batch 0043 / 0050 | loss 0.0581\n",
            "Train: epoch 0007 / 0100 | batch 0044 / 0050 | loss 0.0580\n",
            "Train: epoch 0007 / 0100 | batch 0045 / 0050 | loss 0.0576\n",
            "Train: epoch 0007 / 0100 | batch 0046 / 0050 | loss 0.0580\n",
            "Train: epoch 0007 / 0100 | batch 0047 / 0050 | loss 0.0583\n",
            "Train: epoch 0007 / 0100 | batch 0048 / 0050 | loss 0.0583\n",
            "Train: epoch 0007 / 0100 | batch 0049 / 0050 | loss 0.0582\n",
            "Val loss 0.0532\n",
            "Dice score : 0.04847113788127899\n",
            "Val loss 0.0507\n",
            "Dice score : 0.0225382037460804\n",
            "Val loss 0.0556\n",
            "Dice score : 0.07987821847200394\n",
            "Val loss 0.0603\n",
            "Dice score : 0.0457177609205246\n",
            "Val loss 0.0574\n",
            "Dice score : 0.045373138040304184\n",
            "Val loss 0.0634\n",
            "Dice score : 0.0593462735414505\n",
            "Val loss 0.0633\n",
            "Dice score : 0.06083394214510918\n",
            "Val loss 0.0612\n",
            "Dice score : 0.016501454636454582\n",
            "Val loss 0.0604\n",
            "Dice score : 0.05301382392644882\n",
            "Val loss 0.0597\n",
            "Dice score : 0.020557738840579987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 7/100 [08:40<1:53:22, 73.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0008 / 0100 | batch 0000 / 0050 | loss 0.0664\n",
            "Train: epoch 0008 / 0100 | batch 0001 / 0050 | loss 0.0595\n",
            "Train: epoch 0008 / 0100 | batch 0002 / 0050 | loss 0.0653\n",
            "Train: epoch 0008 / 0100 | batch 0003 / 0050 | loss 0.0630\n",
            "Train: epoch 0008 / 0100 | batch 0004 / 0050 | loss 0.0607\n",
            "Train: epoch 0008 / 0100 | batch 0005 / 0050 | loss 0.0580\n",
            "Train: epoch 0008 / 0100 | batch 0006 / 0050 | loss 0.0561\n",
            "Train: epoch 0008 / 0100 | batch 0007 / 0050 | loss 0.0554\n",
            "Train: epoch 0008 / 0100 | batch 0008 / 0050 | loss 0.0547\n",
            "Train: epoch 0008 / 0100 | batch 0009 / 0050 | loss 0.0547\n",
            "Train: epoch 0008 / 0100 | batch 0010 / 0050 | loss 0.0553\n",
            "Train: epoch 0008 / 0100 | batch 0011 / 0050 | loss 0.0558\n",
            "Train: epoch 0008 / 0100 | batch 0012 / 0050 | loss 0.0551\n",
            "Train: epoch 0008 / 0100 | batch 0013 / 0050 | loss 0.0566\n",
            "Train: epoch 0008 / 0100 | batch 0014 / 0050 | loss 0.0578\n",
            "Train: epoch 0008 / 0100 | batch 0015 / 0050 | loss 0.0569\n",
            "Train: epoch 0008 / 0100 | batch 0016 / 0050 | loss 0.0563\n",
            "Train: epoch 0008 / 0100 | batch 0017 / 0050 | loss 0.0566\n",
            "Train: epoch 0008 / 0100 | batch 0018 / 0050 | loss 0.0560\n",
            "Train: epoch 0008 / 0100 | batch 0019 / 0050 | loss 0.0557\n",
            "Train: epoch 0008 / 0100 | batch 0020 / 0050 | loss 0.0557\n",
            "Train: epoch 0008 / 0100 | batch 0021 / 0050 | loss 0.0562\n",
            "Train: epoch 0008 / 0100 | batch 0022 / 0050 | loss 0.0564\n",
            "Train: epoch 0008 / 0100 | batch 0023 / 0050 | loss 0.0571\n",
            "Train: epoch 0008 / 0100 | batch 0024 / 0050 | loss 0.0576\n",
            "Train: epoch 0008 / 0100 | batch 0025 / 0050 | loss 0.0572\n",
            "Train: epoch 0008 / 0100 | batch 0026 / 0050 | loss 0.0566\n",
            "Train: epoch 0008 / 0100 | batch 0027 / 0050 | loss 0.0570\n",
            "Train: epoch 0008 / 0100 | batch 0028 / 0050 | loss 0.0565\n",
            "Train: epoch 0008 / 0100 | batch 0029 / 0050 | loss 0.0566\n",
            "Train: epoch 0008 / 0100 | batch 0030 / 0050 | loss 0.0569\n",
            "Train: epoch 0008 / 0100 | batch 0031 / 0050 | loss 0.0570\n",
            "Train: epoch 0008 / 0100 | batch 0032 / 0050 | loss 0.0577\n",
            "Train: epoch 0008 / 0100 | batch 0033 / 0050 | loss 0.0578\n",
            "Train: epoch 0008 / 0100 | batch 0034 / 0050 | loss 0.0576\n",
            "Train: epoch 0008 / 0100 | batch 0035 / 0050 | loss 0.0575\n",
            "Train: epoch 0008 / 0100 | batch 0036 / 0050 | loss 0.0571\n",
            "Train: epoch 0008 / 0100 | batch 0037 / 0050 | loss 0.0572\n",
            "Train: epoch 0008 / 0100 | batch 0038 / 0050 | loss 0.0573\n",
            "Train: epoch 0008 / 0100 | batch 0039 / 0050 | loss 0.0570\n",
            "Train: epoch 0008 / 0100 | batch 0040 / 0050 | loss 0.0569\n",
            "Train: epoch 0008 / 0100 | batch 0041 / 0050 | loss 0.0564\n",
            "Train: epoch 0008 / 0100 | batch 0042 / 0050 | loss 0.0568\n",
            "Train: epoch 0008 / 0100 | batch 0043 / 0050 | loss 0.0569\n",
            "Train: epoch 0008 / 0100 | batch 0044 / 0050 | loss 0.0569\n",
            "Train: epoch 0008 / 0100 | batch 0045 / 0050 | loss 0.0577\n",
            "Train: epoch 0008 / 0100 | batch 0046 / 0050 | loss 0.0577\n",
            "Train: epoch 0008 / 0100 | batch 0047 / 0050 | loss 0.0575\n",
            "Train: epoch 0008 / 0100 | batch 0048 / 0050 | loss 0.0574\n",
            "Train: epoch 0008 / 0100 | batch 0049 / 0050 | loss 0.0575\n",
            "Val loss 0.0512\n",
            "Dice score : 0.059491608291864395\n",
            "Val loss 0.0583\n",
            "Dice score : 0.04717608168721199\n",
            "Val loss 0.0535\n",
            "Dice score : 0.024487661197781563\n",
            "Val loss 0.0537\n",
            "Dice score : 0.04822477698326111\n",
            "Val loss 0.0581\n",
            "Dice score : 0.045828625559806824\n",
            "Val loss 0.0560\n",
            "Dice score : 0.03851431608200073\n",
            "Val loss 0.0543\n",
            "Dice score : 0.031851381063461304\n",
            "Val loss 0.0555\n",
            "Dice score : 0.03626367822289467\n",
            "Val loss 0.0595\n",
            "Dice score : 0.0692003145813942\n",
            "Val loss 0.0595\n",
            "Dice score : 0.045564915984869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 8/100 [09:52<1:51:34, 72.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0009 / 0100 | batch 0000 / 0050 | loss 0.0440\n",
            "Train: epoch 0009 / 0100 | batch 0001 / 0050 | loss 0.0442\n",
            "Train: epoch 0009 / 0100 | batch 0002 / 0050 | loss 0.0437\n",
            "Train: epoch 0009 / 0100 | batch 0003 / 0050 | loss 0.0444\n",
            "Train: epoch 0009 / 0100 | batch 0004 / 0050 | loss 0.0455\n",
            "Train: epoch 0009 / 0100 | batch 0005 / 0050 | loss 0.0491\n",
            "Train: epoch 0009 / 0100 | batch 0006 / 0050 | loss 0.0519\n",
            "Train: epoch 0009 / 0100 | batch 0007 / 0050 | loss 0.0524\n",
            "Train: epoch 0009 / 0100 | batch 0008 / 0050 | loss 0.0512\n",
            "Train: epoch 0009 / 0100 | batch 0009 / 0050 | loss 0.0506\n",
            "Train: epoch 0009 / 0100 | batch 0010 / 0050 | loss 0.0518\n",
            "Train: epoch 0009 / 0100 | batch 0011 / 0050 | loss 0.0515\n",
            "Train: epoch 0009 / 0100 | batch 0012 / 0050 | loss 0.0525\n",
            "Train: epoch 0009 / 0100 | batch 0013 / 0050 | loss 0.0519\n",
            "Train: epoch 0009 / 0100 | batch 0014 / 0050 | loss 0.0511\n",
            "Train: epoch 0009 / 0100 | batch 0015 / 0050 | loss 0.0510\n",
            "Train: epoch 0009 / 0100 | batch 0016 / 0050 | loss 0.0511\n",
            "Train: epoch 0009 / 0100 | batch 0017 / 0050 | loss 0.0506\n",
            "Train: epoch 0009 / 0100 | batch 0018 / 0050 | loss 0.0512\n",
            "Train: epoch 0009 / 0100 | batch 0019 / 0050 | loss 0.0509\n",
            "Train: epoch 0009 / 0100 | batch 0020 / 0050 | loss 0.0508\n",
            "Train: epoch 0009 / 0100 | batch 0021 / 0050 | loss 0.0506\n",
            "Train: epoch 0009 / 0100 | batch 0022 / 0050 | loss 0.0517\n",
            "Train: epoch 0009 / 0100 | batch 0023 / 0050 | loss 0.0518\n",
            "Train: epoch 0009 / 0100 | batch 0024 / 0050 | loss 0.0522\n",
            "Train: epoch 0009 / 0100 | batch 0025 / 0050 | loss 0.0523\n",
            "Train: epoch 0009 / 0100 | batch 0026 / 0050 | loss 0.0524\n",
            "Train: epoch 0009 / 0100 | batch 0027 / 0050 | loss 0.0535\n",
            "Train: epoch 0009 / 0100 | batch 0028 / 0050 | loss 0.0534\n",
            "Train: epoch 0009 / 0100 | batch 0029 / 0050 | loss 0.0528\n",
            "Train: epoch 0009 / 0100 | batch 0030 / 0050 | loss 0.0542\n",
            "Train: epoch 0009 / 0100 | batch 0031 / 0050 | loss 0.0544\n",
            "Train: epoch 0009 / 0100 | batch 0032 / 0050 | loss 0.0543\n",
            "Train: epoch 0009 / 0100 | batch 0033 / 0050 | loss 0.0540\n",
            "Train: epoch 0009 / 0100 | batch 0034 / 0050 | loss 0.0545\n",
            "Train: epoch 0009 / 0100 | batch 0035 / 0050 | loss 0.0548\n",
            "Train: epoch 0009 / 0100 | batch 0036 / 0050 | loss 0.0553\n",
            "Train: epoch 0009 / 0100 | batch 0037 / 0050 | loss 0.0551\n",
            "Train: epoch 0009 / 0100 | batch 0038 / 0050 | loss 0.0555\n",
            "Train: epoch 0009 / 0100 | batch 0039 / 0050 | loss 0.0558\n",
            "Train: epoch 0009 / 0100 | batch 0040 / 0050 | loss 0.0556\n",
            "Train: epoch 0009 / 0100 | batch 0041 / 0050 | loss 0.0560\n",
            "Train: epoch 0009 / 0100 | batch 0042 / 0050 | loss 0.0565\n",
            "Train: epoch 0009 / 0100 | batch 0043 / 0050 | loss 0.0568\n",
            "Train: epoch 0009 / 0100 | batch 0044 / 0050 | loss 0.0564\n",
            "Train: epoch 0009 / 0100 | batch 0045 / 0050 | loss 0.0567\n",
            "Train: epoch 0009 / 0100 | batch 0046 / 0050 | loss 0.0570\n",
            "Train: epoch 0009 / 0100 | batch 0047 / 0050 | loss 0.0567\n",
            "Train: epoch 0009 / 0100 | batch 0048 / 0050 | loss 0.0567\n",
            "Train: epoch 0009 / 0100 | batch 0049 / 0050 | loss 0.0569\n",
            "Val loss 0.0644\n",
            "Dice score : 0.03616999089717865\n",
            "Val loss 0.0529\n",
            "Dice score : 0.0298747718334198\n",
            "Val loss 0.0510\n",
            "Dice score : 0.034625981003046036\n",
            "Val loss 0.0540\n",
            "Dice score : 0.07024374604225159\n",
            "Val loss 0.0570\n",
            "Dice score : 0.038860175758600235\n",
            "Val loss 0.0551\n",
            "Dice score : 0.04426143318414688\n",
            "Val loss 0.0555\n",
            "Dice score : 0.041645728051662445\n",
            "Val loss 0.0574\n",
            "Dice score : 0.0483667328953743\n",
            "Val loss 0.0559\n",
            "Dice score : 0.022926824167370796\n",
            "Val loss 0.0575\n",
            "Dice score : 0.0474054254591465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 9/100 [11:05<1:50:34, 72.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0010 / 0100 | batch 0000 / 0050 | loss 0.0513\n",
            "Train: epoch 0010 / 0100 | batch 0001 / 0050 | loss 0.0578\n",
            "Train: epoch 0010 / 0100 | batch 0002 / 0050 | loss 0.0621\n",
            "Train: epoch 0010 / 0100 | batch 0003 / 0050 | loss 0.0600\n",
            "Train: epoch 0010 / 0100 | batch 0004 / 0050 | loss 0.0572\n",
            "Train: epoch 0010 / 0100 | batch 0005 / 0050 | loss 0.0550\n",
            "Train: epoch 0010 / 0100 | batch 0006 / 0050 | loss 0.0570\n",
            "Train: epoch 0010 / 0100 | batch 0007 / 0050 | loss 0.0551\n",
            "Train: epoch 0010 / 0100 | batch 0008 / 0050 | loss 0.0543\n",
            "Train: epoch 0010 / 0100 | batch 0009 / 0050 | loss 0.0533\n",
            "Train: epoch 0010 / 0100 | batch 0010 / 0050 | loss 0.0534\n",
            "Train: epoch 0010 / 0100 | batch 0011 / 0050 | loss 0.0558\n",
            "Train: epoch 0010 / 0100 | batch 0012 / 0050 | loss 0.0557\n",
            "Train: epoch 0010 / 0100 | batch 0013 / 0050 | loss 0.0547\n",
            "Train: epoch 0010 / 0100 | batch 0014 / 0050 | loss 0.0554\n",
            "Train: epoch 0010 / 0100 | batch 0015 / 0050 | loss 0.0557\n",
            "Train: epoch 0010 / 0100 | batch 0016 / 0050 | loss 0.0559\n",
            "Train: epoch 0010 / 0100 | batch 0017 / 0050 | loss 0.0569\n",
            "Train: epoch 0010 / 0100 | batch 0018 / 0050 | loss 0.0572\n",
            "Train: epoch 0010 / 0100 | batch 0019 / 0050 | loss 0.0567\n",
            "Train: epoch 0010 / 0100 | batch 0020 / 0050 | loss 0.0572\n",
            "Train: epoch 0010 / 0100 | batch 0021 / 0050 | loss 0.0572\n",
            "Train: epoch 0010 / 0100 | batch 0022 / 0050 | loss 0.0569\n",
            "Train: epoch 0010 / 0100 | batch 0023 / 0050 | loss 0.0576\n",
            "Train: epoch 0010 / 0100 | batch 0024 / 0050 | loss 0.0578\n",
            "Train: epoch 0010 / 0100 | batch 0025 / 0050 | loss 0.0573\n",
            "Train: epoch 0010 / 0100 | batch 0026 / 0050 | loss 0.0574\n",
            "Train: epoch 0010 / 0100 | batch 0027 / 0050 | loss 0.0577\n",
            "Train: epoch 0010 / 0100 | batch 0028 / 0050 | loss 0.0574\n",
            "Train: epoch 0010 / 0100 | batch 0029 / 0050 | loss 0.0572\n",
            "Train: epoch 0010 / 0100 | batch 0030 / 0050 | loss 0.0569\n",
            "Train: epoch 0010 / 0100 | batch 0031 / 0050 | loss 0.0564\n",
            "Train: epoch 0010 / 0100 | batch 0032 / 0050 | loss 0.0566\n",
            "Train: epoch 0010 / 0100 | batch 0033 / 0050 | loss 0.0568\n",
            "Train: epoch 0010 / 0100 | batch 0034 / 0050 | loss 0.0573\n",
            "Train: epoch 0010 / 0100 | batch 0035 / 0050 | loss 0.0568\n",
            "Train: epoch 0010 / 0100 | batch 0036 / 0050 | loss 0.0566\n",
            "Train: epoch 0010 / 0100 | batch 0037 / 0050 | loss 0.0575\n",
            "Train: epoch 0010 / 0100 | batch 0038 / 0050 | loss 0.0582\n",
            "Train: epoch 0010 / 0100 | batch 0039 / 0050 | loss 0.0578\n",
            "Train: epoch 0010 / 0100 | batch 0040 / 0050 | loss 0.0574\n",
            "Train: epoch 0010 / 0100 | batch 0041 / 0050 | loss 0.0575\n",
            "Train: epoch 0010 / 0100 | batch 0042 / 0050 | loss 0.0574\n",
            "Train: epoch 0010 / 0100 | batch 0043 / 0050 | loss 0.0574\n",
            "Train: epoch 0010 / 0100 | batch 0044 / 0050 | loss 0.0576\n",
            "Train: epoch 0010 / 0100 | batch 0045 / 0050 | loss 0.0573\n",
            "Train: epoch 0010 / 0100 | batch 0046 / 0050 | loss 0.0571\n",
            "Train: epoch 0010 / 0100 | batch 0047 / 0050 | loss 0.0569\n",
            "Train: epoch 0010 / 0100 | batch 0048 / 0050 | loss 0.0567\n",
            "Train: epoch 0010 / 0100 | batch 0049 / 0050 | loss 0.0564\n",
            "Val loss 0.0839\n",
            "Dice score : 0.02609197422862053\n",
            "Val loss 0.0841\n",
            "Dice score : 0.041655849665403366\n",
            "Val loss 0.0700\n",
            "Dice score : 0.012283080257475376\n",
            "Val loss 0.0677\n",
            "Dice score : 0.0370672307908535\n",
            "Val loss 0.0730\n",
            "Dice score : 0.07028432935476303\n",
            "Val loss 0.0686\n",
            "Dice score : 0.02368275821208954\n",
            "Val loss 0.0653\n",
            "Dice score : 0.052236393094062805\n",
            "Val loss 0.0636\n",
            "Dice score : 0.050334032624959946\n",
            "Val loss 0.0616\n",
            "Dice score : 0.02867506630718708\n",
            "Val loss 0.0611\n",
            "Dice score : 0.032522451132535934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 10/100 [12:19<1:49:27, 72.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0011 / 0100 | batch 0000 / 0050 | loss 0.0423\n",
            "Train: epoch 0011 / 0100 | batch 0001 / 0050 | loss 0.0611\n",
            "Train: epoch 0011 / 0100 | batch 0002 / 0050 | loss 0.0591\n",
            "Train: epoch 0011 / 0100 | batch 0003 / 0050 | loss 0.0667\n",
            "Train: epoch 0011 / 0100 | batch 0004 / 0050 | loss 0.0679\n",
            "Train: epoch 0011 / 0100 | batch 0005 / 0050 | loss 0.0635\n",
            "Train: epoch 0011 / 0100 | batch 0006 / 0050 | loss 0.0643\n",
            "Train: epoch 0011 / 0100 | batch 0007 / 0050 | loss 0.0618\n",
            "Train: epoch 0011 / 0100 | batch 0008 / 0050 | loss 0.0607\n",
            "Train: epoch 0011 / 0100 | batch 0009 / 0050 | loss 0.0604\n",
            "Train: epoch 0011 / 0100 | batch 0010 / 0050 | loss 0.0612\n",
            "Train: epoch 0011 / 0100 | batch 0011 / 0050 | loss 0.0602\n",
            "Train: epoch 0011 / 0100 | batch 0012 / 0050 | loss 0.0593\n",
            "Train: epoch 0011 / 0100 | batch 0013 / 0050 | loss 0.0583\n",
            "Train: epoch 0011 / 0100 | batch 0014 / 0050 | loss 0.0586\n",
            "Train: epoch 0011 / 0100 | batch 0015 / 0050 | loss 0.0575\n",
            "Train: epoch 0011 / 0100 | batch 0016 / 0050 | loss 0.0571\n",
            "Train: epoch 0011 / 0100 | batch 0017 / 0050 | loss 0.0584\n",
            "Train: epoch 0011 / 0100 | batch 0018 / 0050 | loss 0.0577\n",
            "Train: epoch 0011 / 0100 | batch 0019 / 0050 | loss 0.0576\n",
            "Train: epoch 0011 / 0100 | batch 0020 / 0050 | loss 0.0578\n",
            "Train: epoch 0011 / 0100 | batch 0021 / 0050 | loss 0.0579\n",
            "Train: epoch 0011 / 0100 | batch 0022 / 0050 | loss 0.0573\n",
            "Train: epoch 0011 / 0100 | batch 0023 / 0050 | loss 0.0569\n",
            "Train: epoch 0011 / 0100 | batch 0024 / 0050 | loss 0.0570\n",
            "Train: epoch 0011 / 0100 | batch 0025 / 0050 | loss 0.0567\n",
            "Train: epoch 0011 / 0100 | batch 0026 / 0050 | loss 0.0563\n",
            "Train: epoch 0011 / 0100 | batch 0027 / 0050 | loss 0.0558\n",
            "Train: epoch 0011 / 0100 | batch 0028 / 0050 | loss 0.0556\n",
            "Train: epoch 0011 / 0100 | batch 0029 / 0050 | loss 0.0557\n",
            "Train: epoch 0011 / 0100 | batch 0030 / 0050 | loss 0.0556\n",
            "Train: epoch 0011 / 0100 | batch 0031 / 0050 | loss 0.0553\n",
            "Train: epoch 0011 / 0100 | batch 0032 / 0050 | loss 0.0550\n",
            "Train: epoch 0011 / 0100 | batch 0033 / 0050 | loss 0.0552\n",
            "Train: epoch 0011 / 0100 | batch 0034 / 0050 | loss 0.0550\n",
            "Train: epoch 0011 / 0100 | batch 0035 / 0050 | loss 0.0549\n",
            "Train: epoch 0011 / 0100 | batch 0036 / 0050 | loss 0.0551\n",
            "Train: epoch 0011 / 0100 | batch 0037 / 0050 | loss 0.0552\n",
            "Train: epoch 0011 / 0100 | batch 0038 / 0050 | loss 0.0549\n",
            "Train: epoch 0011 / 0100 | batch 0039 / 0050 | loss 0.0545\n",
            "Train: epoch 0011 / 0100 | batch 0040 / 0050 | loss 0.0541\n",
            "Train: epoch 0011 / 0100 | batch 0041 / 0050 | loss 0.0542\n",
            "Train: epoch 0011 / 0100 | batch 0042 / 0050 | loss 0.0544\n",
            "Train: epoch 0011 / 0100 | batch 0043 / 0050 | loss 0.0542\n",
            "Train: epoch 0011 / 0100 | batch 0044 / 0050 | loss 0.0540\n",
            "Train: epoch 0011 / 0100 | batch 0045 / 0050 | loss 0.0541\n",
            "Train: epoch 0011 / 0100 | batch 0046 / 0050 | loss 0.0549\n",
            "Train: epoch 0011 / 0100 | batch 0047 / 0050 | loss 0.0551\n",
            "Train: epoch 0011 / 0100 | batch 0048 / 0050 | loss 0.0551\n",
            "Train: epoch 0011 / 0100 | batch 0049 / 0050 | loss 0.0552\n",
            "Val loss 0.0444\n",
            "Dice score : 0.03288362920284271\n",
            "Val loss 0.0444\n",
            "Dice score : 0.04654523730278015\n",
            "Val loss 0.0485\n",
            "Dice score : 0.07296466827392578\n",
            "Val loss 0.0473\n",
            "Dice score : 0.050655871629714966\n",
            "Val loss 0.0514\n",
            "Dice score : 0.03353678435087204\n",
            "Val loss 0.0500\n",
            "Dice score : 0.03882283717393875\n",
            "Val loss 0.0536\n",
            "Dice score : 0.07697878777980804\n",
            "Val loss 0.0548\n",
            "Dice score : 0.028172902762889862\n",
            "Val loss 0.0568\n",
            "Dice score : 0.07205022126436234\n",
            "Val loss 0.0574\n",
            "Dice score : 0.062424130737781525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 11/100 [13:33<1:48:43, 73.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0012 / 0100 | batch 0000 / 0050 | loss 0.0544\n",
            "Train: epoch 0012 / 0100 | batch 0001 / 0050 | loss 0.0504\n",
            "Train: epoch 0012 / 0100 | batch 0002 / 0050 | loss 0.0498\n",
            "Train: epoch 0012 / 0100 | batch 0003 / 0050 | loss 0.0530\n",
            "Train: epoch 0012 / 0100 | batch 0004 / 0050 | loss 0.0503\n",
            "Train: epoch 0012 / 0100 | batch 0005 / 0050 | loss 0.0498\n",
            "Train: epoch 0012 / 0100 | batch 0006 / 0050 | loss 0.0514\n",
            "Train: epoch 0012 / 0100 | batch 0007 / 0050 | loss 0.0528\n",
            "Train: epoch 0012 / 0100 | batch 0008 / 0050 | loss 0.0524\n",
            "Train: epoch 0012 / 0100 | batch 0009 / 0050 | loss 0.0522\n",
            "Train: epoch 0012 / 0100 | batch 0010 / 0050 | loss 0.0512\n",
            "Train: epoch 0012 / 0100 | batch 0011 / 0050 | loss 0.0512\n",
            "Train: epoch 0012 / 0100 | batch 0012 / 0050 | loss 0.0514\n",
            "Train: epoch 0012 / 0100 | batch 0013 / 0050 | loss 0.0515\n",
            "Train: epoch 0012 / 0100 | batch 0014 / 0050 | loss 0.0517\n",
            "Train: epoch 0012 / 0100 | batch 0015 / 0050 | loss 0.0511\n",
            "Train: epoch 0012 / 0100 | batch 0016 / 0050 | loss 0.0507\n",
            "Train: epoch 0012 / 0100 | batch 0017 / 0050 | loss 0.0501\n",
            "Train: epoch 0012 / 0100 | batch 0018 / 0050 | loss 0.0499\n",
            "Train: epoch 0012 / 0100 | batch 0019 / 0050 | loss 0.0502\n",
            "Train: epoch 0012 / 0100 | batch 0020 / 0050 | loss 0.0513\n",
            "Train: epoch 0012 / 0100 | batch 0021 / 0050 | loss 0.0512\n",
            "Train: epoch 0012 / 0100 | batch 0022 / 0050 | loss 0.0511\n",
            "Train: epoch 0012 / 0100 | batch 0023 / 0050 | loss 0.0507\n",
            "Train: epoch 0012 / 0100 | batch 0024 / 0050 | loss 0.0504\n",
            "Train: epoch 0012 / 0100 | batch 0025 / 0050 | loss 0.0514\n",
            "Train: epoch 0012 / 0100 | batch 0026 / 0050 | loss 0.0518\n",
            "Train: epoch 0012 / 0100 | batch 0027 / 0050 | loss 0.0522\n",
            "Train: epoch 0012 / 0100 | batch 0028 / 0050 | loss 0.0535\n",
            "Train: epoch 0012 / 0100 | batch 0029 / 0050 | loss 0.0540\n",
            "Train: epoch 0012 / 0100 | batch 0030 / 0050 | loss 0.0536\n",
            "Train: epoch 0012 / 0100 | batch 0031 / 0050 | loss 0.0540\n",
            "Train: epoch 0012 / 0100 | batch 0032 / 0050 | loss 0.0536\n",
            "Train: epoch 0012 / 0100 | batch 0033 / 0050 | loss 0.0538\n",
            "Train: epoch 0012 / 0100 | batch 0034 / 0050 | loss 0.0540\n",
            "Train: epoch 0012 / 0100 | batch 0035 / 0050 | loss 0.0537\n",
            "Train: epoch 0012 / 0100 | batch 0036 / 0050 | loss 0.0536\n",
            "Train: epoch 0012 / 0100 | batch 0037 / 0050 | loss 0.0535\n",
            "Train: epoch 0012 / 0100 | batch 0038 / 0050 | loss 0.0540\n",
            "Train: epoch 0012 / 0100 | batch 0039 / 0050 | loss 0.0544\n",
            "Train: epoch 0012 / 0100 | batch 0040 / 0050 | loss 0.0541\n",
            "Train: epoch 0012 / 0100 | batch 0041 / 0050 | loss 0.0541\n",
            "Train: epoch 0012 / 0100 | batch 0042 / 0050 | loss 0.0539\n",
            "Train: epoch 0012 / 0100 | batch 0043 / 0050 | loss 0.0536\n",
            "Train: epoch 0012 / 0100 | batch 0044 / 0050 | loss 0.0540\n",
            "Train: epoch 0012 / 0100 | batch 0045 / 0050 | loss 0.0541\n",
            "Train: epoch 0012 / 0100 | batch 0046 / 0050 | loss 0.0539\n",
            "Train: epoch 0012 / 0100 | batch 0047 / 0050 | loss 0.0543\n",
            "Train: epoch 0012 / 0100 | batch 0048 / 0050 | loss 0.0541\n",
            "Train: epoch 0012 / 0100 | batch 0049 / 0050 | loss 0.0541\n",
            "Val loss 0.0436\n",
            "Dice score : 0.041324764490127563\n",
            "Val loss 0.0420\n",
            "Dice score : 0.053552158176898956\n",
            "Val loss 0.0511\n",
            "Dice score : 0.04479668661952019\n",
            "Val loss 0.0602\n",
            "Dice score : 0.06690860539674759\n",
            "Val loss 0.0574\n",
            "Dice score : 0.04023393243551254\n",
            "Val loss 0.0548\n",
            "Dice score : 0.05293932557106018\n",
            "Val loss 0.0559\n",
            "Dice score : 0.04159685596823692\n",
            "Val loss 0.0545\n",
            "Dice score : 0.029434632509946823\n",
            "Val loss 0.0543\n",
            "Dice score : 0.04236871749162674\n",
            "Val loss 0.0564\n",
            "Dice score : 0.07571106404066086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 12/100 [14:46<1:47:28, 73.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0013 / 0100 | batch 0000 / 0050 | loss 0.0708\n",
            "Train: epoch 0013 / 0100 | batch 0001 / 0050 | loss 0.0626\n",
            "Train: epoch 0013 / 0100 | batch 0002 / 0050 | loss 0.0522\n",
            "Train: epoch 0013 / 0100 | batch 0003 / 0050 | loss 0.0512\n",
            "Train: epoch 0013 / 0100 | batch 0004 / 0050 | loss 0.0521\n",
            "Train: epoch 0013 / 0100 | batch 0005 / 0050 | loss 0.0506\n",
            "Train: epoch 0013 / 0100 | batch 0006 / 0050 | loss 0.0510\n",
            "Train: epoch 0013 / 0100 | batch 0007 / 0050 | loss 0.0527\n",
            "Train: epoch 0013 / 0100 | batch 0008 / 0050 | loss 0.0531\n",
            "Train: epoch 0013 / 0100 | batch 0009 / 0050 | loss 0.0525\n",
            "Train: epoch 0013 / 0100 | batch 0010 / 0050 | loss 0.0548\n",
            "Train: epoch 0013 / 0100 | batch 0011 / 0050 | loss 0.0540\n",
            "Train: epoch 0013 / 0100 | batch 0012 / 0050 | loss 0.0537\n",
            "Train: epoch 0013 / 0100 | batch 0013 / 0050 | loss 0.0537\n",
            "Train: epoch 0013 / 0100 | batch 0014 / 0050 | loss 0.0537\n",
            "Train: epoch 0013 / 0100 | batch 0015 / 0050 | loss 0.0544\n",
            "Train: epoch 0013 / 0100 | batch 0016 / 0050 | loss 0.0539\n",
            "Train: epoch 0013 / 0100 | batch 0017 / 0050 | loss 0.0531\n",
            "Train: epoch 0013 / 0100 | batch 0018 / 0050 | loss 0.0527\n",
            "Train: epoch 0013 / 0100 | batch 0019 / 0050 | loss 0.0530\n",
            "Train: epoch 0013 / 0100 | batch 0020 / 0050 | loss 0.0532\n",
            "Train: epoch 0013 / 0100 | batch 0021 / 0050 | loss 0.0528\n",
            "Train: epoch 0013 / 0100 | batch 0022 / 0050 | loss 0.0525\n",
            "Train: epoch 0013 / 0100 | batch 0023 / 0050 | loss 0.0539\n",
            "Train: epoch 0013 / 0100 | batch 0024 / 0050 | loss 0.0537\n",
            "Train: epoch 0013 / 0100 | batch 0025 / 0050 | loss 0.0540\n",
            "Train: epoch 0013 / 0100 | batch 0026 / 0050 | loss 0.0544\n",
            "Train: epoch 0013 / 0100 | batch 0027 / 0050 | loss 0.0543\n",
            "Train: epoch 0013 / 0100 | batch 0028 / 0050 | loss 0.0540\n",
            "Train: epoch 0013 / 0100 | batch 0029 / 0050 | loss 0.0538\n",
            "Train: epoch 0013 / 0100 | batch 0030 / 0050 | loss 0.0536\n",
            "Train: epoch 0013 / 0100 | batch 0031 / 0050 | loss 0.0535\n",
            "Train: epoch 0013 / 0100 | batch 0032 / 0050 | loss 0.0533\n",
            "Train: epoch 0013 / 0100 | batch 0033 / 0050 | loss 0.0529\n",
            "Train: epoch 0013 / 0100 | batch 0034 / 0050 | loss 0.0531\n",
            "Train: epoch 0013 / 0100 | batch 0035 / 0050 | loss 0.0532\n",
            "Train: epoch 0013 / 0100 | batch 0036 / 0050 | loss 0.0527\n",
            "Train: epoch 0013 / 0100 | batch 0037 / 0050 | loss 0.0525\n",
            "Train: epoch 0013 / 0100 | batch 0038 / 0050 | loss 0.0523\n",
            "Train: epoch 0013 / 0100 | batch 0039 / 0050 | loss 0.0522\n",
            "Train: epoch 0013 / 0100 | batch 0040 / 0050 | loss 0.0522\n",
            "Train: epoch 0013 / 0100 | batch 0041 / 0050 | loss 0.0528\n",
            "Train: epoch 0013 / 0100 | batch 0042 / 0050 | loss 0.0524\n",
            "Train: epoch 0013 / 0100 | batch 0043 / 0050 | loss 0.0527\n",
            "Train: epoch 0013 / 0100 | batch 0044 / 0050 | loss 0.0531\n",
            "Train: epoch 0013 / 0100 | batch 0045 / 0050 | loss 0.0535\n",
            "Train: epoch 0013 / 0100 | batch 0046 / 0050 | loss 0.0533\n",
            "Train: epoch 0013 / 0100 | batch 0047 / 0050 | loss 0.0541\n",
            "Train: epoch 0013 / 0100 | batch 0048 / 0050 | loss 0.0539\n",
            "Train: epoch 0013 / 0100 | batch 0049 / 0050 | loss 0.0537\n",
            "Val loss 0.0663\n",
            "Dice score : 0.05142582580447197\n",
            "Val loss 0.0657\n",
            "Dice score : 0.05402710288763046\n",
            "Val loss 0.0662\n",
            "Dice score : 0.06292015314102173\n",
            "Val loss 0.0611\n",
            "Dice score : 0.02293812297284603\n",
            "Val loss 0.0563\n",
            "Dice score : 0.03123062662780285\n",
            "Val loss 0.0541\n",
            "Dice score : 0.049899738281965256\n",
            "Val loss 0.0545\n",
            "Dice score : 0.043795861303806305\n",
            "Val loss 0.0528\n",
            "Dice score : 0.05013532564043999\n",
            "Val loss 0.0541\n",
            "Dice score : 0.05718124285340309\n",
            "Val loss 0.0558\n",
            "Dice score : 0.05550664663314819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 13/100 [15:59<1:46:04, 73.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0014 / 0100 | batch 0000 / 0050 | loss 0.0451\n",
            "Train: epoch 0014 / 0100 | batch 0001 / 0050 | loss 0.0458\n",
            "Train: epoch 0014 / 0100 | batch 0002 / 0050 | loss 0.0457\n",
            "Train: epoch 0014 / 0100 | batch 0003 / 0050 | loss 0.0449\n",
            "Train: epoch 0014 / 0100 | batch 0004 / 0050 | loss 0.0464\n",
            "Train: epoch 0014 / 0100 | batch 0005 / 0050 | loss 0.0532\n",
            "Train: epoch 0014 / 0100 | batch 0006 / 0050 | loss 0.0539\n",
            "Train: epoch 0014 / 0100 | batch 0007 / 0050 | loss 0.0537\n",
            "Train: epoch 0014 / 0100 | batch 0008 / 0050 | loss 0.0530\n",
            "Train: epoch 0014 / 0100 | batch 0009 / 0050 | loss 0.0540\n",
            "Train: epoch 0014 / 0100 | batch 0010 / 0050 | loss 0.0549\n",
            "Train: epoch 0014 / 0100 | batch 0011 / 0050 | loss 0.0536\n",
            "Train: epoch 0014 / 0100 | batch 0012 / 0050 | loss 0.0530\n",
            "Train: epoch 0014 / 0100 | batch 0013 / 0050 | loss 0.0527\n",
            "Train: epoch 0014 / 0100 | batch 0014 / 0050 | loss 0.0533\n",
            "Train: epoch 0014 / 0100 | batch 0015 / 0050 | loss 0.0525\n",
            "Train: epoch 0014 / 0100 | batch 0016 / 0050 | loss 0.0522\n",
            "Train: epoch 0014 / 0100 | batch 0017 / 0050 | loss 0.0529\n",
            "Train: epoch 0014 / 0100 | batch 0018 / 0050 | loss 0.0536\n",
            "Train: epoch 0014 / 0100 | batch 0019 / 0050 | loss 0.0532\n",
            "Train: epoch 0014 / 0100 | batch 0020 / 0050 | loss 0.0530\n",
            "Train: epoch 0014 / 0100 | batch 0021 / 0050 | loss 0.0533\n",
            "Train: epoch 0014 / 0100 | batch 0022 / 0050 | loss 0.0531\n",
            "Train: epoch 0014 / 0100 | batch 0023 / 0050 | loss 0.0526\n",
            "Train: epoch 0014 / 0100 | batch 0024 / 0050 | loss 0.0542\n",
            "Train: epoch 0014 / 0100 | batch 0025 / 0050 | loss 0.0543\n",
            "Train: epoch 0014 / 0100 | batch 0026 / 0050 | loss 0.0539\n",
            "Train: epoch 0014 / 0100 | batch 0027 / 0050 | loss 0.0538\n",
            "Train: epoch 0014 / 0100 | batch 0028 / 0050 | loss 0.0531\n",
            "Train: epoch 0014 / 0100 | batch 0029 / 0050 | loss 0.0533\n",
            "Train: epoch 0014 / 0100 | batch 0030 / 0050 | loss 0.0534\n",
            "Train: epoch 0014 / 0100 | batch 0031 / 0050 | loss 0.0536\n",
            "Train: epoch 0014 / 0100 | batch 0032 / 0050 | loss 0.0536\n",
            "Train: epoch 0014 / 0100 | batch 0033 / 0050 | loss 0.0535\n",
            "Train: epoch 0014 / 0100 | batch 0034 / 0050 | loss 0.0531\n",
            "Train: epoch 0014 / 0100 | batch 0035 / 0050 | loss 0.0540\n",
            "Train: epoch 0014 / 0100 | batch 0036 / 0050 | loss 0.0541\n",
            "Train: epoch 0014 / 0100 | batch 0037 / 0050 | loss 0.0537\n",
            "Train: epoch 0014 / 0100 | batch 0038 / 0050 | loss 0.0531\n",
            "Train: epoch 0014 / 0100 | batch 0039 / 0050 | loss 0.0536\n",
            "Train: epoch 0014 / 0100 | batch 0040 / 0050 | loss 0.0534\n",
            "Train: epoch 0014 / 0100 | batch 0041 / 0050 | loss 0.0538\n",
            "Train: epoch 0014 / 0100 | batch 0042 / 0050 | loss 0.0536\n",
            "Train: epoch 0014 / 0100 | batch 0043 / 0050 | loss 0.0532\n",
            "Train: epoch 0014 / 0100 | batch 0044 / 0050 | loss 0.0534\n",
            "Train: epoch 0014 / 0100 | batch 0045 / 0050 | loss 0.0530\n",
            "Train: epoch 0014 / 0100 | batch 0046 / 0050 | loss 0.0537\n",
            "Train: epoch 0014 / 0100 | batch 0047 / 0050 | loss 0.0538\n",
            "Train: epoch 0014 / 0100 | batch 0048 / 0050 | loss 0.0537\n",
            "Train: epoch 0014 / 0100 | batch 0049 / 0050 | loss 0.0536\n",
            "Val loss 0.0678\n",
            "Dice score : 0.04869190603494644\n",
            "Val loss 0.0683\n",
            "Dice score : 0.03399260714650154\n",
            "Val loss 0.0625\n",
            "Dice score : 0.04032175615429878\n",
            "Val loss 0.0595\n",
            "Dice score : 0.042435407638549805\n",
            "Val loss 0.0574\n",
            "Dice score : 0.028108850121498108\n",
            "Val loss 0.0617\n",
            "Dice score : 0.019762905314564705\n",
            "Val loss 0.0676\n",
            "Dice score : 0.07703530043363571\n",
            "Val loss 0.0650\n",
            "Dice score : 0.038254525512456894\n",
            "Val loss 0.0641\n",
            "Dice score : 0.04597349837422371\n",
            "Val loss 0.0628\n",
            "Dice score : 0.05519504472613335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 14/100 [17:11<1:44:28, 72.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0015 / 0100 | batch 0000 / 0050 | loss 0.0711\n",
            "Train: epoch 0015 / 0100 | batch 0001 / 0050 | loss 0.0605\n",
            "Train: epoch 0015 / 0100 | batch 0002 / 0050 | loss 0.0610\n",
            "Train: epoch 0015 / 0100 | batch 0003 / 0050 | loss 0.0574\n",
            "Train: epoch 0015 / 0100 | batch 0004 / 0050 | loss 0.0570\n",
            "Train: epoch 0015 / 0100 | batch 0005 / 0050 | loss 0.0532\n",
            "Train: epoch 0015 / 0100 | batch 0006 / 0050 | loss 0.0528\n",
            "Train: epoch 0015 / 0100 | batch 0007 / 0050 | loss 0.0544\n",
            "Train: epoch 0015 / 0100 | batch 0008 / 0050 | loss 0.0544\n",
            "Train: epoch 0015 / 0100 | batch 0009 / 0050 | loss 0.0535\n",
            "Train: epoch 0015 / 0100 | batch 0010 / 0050 | loss 0.0543\n",
            "Train: epoch 0015 / 0100 | batch 0011 / 0050 | loss 0.0558\n",
            "Train: epoch 0015 / 0100 | batch 0012 / 0050 | loss 0.0550\n",
            "Train: epoch 0015 / 0100 | batch 0013 / 0050 | loss 0.0550\n",
            "Train: epoch 0015 / 0100 | batch 0014 / 0050 | loss 0.0538\n",
            "Train: epoch 0015 / 0100 | batch 0015 / 0050 | loss 0.0526\n",
            "Train: epoch 0015 / 0100 | batch 0016 / 0050 | loss 0.0523\n",
            "Train: epoch 0015 / 0100 | batch 0017 / 0050 | loss 0.0527\n",
            "Train: epoch 0015 / 0100 | batch 0018 / 0050 | loss 0.0519\n",
            "Train: epoch 0015 / 0100 | batch 0019 / 0050 | loss 0.0523\n",
            "Train: epoch 0015 / 0100 | batch 0020 / 0050 | loss 0.0523\n",
            "Train: epoch 0015 / 0100 | batch 0021 / 0050 | loss 0.0528\n",
            "Train: epoch 0015 / 0100 | batch 0022 / 0050 | loss 0.0522\n",
            "Train: epoch 0015 / 0100 | batch 0023 / 0050 | loss 0.0523\n",
            "Train: epoch 0015 / 0100 | batch 0024 / 0050 | loss 0.0523\n",
            "Train: epoch 0015 / 0100 | batch 0025 / 0050 | loss 0.0521\n",
            "Train: epoch 0015 / 0100 | batch 0026 / 0050 | loss 0.0520\n",
            "Train: epoch 0015 / 0100 | batch 0027 / 0050 | loss 0.0517\n",
            "Train: epoch 0015 / 0100 | batch 0028 / 0050 | loss 0.0514\n",
            "Train: epoch 0015 / 0100 | batch 0029 / 0050 | loss 0.0511\n",
            "Train: epoch 0015 / 0100 | batch 0030 / 0050 | loss 0.0506\n",
            "Train: epoch 0015 / 0100 | batch 0031 / 0050 | loss 0.0504\n",
            "Train: epoch 0015 / 0100 | batch 0032 / 0050 | loss 0.0503\n",
            "Train: epoch 0015 / 0100 | batch 0033 / 0050 | loss 0.0504\n",
            "Train: epoch 0015 / 0100 | batch 0034 / 0050 | loss 0.0511\n",
            "Train: epoch 0015 / 0100 | batch 0035 / 0050 | loss 0.0510\n",
            "Train: epoch 0015 / 0100 | batch 0036 / 0050 | loss 0.0515\n",
            "Train: epoch 0015 / 0100 | batch 0037 / 0050 | loss 0.0513\n",
            "Train: epoch 0015 / 0100 | batch 0038 / 0050 | loss 0.0516\n",
            "Train: epoch 0015 / 0100 | batch 0039 / 0050 | loss 0.0516\n",
            "Train: epoch 0015 / 0100 | batch 0040 / 0050 | loss 0.0519\n",
            "Train: epoch 0015 / 0100 | batch 0041 / 0050 | loss 0.0521\n",
            "Train: epoch 0015 / 0100 | batch 0042 / 0050 | loss 0.0528\n",
            "Train: epoch 0015 / 0100 | batch 0043 / 0050 | loss 0.0526\n",
            "Train: epoch 0015 / 0100 | batch 0044 / 0050 | loss 0.0533\n",
            "Train: epoch 0015 / 0100 | batch 0045 / 0050 | loss 0.0533\n",
            "Train: epoch 0015 / 0100 | batch 0046 / 0050 | loss 0.0532\n",
            "Train: epoch 0015 / 0100 | batch 0047 / 0050 | loss 0.0540\n",
            "Train: epoch 0015 / 0100 | batch 0048 / 0050 | loss 0.0538\n",
            "Train: epoch 0015 / 0100 | batch 0049 / 0050 | loss 0.0536\n",
            "Val loss 0.0729\n",
            "Dice score : 0.06250481307506561\n",
            "Val loss 0.0815\n",
            "Dice score : 0.005057888105511665\n",
            "Val loss 0.0831\n",
            "Dice score : 0.05072563514113426\n",
            "Val loss 0.0765\n",
            "Dice score : 0.03677120804786682\n",
            "Val loss 0.0738\n",
            "Dice score : 0.05145249888300896\n",
            "Val loss 0.0728\n",
            "Dice score : 0.06062553822994232\n",
            "Val loss 0.0705\n",
            "Dice score : 0.0159304179251194\n",
            "Val loss 0.0697\n",
            "Dice score : 0.005836026277393103\n",
            "Val loss 0.0674\n",
            "Dice score : 0.034770041704177856\n",
            "Val loss 0.0684\n",
            "Dice score : 0.04144906625151634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 15/100 [18:23<1:42:43, 72.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0016 / 0100 | batch 0000 / 0050 | loss 0.0683\n",
            "Train: epoch 0016 / 0100 | batch 0001 / 0050 | loss 0.0673\n",
            "Train: epoch 0016 / 0100 | batch 0002 / 0050 | loss 0.0716\n",
            "Train: epoch 0016 / 0100 | batch 0003 / 0050 | loss 0.0646\n",
            "Train: epoch 0016 / 0100 | batch 0004 / 0050 | loss 0.0644\n",
            "Train: epoch 0016 / 0100 | batch 0005 / 0050 | loss 0.0617\n",
            "Train: epoch 0016 / 0100 | batch 0006 / 0050 | loss 0.0590\n",
            "Train: epoch 0016 / 0100 | batch 0007 / 0050 | loss 0.0581\n",
            "Train: epoch 0016 / 0100 | batch 0008 / 0050 | loss 0.0587\n",
            "Train: epoch 0016 / 0100 | batch 0009 / 0050 | loss 0.0567\n",
            "Train: epoch 0016 / 0100 | batch 0010 / 0050 | loss 0.0553\n",
            "Train: epoch 0016 / 0100 | batch 0011 / 0050 | loss 0.0560\n",
            "Train: epoch 0016 / 0100 | batch 0012 / 0050 | loss 0.0551\n",
            "Train: epoch 0016 / 0100 | batch 0013 / 0050 | loss 0.0541\n",
            "Train: epoch 0016 / 0100 | batch 0014 / 0050 | loss 0.0551\n",
            "Train: epoch 0016 / 0100 | batch 0015 / 0050 | loss 0.0543\n",
            "Train: epoch 0016 / 0100 | batch 0016 / 0050 | loss 0.0544\n",
            "Train: epoch 0016 / 0100 | batch 0017 / 0050 | loss 0.0549\n",
            "Train: epoch 0016 / 0100 | batch 0018 / 0050 | loss 0.0543\n",
            "Train: epoch 0016 / 0100 | batch 0019 / 0050 | loss 0.0548\n",
            "Train: epoch 0016 / 0100 | batch 0020 / 0050 | loss 0.0542\n",
            "Train: epoch 0016 / 0100 | batch 0021 / 0050 | loss 0.0535\n",
            "Train: epoch 0016 / 0100 | batch 0022 / 0050 | loss 0.0534\n",
            "Train: epoch 0016 / 0100 | batch 0023 / 0050 | loss 0.0534\n",
            "Train: epoch 0016 / 0100 | batch 0024 / 0050 | loss 0.0540\n",
            "Train: epoch 0016 / 0100 | batch 0025 / 0050 | loss 0.0546\n",
            "Train: epoch 0016 / 0100 | batch 0026 / 0050 | loss 0.0548\n",
            "Train: epoch 0016 / 0100 | batch 0027 / 0050 | loss 0.0555\n",
            "Train: epoch 0016 / 0100 | batch 0028 / 0050 | loss 0.0556\n",
            "Train: epoch 0016 / 0100 | batch 0029 / 0050 | loss 0.0553\n",
            "Train: epoch 0016 / 0100 | batch 0030 / 0050 | loss 0.0551\n",
            "Train: epoch 0016 / 0100 | batch 0031 / 0050 | loss 0.0547\n",
            "Train: epoch 0016 / 0100 | batch 0032 / 0050 | loss 0.0542\n",
            "Train: epoch 0016 / 0100 | batch 0033 / 0050 | loss 0.0545\n",
            "Train: epoch 0016 / 0100 | batch 0034 / 0050 | loss 0.0551\n",
            "Train: epoch 0016 / 0100 | batch 0035 / 0050 | loss 0.0547\n",
            "Train: epoch 0016 / 0100 | batch 0036 / 0050 | loss 0.0546\n",
            "Train: epoch 0016 / 0100 | batch 0037 / 0050 | loss 0.0545\n",
            "Train: epoch 0016 / 0100 | batch 0038 / 0050 | loss 0.0546\n",
            "Train: epoch 0016 / 0100 | batch 0039 / 0050 | loss 0.0546\n",
            "Train: epoch 0016 / 0100 | batch 0040 / 0050 | loss 0.0543\n",
            "Train: epoch 0016 / 0100 | batch 0041 / 0050 | loss 0.0541\n",
            "Train: epoch 0016 / 0100 | batch 0042 / 0050 | loss 0.0543\n",
            "Train: epoch 0016 / 0100 | batch 0043 / 0050 | loss 0.0545\n",
            "Train: epoch 0016 / 0100 | batch 0044 / 0050 | loss 0.0546\n",
            "Train: epoch 0016 / 0100 | batch 0045 / 0050 | loss 0.0549\n",
            "Train: epoch 0016 / 0100 | batch 0046 / 0050 | loss 0.0549\n",
            "Train: epoch 0016 / 0100 | batch 0047 / 0050 | loss 0.0549\n",
            "Train: epoch 0016 / 0100 | batch 0048 / 0050 | loss 0.0552\n",
            "Train: epoch 0016 / 0100 | batch 0049 / 0050 | loss 0.0549\n",
            "Val loss 0.0413\n",
            "Dice score : 0.03183199092745781\n",
            "Val loss 0.0519\n",
            "Dice score : 0.07332345098257065\n",
            "Val loss 0.0528\n",
            "Dice score : 0.06989014148712158\n",
            "Val loss 0.0646\n",
            "Dice score : 0.10261247307062149\n",
            "Val loss 0.0597\n",
            "Dice score : 0.0459185354411602\n",
            "Val loss 0.0580\n",
            "Dice score : 0.047819726169109344\n",
            "Val loss 0.0582\n",
            "Dice score : 0.04127988964319229\n",
            "Val loss 0.0587\n",
            "Dice score : 0.03361305594444275\n",
            "Val loss 0.0571\n",
            "Dice score : 0.04020051658153534\n",
            "Val loss 0.0563\n",
            "Dice score : 0.03882245719432831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 16/100 [19:34<1:40:53, 72.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0017 / 0100 | batch 0000 / 0050 | loss 0.0660\n",
            "Train: epoch 0017 / 0100 | batch 0001 / 0050 | loss 0.0536\n",
            "Train: epoch 0017 / 0100 | batch 0002 / 0050 | loss 0.0498\n",
            "Train: epoch 0017 / 0100 | batch 0003 / 0050 | loss 0.0498\n",
            "Train: epoch 0017 / 0100 | batch 0004 / 0050 | loss 0.0487\n",
            "Train: epoch 0017 / 0100 | batch 0005 / 0050 | loss 0.0487\n",
            "Train: epoch 0017 / 0100 | batch 0006 / 0050 | loss 0.0474\n",
            "Train: epoch 0017 / 0100 | batch 0007 / 0050 | loss 0.0464\n",
            "Train: epoch 0017 / 0100 | batch 0008 / 0050 | loss 0.0472\n",
            "Train: epoch 0017 / 0100 | batch 0009 / 0050 | loss 0.0478\n",
            "Train: epoch 0017 / 0100 | batch 0010 / 0050 | loss 0.0485\n",
            "Train: epoch 0017 / 0100 | batch 0011 / 0050 | loss 0.0509\n",
            "Train: epoch 0017 / 0100 | batch 0012 / 0050 | loss 0.0501\n",
            "Train: epoch 0017 / 0100 | batch 0013 / 0050 | loss 0.0508\n",
            "Train: epoch 0017 / 0100 | batch 0014 / 0050 | loss 0.0525\n",
            "Train: epoch 0017 / 0100 | batch 0015 / 0050 | loss 0.0520\n",
            "Train: epoch 0017 / 0100 | batch 0016 / 0050 | loss 0.0530\n",
            "Train: epoch 0017 / 0100 | batch 0017 / 0050 | loss 0.0524\n",
            "Train: epoch 0017 / 0100 | batch 0018 / 0050 | loss 0.0518\n",
            "Train: epoch 0017 / 0100 | batch 0019 / 0050 | loss 0.0521\n",
            "Train: epoch 0017 / 0100 | batch 0020 / 0050 | loss 0.0521\n",
            "Train: epoch 0017 / 0100 | batch 0021 / 0050 | loss 0.0523\n",
            "Train: epoch 0017 / 0100 | batch 0022 / 0050 | loss 0.0542\n",
            "Train: epoch 0017 / 0100 | batch 0023 / 0050 | loss 0.0544\n",
            "Train: epoch 0017 / 0100 | batch 0024 / 0050 | loss 0.0541\n",
            "Train: epoch 0017 / 0100 | batch 0025 / 0050 | loss 0.0538\n",
            "Train: epoch 0017 / 0100 | batch 0026 / 0050 | loss 0.0535\n",
            "Train: epoch 0017 / 0100 | batch 0027 / 0050 | loss 0.0540\n",
            "Train: epoch 0017 / 0100 | batch 0028 / 0050 | loss 0.0536\n",
            "Train: epoch 0017 / 0100 | batch 0029 / 0050 | loss 0.0537\n",
            "Train: epoch 0017 / 0100 | batch 0030 / 0050 | loss 0.0537\n",
            "Train: epoch 0017 / 0100 | batch 0031 / 0050 | loss 0.0545\n",
            "Train: epoch 0017 / 0100 | batch 0032 / 0050 | loss 0.0546\n",
            "Train: epoch 0017 / 0100 | batch 0033 / 0050 | loss 0.0550\n",
            "Train: epoch 0017 / 0100 | batch 0034 / 0050 | loss 0.0544\n",
            "Train: epoch 0017 / 0100 | batch 0035 / 0050 | loss 0.0540\n",
            "Train: epoch 0017 / 0100 | batch 0036 / 0050 | loss 0.0542\n",
            "Train: epoch 0017 / 0100 | batch 0037 / 0050 | loss 0.0540\n",
            "Train: epoch 0017 / 0100 | batch 0038 / 0050 | loss 0.0537\n",
            "Train: epoch 0017 / 0100 | batch 0039 / 0050 | loss 0.0535\n",
            "Train: epoch 0017 / 0100 | batch 0040 / 0050 | loss 0.0533\n",
            "Train: epoch 0017 / 0100 | batch 0041 / 0050 | loss 0.0531\n",
            "Train: epoch 0017 / 0100 | batch 0042 / 0050 | loss 0.0529\n",
            "Train: epoch 0017 / 0100 | batch 0043 / 0050 | loss 0.0531\n",
            "Train: epoch 0017 / 0100 | batch 0044 / 0050 | loss 0.0531\n",
            "Train: epoch 0017 / 0100 | batch 0045 / 0050 | loss 0.0530\n",
            "Train: epoch 0017 / 0100 | batch 0046 / 0050 | loss 0.0533\n",
            "Train: epoch 0017 / 0100 | batch 0047 / 0050 | loss 0.0533\n",
            "Train: epoch 0017 / 0100 | batch 0048 / 0050 | loss 0.0537\n",
            "Train: epoch 0017 / 0100 | batch 0049 / 0050 | loss 0.0537\n",
            "Val loss 0.0715\n",
            "Dice score : 0.05038686841726303\n",
            "Val loss 0.0659\n",
            "Dice score : 0.03984421119093895\n",
            "Val loss 0.0686\n",
            "Dice score : 0.07110415399074554\n",
            "Val loss 0.0691\n",
            "Dice score : 0.04670463502407074\n",
            "Val loss 0.0677\n",
            "Dice score : 0.030683433637022972\n",
            "Val loss 0.0643\n",
            "Dice score : 0.06621011346578598\n",
            "Val loss 0.0612\n",
            "Dice score : 0.0338301919400692\n",
            "Val loss 0.0590\n",
            "Dice score : 0.0484212189912796\n",
            "Val loss 0.0570\n",
            "Dice score : 0.025963163003325462\n",
            "Val loss 0.0551\n",
            "Dice score : 0.032588470727205276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 17/100 [20:47<1:40:25, 72.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0018 / 0100 | batch 0000 / 0050 | loss 0.0440\n",
            "Train: epoch 0018 / 0100 | batch 0001 / 0050 | loss 0.0450\n",
            "Train: epoch 0018 / 0100 | batch 0002 / 0050 | loss 0.0480\n",
            "Train: epoch 0018 / 0100 | batch 0003 / 0050 | loss 0.0460\n",
            "Train: epoch 0018 / 0100 | batch 0004 / 0050 | loss 0.0483\n",
            "Train: epoch 0018 / 0100 | batch 0005 / 0050 | loss 0.0470\n",
            "Train: epoch 0018 / 0100 | batch 0006 / 0050 | loss 0.0464\n",
            "Train: epoch 0018 / 0100 | batch 0007 / 0050 | loss 0.0459\n",
            "Train: epoch 0018 / 0100 | batch 0008 / 0050 | loss 0.0461\n",
            "Train: epoch 0018 / 0100 | batch 0009 / 0050 | loss 0.0452\n",
            "Train: epoch 0018 / 0100 | batch 0010 / 0050 | loss 0.0451\n",
            "Train: epoch 0018 / 0100 | batch 0011 / 0050 | loss 0.0450\n",
            "Train: epoch 0018 / 0100 | batch 0012 / 0050 | loss 0.0449\n",
            "Train: epoch 0018 / 0100 | batch 0013 / 0050 | loss 0.0468\n",
            "Train: epoch 0018 / 0100 | batch 0014 / 0050 | loss 0.0500\n",
            "Train: epoch 0018 / 0100 | batch 0015 / 0050 | loss 0.0513\n",
            "Train: epoch 0018 / 0100 | batch 0016 / 0050 | loss 0.0516\n",
            "Train: epoch 0018 / 0100 | batch 0017 / 0050 | loss 0.0518\n",
            "Train: epoch 0018 / 0100 | batch 0018 / 0050 | loss 0.0513\n",
            "Train: epoch 0018 / 0100 | batch 0019 / 0050 | loss 0.0513\n",
            "Train: epoch 0018 / 0100 | batch 0020 / 0050 | loss 0.0519\n",
            "Train: epoch 0018 / 0100 | batch 0021 / 0050 | loss 0.0528\n",
            "Train: epoch 0018 / 0100 | batch 0022 / 0050 | loss 0.0543\n",
            "Train: epoch 0018 / 0100 | batch 0023 / 0050 | loss 0.0545\n",
            "Train: epoch 0018 / 0100 | batch 0024 / 0050 | loss 0.0546\n",
            "Train: epoch 0018 / 0100 | batch 0025 / 0050 | loss 0.0543\n",
            "Train: epoch 0018 / 0100 | batch 0026 / 0050 | loss 0.0543\n",
            "Train: epoch 0018 / 0100 | batch 0027 / 0050 | loss 0.0540\n",
            "Train: epoch 0018 / 0100 | batch 0028 / 0050 | loss 0.0544\n",
            "Train: epoch 0018 / 0100 | batch 0029 / 0050 | loss 0.0546\n",
            "Train: epoch 0018 / 0100 | batch 0030 / 0050 | loss 0.0543\n",
            "Train: epoch 0018 / 0100 | batch 0031 / 0050 | loss 0.0540\n",
            "Train: epoch 0018 / 0100 | batch 0032 / 0050 | loss 0.0546\n",
            "Train: epoch 0018 / 0100 | batch 0033 / 0050 | loss 0.0543\n",
            "Train: epoch 0018 / 0100 | batch 0034 / 0050 | loss 0.0544\n",
            "Train: epoch 0018 / 0100 | batch 0035 / 0050 | loss 0.0543\n",
            "Train: epoch 0018 / 0100 | batch 0036 / 0050 | loss 0.0544\n",
            "Train: epoch 0018 / 0100 | batch 0037 / 0050 | loss 0.0546\n",
            "Train: epoch 0018 / 0100 | batch 0038 / 0050 | loss 0.0543\n",
            "Train: epoch 0018 / 0100 | batch 0039 / 0050 | loss 0.0539\n",
            "Train: epoch 0018 / 0100 | batch 0040 / 0050 | loss 0.0535\n",
            "Train: epoch 0018 / 0100 | batch 0041 / 0050 | loss 0.0531\n",
            "Train: epoch 0018 / 0100 | batch 0042 / 0050 | loss 0.0535\n",
            "Train: epoch 0018 / 0100 | batch 0043 / 0050 | loss 0.0529\n",
            "Train: epoch 0018 / 0100 | batch 0044 / 0050 | loss 0.0526\n",
            "Train: epoch 0018 / 0100 | batch 0045 / 0050 | loss 0.0530\n",
            "Train: epoch 0018 / 0100 | batch 0046 / 0050 | loss 0.0532\n",
            "Train: epoch 0018 / 0100 | batch 0047 / 0050 | loss 0.0531\n",
            "Train: epoch 0018 / 0100 | batch 0048 / 0050 | loss 0.0534\n",
            "Train: epoch 0018 / 0100 | batch 0049 / 0050 | loss 0.0531\n",
            "Val loss 0.0465\n",
            "Dice score : 0.045383963733911514\n",
            "Val loss 0.0433\n",
            "Dice score : 0.035039693117141724\n",
            "Val loss 0.0427\n",
            "Dice score : 0.03263697773218155\n",
            "Val loss 0.0454\n",
            "Dice score : 0.04624322056770325\n",
            "Val loss 0.0462\n",
            "Dice score : 0.05240800604224205\n",
            "Val loss 0.0514\n",
            "Dice score : 0.0615476556122303\n",
            "Val loss 0.0498\n",
            "Dice score : 0.0331866517663002\n",
            "Val loss 0.0517\n",
            "Dice score : 0.036223798990249634\n",
            "Val loss 0.0536\n",
            "Dice score : 0.07415231317281723\n",
            "Val loss 0.0553\n",
            "Dice score : 0.06659431010484695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 18/100 [22:00<1:39:22, 72.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0019 / 0100 | batch 0000 / 0050 | loss 0.0491\n",
            "Train: epoch 0019 / 0100 | batch 0001 / 0050 | loss 0.0461\n",
            "Train: epoch 0019 / 0100 | batch 0002 / 0050 | loss 0.0474\n",
            "Train: epoch 0019 / 0100 | batch 0003 / 0050 | loss 0.0545\n",
            "Train: epoch 0019 / 0100 | batch 0004 / 0050 | loss 0.0560\n",
            "Train: epoch 0019 / 0100 | batch 0005 / 0050 | loss 0.0592\n",
            "Train: epoch 0019 / 0100 | batch 0006 / 0050 | loss 0.0582\n",
            "Train: epoch 0019 / 0100 | batch 0007 / 0050 | loss 0.0587\n",
            "Train: epoch 0019 / 0100 | batch 0008 / 0050 | loss 0.0598\n",
            "Train: epoch 0019 / 0100 | batch 0009 / 0050 | loss 0.0598\n",
            "Train: epoch 0019 / 0100 | batch 0010 / 0050 | loss 0.0578\n",
            "Train: epoch 0019 / 0100 | batch 0011 / 0050 | loss 0.0561\n",
            "Train: epoch 0019 / 0100 | batch 0012 / 0050 | loss 0.0549\n",
            "Train: epoch 0019 / 0100 | batch 0013 / 0050 | loss 0.0542\n",
            "Train: epoch 0019 / 0100 | batch 0014 / 0050 | loss 0.0536\n",
            "Train: epoch 0019 / 0100 | batch 0015 / 0050 | loss 0.0525\n",
            "Train: epoch 0019 / 0100 | batch 0016 / 0050 | loss 0.0514\n",
            "Train: epoch 0019 / 0100 | batch 0017 / 0050 | loss 0.0520\n",
            "Train: epoch 0019 / 0100 | batch 0018 / 0050 | loss 0.0519\n",
            "Train: epoch 0019 / 0100 | batch 0019 / 0050 | loss 0.0513\n",
            "Train: epoch 0019 / 0100 | batch 0020 / 0050 | loss 0.0516\n",
            "Train: epoch 0019 / 0100 | batch 0021 / 0050 | loss 0.0514\n",
            "Train: epoch 0019 / 0100 | batch 0022 / 0050 | loss 0.0514\n",
            "Train: epoch 0019 / 0100 | batch 0023 / 0050 | loss 0.0525\n",
            "Train: epoch 0019 / 0100 | batch 0024 / 0050 | loss 0.0522\n",
            "Train: epoch 0019 / 0100 | batch 0025 / 0050 | loss 0.0523\n",
            "Train: epoch 0019 / 0100 | batch 0026 / 0050 | loss 0.0518\n",
            "Train: epoch 0019 / 0100 | batch 0027 / 0050 | loss 0.0526\n",
            "Train: epoch 0019 / 0100 | batch 0028 / 0050 | loss 0.0525\n",
            "Train: epoch 0019 / 0100 | batch 0029 / 0050 | loss 0.0528\n",
            "Train: epoch 0019 / 0100 | batch 0030 / 0050 | loss 0.0528\n",
            "Train: epoch 0019 / 0100 | batch 0031 / 0050 | loss 0.0525\n",
            "Train: epoch 0019 / 0100 | batch 0032 / 0050 | loss 0.0528\n",
            "Train: epoch 0019 / 0100 | batch 0033 / 0050 | loss 0.0527\n",
            "Train: epoch 0019 / 0100 | batch 0034 / 0050 | loss 0.0537\n",
            "Train: epoch 0019 / 0100 | batch 0035 / 0050 | loss 0.0539\n",
            "Train: epoch 0019 / 0100 | batch 0036 / 0050 | loss 0.0541\n",
            "Train: epoch 0019 / 0100 | batch 0037 / 0050 | loss 0.0538\n",
            "Train: epoch 0019 / 0100 | batch 0038 / 0050 | loss 0.0536\n",
            "Train: epoch 0019 / 0100 | batch 0039 / 0050 | loss 0.0536\n",
            "Train: epoch 0019 / 0100 | batch 0040 / 0050 | loss 0.0534\n",
            "Train: epoch 0019 / 0100 | batch 0041 / 0050 | loss 0.0531\n",
            "Train: epoch 0019 / 0100 | batch 0042 / 0050 | loss 0.0529\n",
            "Train: epoch 0019 / 0100 | batch 0043 / 0050 | loss 0.0527\n",
            "Train: epoch 0019 / 0100 | batch 0044 / 0050 | loss 0.0524\n",
            "Train: epoch 0019 / 0100 | batch 0045 / 0050 | loss 0.0525\n",
            "Train: epoch 0019 / 0100 | batch 0046 / 0050 | loss 0.0524\n",
            "Train: epoch 0019 / 0100 | batch 0047 / 0050 | loss 0.0521\n",
            "Train: epoch 0019 / 0100 | batch 0048 / 0050 | loss 0.0520\n",
            "Train: epoch 0019 / 0100 | batch 0049 / 0050 | loss 0.0526\n",
            "Val loss 0.0401\n",
            "Dice score : 0.044980958104133606\n",
            "Val loss 0.0482\n",
            "Dice score : 0.045072998851537704\n",
            "Val loss 0.0551\n",
            "Dice score : 0.09300931543111801\n",
            "Val loss 0.0577\n",
            "Dice score : 0.05145767331123352\n",
            "Val loss 0.0539\n",
            "Dice score : 0.034040071070194244\n",
            "Val loss 0.0521\n",
            "Dice score : 0.056091222912073135\n",
            "Val loss 0.0513\n",
            "Dice score : 0.06860236078500748\n",
            "Val loss 0.0531\n",
            "Dice score : 0.04077833890914917\n",
            "Val loss 0.0550\n",
            "Dice score : 0.06814248859882355\n",
            "Val loss 0.0558\n",
            "Dice score : 0.041361551731824875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 19/100 [23:13<1:38:02, 72.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0020 / 0100 | batch 0000 / 0050 | loss 0.0416\n",
            "Train: epoch 0020 / 0100 | batch 0001 / 0050 | loss 0.0397\n",
            "Train: epoch 0020 / 0100 | batch 0002 / 0050 | loss 0.0413\n",
            "Train: epoch 0020 / 0100 | batch 0003 / 0050 | loss 0.0521\n",
            "Train: epoch 0020 / 0100 | batch 0004 / 0050 | loss 0.0507\n",
            "Train: epoch 0020 / 0100 | batch 0005 / 0050 | loss 0.0524\n",
            "Train: epoch 0020 / 0100 | batch 0006 / 0050 | loss 0.0507\n",
            "Train: epoch 0020 / 0100 | batch 0007 / 0050 | loss 0.0529\n",
            "Train: epoch 0020 / 0100 | batch 0008 / 0050 | loss 0.0533\n",
            "Train: epoch 0020 / 0100 | batch 0009 / 0050 | loss 0.0532\n",
            "Train: epoch 0020 / 0100 | batch 0010 / 0050 | loss 0.0521\n",
            "Train: epoch 0020 / 0100 | batch 0011 / 0050 | loss 0.0530\n",
            "Train: epoch 0020 / 0100 | batch 0012 / 0050 | loss 0.0526\n",
            "Train: epoch 0020 / 0100 | batch 0013 / 0050 | loss 0.0523\n",
            "Train: epoch 0020 / 0100 | batch 0014 / 0050 | loss 0.0558\n",
            "Train: epoch 0020 / 0100 | batch 0015 / 0050 | loss 0.0555\n",
            "Train: epoch 0020 / 0100 | batch 0016 / 0050 | loss 0.0556\n",
            "Train: epoch 0020 / 0100 | batch 0017 / 0050 | loss 0.0558\n",
            "Train: epoch 0020 / 0100 | batch 0018 / 0050 | loss 0.0549\n",
            "Train: epoch 0020 / 0100 | batch 0019 / 0050 | loss 0.0541\n",
            "Train: epoch 0020 / 0100 | batch 0020 / 0050 | loss 0.0548\n",
            "Train: epoch 0020 / 0100 | batch 0021 / 0050 | loss 0.0538\n",
            "Train: epoch 0020 / 0100 | batch 0022 / 0050 | loss 0.0531\n",
            "Train: epoch 0020 / 0100 | batch 0023 / 0050 | loss 0.0538\n",
            "Train: epoch 0020 / 0100 | batch 0024 / 0050 | loss 0.0542\n",
            "Train: epoch 0020 / 0100 | batch 0025 / 0050 | loss 0.0541\n",
            "Train: epoch 0020 / 0100 | batch 0026 / 0050 | loss 0.0541\n",
            "Train: epoch 0020 / 0100 | batch 0027 / 0050 | loss 0.0538\n",
            "Train: epoch 0020 / 0100 | batch 0028 / 0050 | loss 0.0537\n",
            "Train: epoch 0020 / 0100 | batch 0029 / 0050 | loss 0.0541\n",
            "Train: epoch 0020 / 0100 | batch 0030 / 0050 | loss 0.0537\n",
            "Train: epoch 0020 / 0100 | batch 0031 / 0050 | loss 0.0537\n",
            "Train: epoch 0020 / 0100 | batch 0032 / 0050 | loss 0.0533\n",
            "Train: epoch 0020 / 0100 | batch 0033 / 0050 | loss 0.0531\n",
            "Train: epoch 0020 / 0100 | batch 0034 / 0050 | loss 0.0531\n",
            "Train: epoch 0020 / 0100 | batch 0035 / 0050 | loss 0.0540\n",
            "Train: epoch 0020 / 0100 | batch 0036 / 0050 | loss 0.0540\n",
            "Train: epoch 0020 / 0100 | batch 0037 / 0050 | loss 0.0538\n",
            "Train: epoch 0020 / 0100 | batch 0038 / 0050 | loss 0.0536\n",
            "Train: epoch 0020 / 0100 | batch 0039 / 0050 | loss 0.0547\n",
            "Train: epoch 0020 / 0100 | batch 0040 / 0050 | loss 0.0543\n",
            "Train: epoch 0020 / 0100 | batch 0041 / 0050 | loss 0.0540\n",
            "Train: epoch 0020 / 0100 | batch 0042 / 0050 | loss 0.0537\n",
            "Train: epoch 0020 / 0100 | batch 0043 / 0050 | loss 0.0537\n",
            "Train: epoch 0020 / 0100 | batch 0044 / 0050 | loss 0.0533\n",
            "Train: epoch 0020 / 0100 | batch 0045 / 0050 | loss 0.0531\n",
            "Train: epoch 0020 / 0100 | batch 0046 / 0050 | loss 0.0527\n",
            "Train: epoch 0020 / 0100 | batch 0047 / 0050 | loss 0.0525\n",
            "Train: epoch 0020 / 0100 | batch 0048 / 0050 | loss 0.0525\n",
            "Train: epoch 0020 / 0100 | batch 0049 / 0050 | loss 0.0524\n",
            "Val loss 0.0679\n",
            "Dice score : 0.029528310522437096\n",
            "Val loss 0.0565\n",
            "Dice score : 0.05979287251830101\n",
            "Val loss 0.0569\n",
            "Dice score : 0.043946702033281326\n",
            "Val loss 0.0526\n",
            "Dice score : 0.028145048767328262\n",
            "Val loss 0.0513\n",
            "Dice score : 0.058760300278663635\n",
            "Val loss 0.0548\n",
            "Dice score : 0.04645238444209099\n",
            "Val loss 0.0530\n",
            "Dice score : 0.04624129831790924\n",
            "Val loss 0.0582\n",
            "Dice score : 0.08612363785505295\n",
            "Val loss 0.0602\n",
            "Dice score : 0.06483624130487442\n",
            "Val loss 0.0586\n",
            "Dice score : 0.03074565716087818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 20/100 [24:27<1:37:16, 72.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0021 / 0100 | batch 0000 / 0050 | loss 0.0651\n",
            "Train: epoch 0021 / 0100 | batch 0001 / 0050 | loss 0.0504\n",
            "Train: epoch 0021 / 0100 | batch 0002 / 0050 | loss 0.0514\n",
            "Train: epoch 0021 / 0100 | batch 0003 / 0050 | loss 0.0510\n",
            "Train: epoch 0021 / 0100 | batch 0004 / 0050 | loss 0.0491\n",
            "Train: epoch 0021 / 0100 | batch 0005 / 0050 | loss 0.0487\n",
            "Train: epoch 0021 / 0100 | batch 0006 / 0050 | loss 0.0544\n",
            "Train: epoch 0021 / 0100 | batch 0007 / 0050 | loss 0.0541\n",
            "Train: epoch 0021 / 0100 | batch 0008 / 0050 | loss 0.0559\n",
            "Train: epoch 0021 / 0100 | batch 0009 / 0050 | loss 0.0580\n",
            "Train: epoch 0021 / 0100 | batch 0010 / 0050 | loss 0.0575\n",
            "Train: epoch 0021 / 0100 | batch 0011 / 0050 | loss 0.0597\n",
            "Train: epoch 0021 / 0100 | batch 0012 / 0050 | loss 0.0589\n",
            "Train: epoch 0021 / 0100 | batch 0013 / 0050 | loss 0.0584\n",
            "Train: epoch 0021 / 0100 | batch 0014 / 0050 | loss 0.0585\n",
            "Train: epoch 0021 / 0100 | batch 0015 / 0050 | loss 0.0578\n",
            "Train: epoch 0021 / 0100 | batch 0016 / 0050 | loss 0.0566\n",
            "Train: epoch 0021 / 0100 | batch 0017 / 0050 | loss 0.0569\n",
            "Train: epoch 0021 / 0100 | batch 0018 / 0050 | loss 0.0561\n",
            "Train: epoch 0021 / 0100 | batch 0019 / 0050 | loss 0.0557\n",
            "Train: epoch 0021 / 0100 | batch 0020 / 0050 | loss 0.0567\n",
            "Train: epoch 0021 / 0100 | batch 0021 / 0050 | loss 0.0570\n",
            "Train: epoch 0021 / 0100 | batch 0022 / 0050 | loss 0.0563\n",
            "Train: epoch 0021 / 0100 | batch 0023 / 0050 | loss 0.0556\n",
            "Train: epoch 0021 / 0100 | batch 0024 / 0050 | loss 0.0553\n",
            "Train: epoch 0021 / 0100 | batch 0025 / 0050 | loss 0.0546\n",
            "Train: epoch 0021 / 0100 | batch 0026 / 0050 | loss 0.0546\n",
            "Train: epoch 0021 / 0100 | batch 0027 / 0050 | loss 0.0545\n",
            "Train: epoch 0021 / 0100 | batch 0028 / 0050 | loss 0.0541\n",
            "Train: epoch 0021 / 0100 | batch 0029 / 0050 | loss 0.0533\n",
            "Train: epoch 0021 / 0100 | batch 0030 / 0050 | loss 0.0528\n",
            "Train: epoch 0021 / 0100 | batch 0031 / 0050 | loss 0.0526\n",
            "Train: epoch 0021 / 0100 | batch 0032 / 0050 | loss 0.0523\n",
            "Train: epoch 0021 / 0100 | batch 0033 / 0050 | loss 0.0522\n",
            "Train: epoch 0021 / 0100 | batch 0034 / 0050 | loss 0.0522\n",
            "Train: epoch 0021 / 0100 | batch 0035 / 0050 | loss 0.0529\n",
            "Train: epoch 0021 / 0100 | batch 0036 / 0050 | loss 0.0532\n",
            "Train: epoch 0021 / 0100 | batch 0037 / 0050 | loss 0.0528\n",
            "Train: epoch 0021 / 0100 | batch 0038 / 0050 | loss 0.0529\n",
            "Train: epoch 0021 / 0100 | batch 0039 / 0050 | loss 0.0527\n",
            "Train: epoch 0021 / 0100 | batch 0040 / 0050 | loss 0.0530\n",
            "Train: epoch 0021 / 0100 | batch 0041 / 0050 | loss 0.0530\n",
            "Train: epoch 0021 / 0100 | batch 0042 / 0050 | loss 0.0528\n",
            "Train: epoch 0021 / 0100 | batch 0043 / 0050 | loss 0.0525\n",
            "Train: epoch 0021 / 0100 | batch 0044 / 0050 | loss 0.0524\n",
            "Train: epoch 0021 / 0100 | batch 0045 / 0050 | loss 0.0520\n",
            "Train: epoch 0021 / 0100 | batch 0046 / 0050 | loss 0.0521\n",
            "Train: epoch 0021 / 0100 | batch 0047 / 0050 | loss 0.0521\n",
            "Train: epoch 0021 / 0100 | batch 0048 / 0050 | loss 0.0521\n",
            "Train: epoch 0021 / 0100 | batch 0049 / 0050 | loss 0.0520\n",
            "Val loss 0.0471\n",
            "Dice score : 0.05600682646036148\n",
            "Val loss 0.0438\n",
            "Dice score : 0.056400299072265625\n",
            "Val loss 0.0504\n",
            "Dice score : 0.050938233733177185\n",
            "Val loss 0.0537\n",
            "Dice score : 0.062121279537677765\n",
            "Val loss 0.0563\n",
            "Dice score : 0.06982604414224625\n",
            "Val loss 0.0533\n",
            "Dice score : 0.04103884473443031\n",
            "Val loss 0.0556\n",
            "Dice score : 0.06335711479187012\n",
            "Val loss 0.0560\n",
            "Dice score : 0.06468471139669418\n",
            "Val loss 0.0565\n",
            "Dice score : 0.05181024596095085\n",
            "Val loss 0.0544\n",
            "Dice score : 0.0350341834127903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 21/100 [25:39<1:35:56, 72.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0022 / 0100 | batch 0000 / 0050 | loss 0.0437\n",
            "Train: epoch 0022 / 0100 | batch 0001 / 0050 | loss 0.0498\n",
            "Train: epoch 0022 / 0100 | batch 0002 / 0050 | loss 0.0572\n",
            "Train: epoch 0022 / 0100 | batch 0003 / 0050 | loss 0.0524\n",
            "Train: epoch 0022 / 0100 | batch 0004 / 0050 | loss 0.0535\n",
            "Train: epoch 0022 / 0100 | batch 0005 / 0050 | loss 0.0534\n",
            "Train: epoch 0022 / 0100 | batch 0006 / 0050 | loss 0.0522\n",
            "Train: epoch 0022 / 0100 | batch 0007 / 0050 | loss 0.0507\n",
            "Train: epoch 0022 / 0100 | batch 0008 / 0050 | loss 0.0503\n",
            "Train: epoch 0022 / 0100 | batch 0009 / 0050 | loss 0.0496\n",
            "Train: epoch 0022 / 0100 | batch 0010 / 0050 | loss 0.0522\n",
            "Train: epoch 0022 / 0100 | batch 0011 / 0050 | loss 0.0522\n",
            "Train: epoch 0022 / 0100 | batch 0012 / 0050 | loss 0.0519\n",
            "Train: epoch 0022 / 0100 | batch 0013 / 0050 | loss 0.0512\n",
            "Train: epoch 0022 / 0100 | batch 0014 / 0050 | loss 0.0504\n",
            "Train: epoch 0022 / 0100 | batch 0015 / 0050 | loss 0.0498\n",
            "Train: epoch 0022 / 0100 | batch 0016 / 0050 | loss 0.0506\n",
            "Train: epoch 0022 / 0100 | batch 0017 / 0050 | loss 0.0499\n",
            "Train: epoch 0022 / 0100 | batch 0018 / 0050 | loss 0.0490\n",
            "Train: epoch 0022 / 0100 | batch 0019 / 0050 | loss 0.0487\n",
            "Train: epoch 0022 / 0100 | batch 0020 / 0050 | loss 0.0486\n",
            "Train: epoch 0022 / 0100 | batch 0021 / 0050 | loss 0.0484\n",
            "Train: epoch 0022 / 0100 | batch 0022 / 0050 | loss 0.0492\n",
            "Train: epoch 0022 / 0100 | batch 0023 / 0050 | loss 0.0498\n",
            "Train: epoch 0022 / 0100 | batch 0024 / 0050 | loss 0.0502\n",
            "Train: epoch 0022 / 0100 | batch 0025 / 0050 | loss 0.0501\n",
            "Train: epoch 0022 / 0100 | batch 0026 / 0050 | loss 0.0501\n",
            "Train: epoch 0022 / 0100 | batch 0027 / 0050 | loss 0.0506\n",
            "Train: epoch 0022 / 0100 | batch 0028 / 0050 | loss 0.0519\n",
            "Train: epoch 0022 / 0100 | batch 0029 / 0050 | loss 0.0522\n",
            "Train: epoch 0022 / 0100 | batch 0030 / 0050 | loss 0.0518\n",
            "Train: epoch 0022 / 0100 | batch 0031 / 0050 | loss 0.0514\n",
            "Train: epoch 0022 / 0100 | batch 0032 / 0050 | loss 0.0514\n",
            "Train: epoch 0022 / 0100 | batch 0033 / 0050 | loss 0.0515\n",
            "Train: epoch 0022 / 0100 | batch 0034 / 0050 | loss 0.0514\n",
            "Train: epoch 0022 / 0100 | batch 0035 / 0050 | loss 0.0520\n",
            "Train: epoch 0022 / 0100 | batch 0036 / 0050 | loss 0.0528\n",
            "Train: epoch 0022 / 0100 | batch 0037 / 0050 | loss 0.0525\n",
            "Train: epoch 0022 / 0100 | batch 0038 / 0050 | loss 0.0522\n",
            "Train: epoch 0022 / 0100 | batch 0039 / 0050 | loss 0.0518\n",
            "Train: epoch 0022 / 0100 | batch 0040 / 0050 | loss 0.0523\n",
            "Train: epoch 0022 / 0100 | batch 0041 / 0050 | loss 0.0520\n",
            "Train: epoch 0022 / 0100 | batch 0042 / 0050 | loss 0.0520\n",
            "Train: epoch 0022 / 0100 | batch 0043 / 0050 | loss 0.0521\n",
            "Train: epoch 0022 / 0100 | batch 0044 / 0050 | loss 0.0518\n",
            "Train: epoch 0022 / 0100 | batch 0045 / 0050 | loss 0.0514\n",
            "Train: epoch 0022 / 0100 | batch 0046 / 0050 | loss 0.0517\n",
            "Train: epoch 0022 / 0100 | batch 0047 / 0050 | loss 0.0515\n",
            "Train: epoch 0022 / 0100 | batch 0048 / 0050 | loss 0.0519\n",
            "Train: epoch 0022 / 0100 | batch 0049 / 0050 | loss 0.0516\n",
            "Val loss 0.0783\n",
            "Dice score : 0.0812392383813858\n",
            "Val loss 0.0627\n",
            "Dice score : 0.06040780991315842\n",
            "Val loss 0.0678\n",
            "Dice score : 0.07604648172855377\n",
            "Val loss 0.0617\n",
            "Dice score : 0.04021673649549484\n",
            "Val loss 0.0605\n",
            "Dice score : 0.08107861876487732\n",
            "Val loss 0.0581\n",
            "Dice score : 0.06202821061015129\n",
            "Val loss 0.0630\n",
            "Dice score : 0.047456834465265274\n",
            "Val loss 0.0605\n",
            "Dice score : 0.03538370132446289\n",
            "Val loss 0.0620\n",
            "Dice score : 0.05243054777383804\n",
            "Val loss 0.0631\n",
            "Dice score : 0.046750299632549286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 22/100 [26:51<1:34:27, 72.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0023 / 0100 | batch 0000 / 0050 | loss 0.0465\n",
            "Train: epoch 0023 / 0100 | batch 0001 / 0050 | loss 0.0565\n",
            "Train: epoch 0023 / 0100 | batch 0002 / 0050 | loss 0.0513\n",
            "Train: epoch 0023 / 0100 | batch 0003 / 0050 | loss 0.0598\n",
            "Train: epoch 0023 / 0100 | batch 0004 / 0050 | loss 0.0591\n",
            "Train: epoch 0023 / 0100 | batch 0005 / 0050 | loss 0.0577\n",
            "Train: epoch 0023 / 0100 | batch 0006 / 0050 | loss 0.0556\n",
            "Train: epoch 0023 / 0100 | batch 0007 / 0050 | loss 0.0558\n",
            "Train: epoch 0023 / 0100 | batch 0008 / 0050 | loss 0.0552\n",
            "Train: epoch 0023 / 0100 | batch 0009 / 0050 | loss 0.0535\n",
            "Train: epoch 0023 / 0100 | batch 0010 / 0050 | loss 0.0531\n",
            "Train: epoch 0023 / 0100 | batch 0011 / 0050 | loss 0.0524\n",
            "Train: epoch 0023 / 0100 | batch 0012 / 0050 | loss 0.0511\n",
            "Train: epoch 0023 / 0100 | batch 0013 / 0050 | loss 0.0512\n",
            "Train: epoch 0023 / 0100 | batch 0014 / 0050 | loss 0.0509\n",
            "Train: epoch 0023 / 0100 | batch 0015 / 0050 | loss 0.0529\n",
            "Train: epoch 0023 / 0100 | batch 0016 / 0050 | loss 0.0526\n",
            "Train: epoch 0023 / 0100 | batch 0017 / 0050 | loss 0.0526\n",
            "Train: epoch 0023 / 0100 | batch 0018 / 0050 | loss 0.0519\n",
            "Train: epoch 0023 / 0100 | batch 0019 / 0050 | loss 0.0524\n",
            "Train: epoch 0023 / 0100 | batch 0020 / 0050 | loss 0.0516\n",
            "Train: epoch 0023 / 0100 | batch 0021 / 0050 | loss 0.0518\n",
            "Train: epoch 0023 / 0100 | batch 0022 / 0050 | loss 0.0514\n",
            "Train: epoch 0023 / 0100 | batch 0023 / 0050 | loss 0.0515\n",
            "Train: epoch 0023 / 0100 | batch 0024 / 0050 | loss 0.0508\n",
            "Train: epoch 0023 / 0100 | batch 0025 / 0050 | loss 0.0515\n",
            "Train: epoch 0023 / 0100 | batch 0026 / 0050 | loss 0.0515\n",
            "Train: epoch 0023 / 0100 | batch 0027 / 0050 | loss 0.0519\n",
            "Train: epoch 0023 / 0100 | batch 0028 / 0050 | loss 0.0517\n",
            "Train: epoch 0023 / 0100 | batch 0029 / 0050 | loss 0.0528\n",
            "Train: epoch 0023 / 0100 | batch 0030 / 0050 | loss 0.0526\n",
            "Train: epoch 0023 / 0100 | batch 0031 / 0050 | loss 0.0531\n",
            "Train: epoch 0023 / 0100 | batch 0032 / 0050 | loss 0.0527\n",
            "Train: epoch 0023 / 0100 | batch 0033 / 0050 | loss 0.0530\n",
            "Train: epoch 0023 / 0100 | batch 0034 / 0050 | loss 0.0529\n",
            "Train: epoch 0023 / 0100 | batch 0035 / 0050 | loss 0.0526\n",
            "Train: epoch 0023 / 0100 | batch 0036 / 0050 | loss 0.0522\n",
            "Train: epoch 0023 / 0100 | batch 0037 / 0050 | loss 0.0522\n",
            "Train: epoch 0023 / 0100 | batch 0038 / 0050 | loss 0.0521\n",
            "Train: epoch 0023 / 0100 | batch 0039 / 0050 | loss 0.0520\n",
            "Train: epoch 0023 / 0100 | batch 0040 / 0050 | loss 0.0518\n",
            "Train: epoch 0023 / 0100 | batch 0041 / 0050 | loss 0.0518\n",
            "Train: epoch 0023 / 0100 | batch 0042 / 0050 | loss 0.0517\n",
            "Train: epoch 0023 / 0100 | batch 0043 / 0050 | loss 0.0516\n",
            "Train: epoch 0023 / 0100 | batch 0044 / 0050 | loss 0.0513\n",
            "Train: epoch 0023 / 0100 | batch 0045 / 0050 | loss 0.0508\n",
            "Train: epoch 0023 / 0100 | batch 0046 / 0050 | loss 0.0511\n",
            "Train: epoch 0023 / 0100 | batch 0047 / 0050 | loss 0.0513\n",
            "Train: epoch 0023 / 0100 | batch 0048 / 0050 | loss 0.0511\n",
            "Train: epoch 0023 / 0100 | batch 0049 / 0050 | loss 0.0517\n",
            "Val loss 0.0722\n",
            "Dice score : 0.06432537734508514\n",
            "Val loss 0.0605\n",
            "Dice score : 0.05232731252908707\n",
            "Val loss 0.0614\n",
            "Dice score : 0.0323646143078804\n",
            "Val loss 0.0568\n",
            "Dice score : 0.03780362755060196\n",
            "Val loss 0.0535\n",
            "Dice score : 0.05140436068177223\n",
            "Val loss 0.0527\n",
            "Dice score : 0.05013393983244896\n",
            "Val loss 0.0583\n",
            "Dice score : 0.07222938537597656\n",
            "Val loss 0.0561\n",
            "Dice score : 0.029650595039129257\n",
            "Val loss 0.0551\n",
            "Dice score : 0.051046621054410934\n",
            "Val loss 0.0553\n",
            "Dice score : 0.08777828514575958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 23/100 [28:05<1:33:30, 72.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0024 / 0100 | batch 0000 / 0050 | loss 0.0433\n",
            "Train: epoch 0024 / 0100 | batch 0001 / 0050 | loss 0.0443\n",
            "Train: epoch 0024 / 0100 | batch 0002 / 0050 | loss 0.0416\n",
            "Train: epoch 0024 / 0100 | batch 0003 / 0050 | loss 0.0423\n",
            "Train: epoch 0024 / 0100 | batch 0004 / 0050 | loss 0.0457\n",
            "Train: epoch 0024 / 0100 | batch 0005 / 0050 | loss 0.0490\n",
            "Train: epoch 0024 / 0100 | batch 0006 / 0050 | loss 0.0490\n",
            "Train: epoch 0024 / 0100 | batch 0007 / 0050 | loss 0.0489\n",
            "Train: epoch 0024 / 0100 | batch 0008 / 0050 | loss 0.0529\n",
            "Train: epoch 0024 / 0100 | batch 0009 / 0050 | loss 0.0516\n",
            "Train: epoch 0024 / 0100 | batch 0010 / 0050 | loss 0.0513\n",
            "Train: epoch 0024 / 0100 | batch 0011 / 0050 | loss 0.0511\n",
            "Train: epoch 0024 / 0100 | batch 0012 / 0050 | loss 0.0515\n",
            "Train: epoch 0024 / 0100 | batch 0013 / 0050 | loss 0.0514\n",
            "Train: epoch 0024 / 0100 | batch 0014 / 0050 | loss 0.0505\n",
            "Train: epoch 0024 / 0100 | batch 0015 / 0050 | loss 0.0521\n",
            "Train: epoch 0024 / 0100 | batch 0016 / 0050 | loss 0.0525\n",
            "Train: epoch 0024 / 0100 | batch 0017 / 0050 | loss 0.0532\n",
            "Train: epoch 0024 / 0100 | batch 0018 / 0050 | loss 0.0532\n",
            "Train: epoch 0024 / 0100 | batch 0019 / 0050 | loss 0.0524\n",
            "Train: epoch 0024 / 0100 | batch 0020 / 0050 | loss 0.0528\n",
            "Train: epoch 0024 / 0100 | batch 0021 / 0050 | loss 0.0531\n",
            "Train: epoch 0024 / 0100 | batch 0022 / 0050 | loss 0.0530\n",
            "Train: epoch 0024 / 0100 | batch 0023 / 0050 | loss 0.0536\n",
            "Train: epoch 0024 / 0100 | batch 0024 / 0050 | loss 0.0531\n",
            "Train: epoch 0024 / 0100 | batch 0025 / 0050 | loss 0.0543\n",
            "Train: epoch 0024 / 0100 | batch 0026 / 0050 | loss 0.0539\n",
            "Train: epoch 0024 / 0100 | batch 0027 / 0050 | loss 0.0536\n",
            "Train: epoch 0024 / 0100 | batch 0028 / 0050 | loss 0.0539\n",
            "Train: epoch 0024 / 0100 | batch 0029 / 0050 | loss 0.0540\n",
            "Train: epoch 0024 / 0100 | batch 0030 / 0050 | loss 0.0536\n",
            "Train: epoch 0024 / 0100 | batch 0031 / 0050 | loss 0.0541\n",
            "Train: epoch 0024 / 0100 | batch 0032 / 0050 | loss 0.0540\n",
            "Train: epoch 0024 / 0100 | batch 0033 / 0050 | loss 0.0535\n",
            "Train: epoch 0024 / 0100 | batch 0034 / 0050 | loss 0.0536\n",
            "Train: epoch 0024 / 0100 | batch 0035 / 0050 | loss 0.0535\n",
            "Train: epoch 0024 / 0100 | batch 0036 / 0050 | loss 0.0531\n",
            "Train: epoch 0024 / 0100 | batch 0037 / 0050 | loss 0.0535\n",
            "Train: epoch 0024 / 0100 | batch 0038 / 0050 | loss 0.0531\n",
            "Train: epoch 0024 / 0100 | batch 0039 / 0050 | loss 0.0531\n",
            "Train: epoch 0024 / 0100 | batch 0040 / 0050 | loss 0.0535\n",
            "Train: epoch 0024 / 0100 | batch 0041 / 0050 | loss 0.0531\n",
            "Train: epoch 0024 / 0100 | batch 0042 / 0050 | loss 0.0529\n",
            "Train: epoch 0024 / 0100 | batch 0043 / 0050 | loss 0.0530\n",
            "Train: epoch 0024 / 0100 | batch 0044 / 0050 | loss 0.0532\n",
            "Train: epoch 0024 / 0100 | batch 0045 / 0050 | loss 0.0529\n",
            "Train: epoch 0024 / 0100 | batch 0046 / 0050 | loss 0.0526\n",
            "Train: epoch 0024 / 0100 | batch 0047 / 0050 | loss 0.0523\n",
            "Train: epoch 0024 / 0100 | batch 0048 / 0050 | loss 0.0519\n",
            "Train: epoch 0024 / 0100 | batch 0049 / 0050 | loss 0.0517\n",
            "Val loss 0.0425\n",
            "Dice score : 0.05761704593896866\n",
            "Val loss 0.0673\n",
            "Dice score : 0.06901559233665466\n",
            "Val loss 0.0580\n",
            "Dice score : 0.04092402756214142\n",
            "Val loss 0.0520\n",
            "Dice score : 0.024235161021351814\n",
            "Val loss 0.0573\n",
            "Dice score : 0.05804147571325302\n",
            "Val loss 0.0539\n",
            "Dice score : 0.061682991683483124\n",
            "Val loss 0.0525\n",
            "Dice score : 0.046321820467710495\n",
            "Val loss 0.0510\n",
            "Dice score : 0.04955942556262016\n",
            "Val loss 0.0526\n",
            "Dice score : 0.07594922930002213\n",
            "Val loss 0.0544\n",
            "Dice score : 0.06485942006111145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 24/100 [29:18<1:32:29, 73.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0025 / 0100 | batch 0000 / 0050 | loss 0.0831\n",
            "Train: epoch 0025 / 0100 | batch 0001 / 0050 | loss 0.0658\n",
            "Train: epoch 0025 / 0100 | batch 0002 / 0050 | loss 0.0612\n",
            "Train: epoch 0025 / 0100 | batch 0003 / 0050 | loss 0.0596\n",
            "Train: epoch 0025 / 0100 | batch 0004 / 0050 | loss 0.0585\n",
            "Train: epoch 0025 / 0100 | batch 0005 / 0050 | loss 0.0597\n",
            "Train: epoch 0025 / 0100 | batch 0006 / 0050 | loss 0.0572\n",
            "Train: epoch 0025 / 0100 | batch 0007 / 0050 | loss 0.0544\n",
            "Train: epoch 0025 / 0100 | batch 0008 / 0050 | loss 0.0532\n",
            "Train: epoch 0025 / 0100 | batch 0009 / 0050 | loss 0.0534\n",
            "Train: epoch 0025 / 0100 | batch 0010 / 0050 | loss 0.0521\n",
            "Train: epoch 0025 / 0100 | batch 0011 / 0050 | loss 0.0520\n",
            "Train: epoch 0025 / 0100 | batch 0012 / 0050 | loss 0.0522\n",
            "Train: epoch 0025 / 0100 | batch 0013 / 0050 | loss 0.0515\n",
            "Train: epoch 0025 / 0100 | batch 0014 / 0050 | loss 0.0521\n",
            "Train: epoch 0025 / 0100 | batch 0015 / 0050 | loss 0.0545\n",
            "Train: epoch 0025 / 0100 | batch 0016 / 0050 | loss 0.0542\n",
            "Train: epoch 0025 / 0100 | batch 0017 / 0050 | loss 0.0539\n",
            "Train: epoch 0025 / 0100 | batch 0018 / 0050 | loss 0.0540\n",
            "Train: epoch 0025 / 0100 | batch 0019 / 0050 | loss 0.0548\n",
            "Train: epoch 0025 / 0100 | batch 0020 / 0050 | loss 0.0541\n",
            "Train: epoch 0025 / 0100 | batch 0021 / 0050 | loss 0.0545\n",
            "Train: epoch 0025 / 0100 | batch 0022 / 0050 | loss 0.0540\n",
            "Train: epoch 0025 / 0100 | batch 0023 / 0050 | loss 0.0541\n",
            "Train: epoch 0025 / 0100 | batch 0024 / 0050 | loss 0.0534\n",
            "Train: epoch 0025 / 0100 | batch 0025 / 0050 | loss 0.0538\n",
            "Train: epoch 0025 / 0100 | batch 0026 / 0050 | loss 0.0540\n",
            "Train: epoch 0025 / 0100 | batch 0027 / 0050 | loss 0.0537\n",
            "Train: epoch 0025 / 0100 | batch 0028 / 0050 | loss 0.0534\n",
            "Train: epoch 0025 / 0100 | batch 0029 / 0050 | loss 0.0532\n",
            "Train: epoch 0025 / 0100 | batch 0030 / 0050 | loss 0.0542\n",
            "Train: epoch 0025 / 0100 | batch 0031 / 0050 | loss 0.0540\n",
            "Train: epoch 0025 / 0100 | batch 0032 / 0050 | loss 0.0542\n",
            "Train: epoch 0025 / 0100 | batch 0033 / 0050 | loss 0.0539\n",
            "Train: epoch 0025 / 0100 | batch 0034 / 0050 | loss 0.0539\n",
            "Train: epoch 0025 / 0100 | batch 0035 / 0050 | loss 0.0534\n",
            "Train: epoch 0025 / 0100 | batch 0036 / 0050 | loss 0.0531\n",
            "Train: epoch 0025 / 0100 | batch 0037 / 0050 | loss 0.0528\n",
            "Train: epoch 0025 / 0100 | batch 0038 / 0050 | loss 0.0529\n",
            "Train: epoch 0025 / 0100 | batch 0039 / 0050 | loss 0.0524\n",
            "Train: epoch 0025 / 0100 | batch 0040 / 0050 | loss 0.0520\n",
            "Train: epoch 0025 / 0100 | batch 0041 / 0050 | loss 0.0518\n",
            "Train: epoch 0025 / 0100 | batch 0042 / 0050 | loss 0.0518\n",
            "Train: epoch 0025 / 0100 | batch 0043 / 0050 | loss 0.0514\n",
            "Train: epoch 0025 / 0100 | batch 0044 / 0050 | loss 0.0517\n",
            "Train: epoch 0025 / 0100 | batch 0045 / 0050 | loss 0.0517\n",
            "Train: epoch 0025 / 0100 | batch 0046 / 0050 | loss 0.0516\n",
            "Train: epoch 0025 / 0100 | batch 0047 / 0050 | loss 0.0515\n",
            "Train: epoch 0025 / 0100 | batch 0048 / 0050 | loss 0.0516\n",
            "Train: epoch 0025 / 0100 | batch 0049 / 0050 | loss 0.0515\n",
            "Val loss 0.0435\n",
            "Dice score : 0.026910219341516495\n",
            "Val loss 0.0459\n",
            "Dice score : 0.041589707136154175\n",
            "Val loss 0.0442\n",
            "Dice score : 0.04931359365582466\n",
            "Val loss 0.0414\n",
            "Dice score : 0.03481833636760712\n",
            "Val loss 0.0505\n",
            "Dice score : 0.04690967872738838\n",
            "Val loss 0.0561\n",
            "Dice score : 0.06568509340286255\n",
            "Val loss 0.0583\n",
            "Dice score : 0.054020583629608154\n",
            "Val loss 0.0591\n",
            "Dice score : 0.09347350895404816\n",
            "Val loss 0.0578\n",
            "Dice score : 0.04559280350804329\n",
            "Val loss 0.0570\n",
            "Dice score : 0.041754283010959625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 25/100 [30:30<1:30:59, 72.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0026 / 0100 | batch 0000 / 0050 | loss 0.0404\n",
            "Train: epoch 0026 / 0100 | batch 0001 / 0050 | loss 0.0545\n",
            "Train: epoch 0026 / 0100 | batch 0002 / 0050 | loss 0.0539\n",
            "Train: epoch 0026 / 0100 | batch 0003 / 0050 | loss 0.0580\n",
            "Train: epoch 0026 / 0100 | batch 0004 / 0050 | loss 0.0546\n",
            "Train: epoch 0026 / 0100 | batch 0005 / 0050 | loss 0.0537\n",
            "Train: epoch 0026 / 0100 | batch 0006 / 0050 | loss 0.0515\n",
            "Train: epoch 0026 / 0100 | batch 0007 / 0050 | loss 0.0502\n",
            "Train: epoch 0026 / 0100 | batch 0008 / 0050 | loss 0.0495\n",
            "Train: epoch 0026 / 0100 | batch 0009 / 0050 | loss 0.0502\n",
            "Train: epoch 0026 / 0100 | batch 0010 / 0050 | loss 0.0494\n",
            "Train: epoch 0026 / 0100 | batch 0011 / 0050 | loss 0.0488\n",
            "Train: epoch 0026 / 0100 | batch 0012 / 0050 | loss 0.0484\n",
            "Train: epoch 0026 / 0100 | batch 0013 / 0050 | loss 0.0505\n",
            "Train: epoch 0026 / 0100 | batch 0014 / 0050 | loss 0.0501\n",
            "Train: epoch 0026 / 0100 | batch 0015 / 0050 | loss 0.0506\n",
            "Train: epoch 0026 / 0100 | batch 0016 / 0050 | loss 0.0523\n",
            "Train: epoch 0026 / 0100 | batch 0017 / 0050 | loss 0.0516\n",
            "Train: epoch 0026 / 0100 | batch 0018 / 0050 | loss 0.0511\n",
            "Train: epoch 0026 / 0100 | batch 0019 / 0050 | loss 0.0506\n",
            "Train: epoch 0026 / 0100 | batch 0020 / 0050 | loss 0.0516\n",
            "Train: epoch 0026 / 0100 | batch 0021 / 0050 | loss 0.0521\n",
            "Train: epoch 0026 / 0100 | batch 0022 / 0050 | loss 0.0517\n",
            "Train: epoch 0026 / 0100 | batch 0023 / 0050 | loss 0.0512\n",
            "Train: epoch 0026 / 0100 | batch 0024 / 0050 | loss 0.0517\n",
            "Train: epoch 0026 / 0100 | batch 0025 / 0050 | loss 0.0518\n",
            "Train: epoch 0026 / 0100 | batch 0026 / 0050 | loss 0.0521\n",
            "Train: epoch 0026 / 0100 | batch 0027 / 0050 | loss 0.0517\n",
            "Train: epoch 0026 / 0100 | batch 0028 / 0050 | loss 0.0514\n",
            "Train: epoch 0026 / 0100 | batch 0029 / 0050 | loss 0.0509\n",
            "Train: epoch 0026 / 0100 | batch 0030 / 0050 | loss 0.0508\n",
            "Train: epoch 0026 / 0100 | batch 0031 / 0050 | loss 0.0505\n",
            "Train: epoch 0026 / 0100 | batch 0032 / 0050 | loss 0.0508\n",
            "Train: epoch 0026 / 0100 | batch 0033 / 0050 | loss 0.0513\n",
            "Train: epoch 0026 / 0100 | batch 0034 / 0050 | loss 0.0515\n",
            "Train: epoch 0026 / 0100 | batch 0035 / 0050 | loss 0.0521\n",
            "Train: epoch 0026 / 0100 | batch 0036 / 0050 | loss 0.0516\n",
            "Train: epoch 0026 / 0100 | batch 0037 / 0050 | loss 0.0513\n",
            "Train: epoch 0026 / 0100 | batch 0038 / 0050 | loss 0.0512\n",
            "Train: epoch 0026 / 0100 | batch 0039 / 0050 | loss 0.0511\n",
            "Train: epoch 0026 / 0100 | batch 0040 / 0050 | loss 0.0511\n",
            "Train: epoch 0026 / 0100 | batch 0041 / 0050 | loss 0.0513\n",
            "Train: epoch 0026 / 0100 | batch 0042 / 0050 | loss 0.0509\n",
            "Train: epoch 0026 / 0100 | batch 0043 / 0050 | loss 0.0507\n",
            "Train: epoch 0026 / 0100 | batch 0044 / 0050 | loss 0.0508\n",
            "Train: epoch 0026 / 0100 | batch 0045 / 0050 | loss 0.0507\n",
            "Train: epoch 0026 / 0100 | batch 0046 / 0050 | loss 0.0505\n",
            "Train: epoch 0026 / 0100 | batch 0047 / 0050 | loss 0.0507\n",
            "Train: epoch 0026 / 0100 | batch 0048 / 0050 | loss 0.0505\n",
            "Train: epoch 0026 / 0100 | batch 0049 / 0050 | loss 0.0514\n",
            "Val loss 0.0579\n",
            "Dice score : 0.05095396935939789\n",
            "Val loss 0.0607\n",
            "Dice score : 0.038135647773742676\n",
            "Val loss 0.0577\n",
            "Dice score : 0.04884937033057213\n",
            "Val loss 0.0540\n",
            "Dice score : 0.030173204839229584\n",
            "Val loss 0.0574\n",
            "Dice score : 0.06189960241317749\n",
            "Val loss 0.0619\n",
            "Dice score : 0.07806796580553055\n",
            "Val loss 0.0588\n",
            "Dice score : 0.0465124249458313\n",
            "Val loss 0.0570\n",
            "Dice score : 0.029397476464509964\n",
            "Val loss 0.0560\n",
            "Dice score : 0.056551236659288406\n",
            "Val loss 0.0571\n",
            "Dice score : 0.0844205692410469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 26/100 [31:43<1:29:52, 72.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0027 / 0100 | batch 0000 / 0050 | loss 0.0436\n",
            "Train: epoch 0027 / 0100 | batch 0001 / 0050 | loss 0.0436\n",
            "Train: epoch 0027 / 0100 | batch 0002 / 0050 | loss 0.0449\n",
            "Train: epoch 0027 / 0100 | batch 0003 / 0050 | loss 0.0445\n",
            "Train: epoch 0027 / 0100 | batch 0004 / 0050 | loss 0.0488\n",
            "Train: epoch 0027 / 0100 | batch 0005 / 0050 | loss 0.0487\n",
            "Train: epoch 0027 / 0100 | batch 0006 / 0050 | loss 0.0477\n",
            "Train: epoch 0027 / 0100 | batch 0007 / 0050 | loss 0.0489\n",
            "Train: epoch 0027 / 0100 | batch 0008 / 0050 | loss 0.0523\n",
            "Train: epoch 0027 / 0100 | batch 0009 / 0050 | loss 0.0505\n",
            "Train: epoch 0027 / 0100 | batch 0010 / 0050 | loss 0.0505\n",
            "Train: epoch 0027 / 0100 | batch 0011 / 0050 | loss 0.0516\n",
            "Train: epoch 0027 / 0100 | batch 0012 / 0050 | loss 0.0511\n",
            "Train: epoch 0027 / 0100 | batch 0013 / 0050 | loss 0.0513\n",
            "Train: epoch 0027 / 0100 | batch 0014 / 0050 | loss 0.0505\n",
            "Train: epoch 0027 / 0100 | batch 0015 / 0050 | loss 0.0500\n",
            "Train: epoch 0027 / 0100 | batch 0016 / 0050 | loss 0.0502\n",
            "Train: epoch 0027 / 0100 | batch 0017 / 0050 | loss 0.0508\n",
            "Train: epoch 0027 / 0100 | batch 0018 / 0050 | loss 0.0522\n",
            "Train: epoch 0027 / 0100 | batch 0019 / 0050 | loss 0.0521\n",
            "Train: epoch 0027 / 0100 | batch 0020 / 0050 | loss 0.0516\n",
            "Train: epoch 0027 / 0100 | batch 0021 / 0050 | loss 0.0511\n",
            "Train: epoch 0027 / 0100 | batch 0022 / 0050 | loss 0.0508\n",
            "Train: epoch 0027 / 0100 | batch 0023 / 0050 | loss 0.0504\n",
            "Train: epoch 0027 / 0100 | batch 0024 / 0050 | loss 0.0498\n",
            "Train: epoch 0027 / 0100 | batch 0025 / 0050 | loss 0.0510\n",
            "Train: epoch 0027 / 0100 | batch 0026 / 0050 | loss 0.0505\n",
            "Train: epoch 0027 / 0100 | batch 0027 / 0050 | loss 0.0499\n",
            "Train: epoch 0027 / 0100 | batch 0028 / 0050 | loss 0.0502\n",
            "Train: epoch 0027 / 0100 | batch 0029 / 0050 | loss 0.0501\n",
            "Train: epoch 0027 / 0100 | batch 0030 / 0050 | loss 0.0500\n",
            "Train: epoch 0027 / 0100 | batch 0031 / 0050 | loss 0.0507\n",
            "Train: epoch 0027 / 0100 | batch 0032 / 0050 | loss 0.0503\n",
            "Train: epoch 0027 / 0100 | batch 0033 / 0050 | loss 0.0499\n",
            "Train: epoch 0027 / 0100 | batch 0034 / 0050 | loss 0.0504\n",
            "Train: epoch 0027 / 0100 | batch 0035 / 0050 | loss 0.0508\n",
            "Train: epoch 0027 / 0100 | batch 0036 / 0050 | loss 0.0512\n",
            "Train: epoch 0027 / 0100 | batch 0037 / 0050 | loss 0.0512\n",
            "Train: epoch 0027 / 0100 | batch 0038 / 0050 | loss 0.0512\n",
            "Train: epoch 0027 / 0100 | batch 0039 / 0050 | loss 0.0513\n",
            "Train: epoch 0027 / 0100 | batch 0040 / 0050 | loss 0.0510\n",
            "Train: epoch 0027 / 0100 | batch 0041 / 0050 | loss 0.0511\n",
            "Train: epoch 0027 / 0100 | batch 0042 / 0050 | loss 0.0510\n",
            "Train: epoch 0027 / 0100 | batch 0043 / 0050 | loss 0.0507\n",
            "Train: epoch 0027 / 0100 | batch 0044 / 0050 | loss 0.0508\n",
            "Train: epoch 0027 / 0100 | batch 0045 / 0050 | loss 0.0506\n",
            "Train: epoch 0027 / 0100 | batch 0046 / 0050 | loss 0.0506\n",
            "Train: epoch 0027 / 0100 | batch 0047 / 0050 | loss 0.0506\n",
            "Train: epoch 0027 / 0100 | batch 0048 / 0050 | loss 0.0507\n",
            "Train: epoch 0027 / 0100 | batch 0049 / 0050 | loss 0.0506\n",
            "Val loss 0.0601\n",
            "Dice score : 0.06342919170856476\n",
            "Val loss 0.0545\n",
            "Dice score : 0.06921758502721786\n",
            "Val loss 0.0564\n",
            "Dice score : 0.03351793810725212\n",
            "Val loss 0.0527\n",
            "Dice score : 0.03222690150141716\n",
            "Val loss 0.0526\n",
            "Dice score : 0.04710785299539566\n",
            "Val loss 0.0573\n",
            "Dice score : 0.08241058886051178\n",
            "Val loss 0.0598\n",
            "Dice score : 0.0377047024667263\n",
            "Val loss 0.0580\n",
            "Dice score : 0.028956715017557144\n",
            "Val loss 0.0562\n",
            "Dice score : 0.05628511682152748\n",
            "Val loss 0.0551\n",
            "Dice score : 0.055831581354141235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 27/100 [32:57<1:28:46, 72.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0028 / 0100 | batch 0000 / 0050 | loss 0.0406\n",
            "Train: epoch 0028 / 0100 | batch 0001 / 0050 | loss 0.0408\n",
            "Train: epoch 0028 / 0100 | batch 0002 / 0050 | loss 0.0486\n",
            "Train: epoch 0028 / 0100 | batch 0003 / 0050 | loss 0.0477\n",
            "Train: epoch 0028 / 0100 | batch 0004 / 0050 | loss 0.0526\n",
            "Train: epoch 0028 / 0100 | batch 0005 / 0050 | loss 0.0558\n",
            "Train: epoch 0028 / 0100 | batch 0006 / 0050 | loss 0.0516\n",
            "Train: epoch 0028 / 0100 | batch 0007 / 0050 | loss 0.0498\n",
            "Train: epoch 0028 / 0100 | batch 0008 / 0050 | loss 0.0507\n",
            "Train: epoch 0028 / 0100 | batch 0009 / 0050 | loss 0.0507\n",
            "Train: epoch 0028 / 0100 | batch 0010 / 0050 | loss 0.0494\n",
            "Train: epoch 0028 / 0100 | batch 0011 / 0050 | loss 0.0491\n",
            "Train: epoch 0028 / 0100 | batch 0012 / 0050 | loss 0.0486\n",
            "Train: epoch 0028 / 0100 | batch 0013 / 0050 | loss 0.0489\n",
            "Train: epoch 0028 / 0100 | batch 0014 / 0050 | loss 0.0485\n",
            "Train: epoch 0028 / 0100 | batch 0015 / 0050 | loss 0.0485\n",
            "Train: epoch 0028 / 0100 | batch 0016 / 0050 | loss 0.0480\n",
            "Train: epoch 0028 / 0100 | batch 0017 / 0050 | loss 0.0476\n",
            "Train: epoch 0028 / 0100 | batch 0018 / 0050 | loss 0.0498\n",
            "Train: epoch 0028 / 0100 | batch 0019 / 0050 | loss 0.0506\n",
            "Train: epoch 0028 / 0100 | batch 0020 / 0050 | loss 0.0504\n",
            "Train: epoch 0028 / 0100 | batch 0021 / 0050 | loss 0.0509\n",
            "Train: epoch 0028 / 0100 | batch 0022 / 0050 | loss 0.0511\n",
            "Train: epoch 0028 / 0100 | batch 0023 / 0050 | loss 0.0508\n",
            "Train: epoch 0028 / 0100 | batch 0024 / 0050 | loss 0.0501\n",
            "Train: epoch 0028 / 0100 | batch 0025 / 0050 | loss 0.0497\n",
            "Train: epoch 0028 / 0100 | batch 0026 / 0050 | loss 0.0500\n",
            "Train: epoch 0028 / 0100 | batch 0027 / 0050 | loss 0.0501\n",
            "Train: epoch 0028 / 0100 | batch 0028 / 0050 | loss 0.0505\n",
            "Train: epoch 0028 / 0100 | batch 0029 / 0050 | loss 0.0510\n",
            "Train: epoch 0028 / 0100 | batch 0030 / 0050 | loss 0.0508\n",
            "Train: epoch 0028 / 0100 | batch 0031 / 0050 | loss 0.0509\n",
            "Train: epoch 0028 / 0100 | batch 0032 / 0050 | loss 0.0505\n",
            "Train: epoch 0028 / 0100 | batch 0033 / 0050 | loss 0.0504\n",
            "Train: epoch 0028 / 0100 | batch 0034 / 0050 | loss 0.0500\n",
            "Train: epoch 0028 / 0100 | batch 0035 / 0050 | loss 0.0501\n",
            "Train: epoch 0028 / 0100 | batch 0036 / 0050 | loss 0.0500\n",
            "Train: epoch 0028 / 0100 | batch 0037 / 0050 | loss 0.0499\n",
            "Train: epoch 0028 / 0100 | batch 0038 / 0050 | loss 0.0500\n",
            "Train: epoch 0028 / 0100 | batch 0039 / 0050 | loss 0.0497\n",
            "Train: epoch 0028 / 0100 | batch 0040 / 0050 | loss 0.0498\n",
            "Train: epoch 0028 / 0100 | batch 0041 / 0050 | loss 0.0498\n",
            "Train: epoch 0028 / 0100 | batch 0042 / 0050 | loss 0.0493\n",
            "Train: epoch 0028 / 0100 | batch 0043 / 0050 | loss 0.0494\n",
            "Train: epoch 0028 / 0100 | batch 0044 / 0050 | loss 0.0498\n",
            "Train: epoch 0028 / 0100 | batch 0045 / 0050 | loss 0.0503\n",
            "Train: epoch 0028 / 0100 | batch 0046 / 0050 | loss 0.0503\n",
            "Train: epoch 0028 / 0100 | batch 0047 / 0050 | loss 0.0501\n",
            "Train: epoch 0028 / 0100 | batch 0048 / 0050 | loss 0.0505\n",
            "Train: epoch 0028 / 0100 | batch 0049 / 0050 | loss 0.0505\n",
            "Val loss 0.0428\n",
            "Dice score : 0.03734461963176727\n",
            "Val loss 0.0532\n",
            "Dice score : 0.03140267729759216\n",
            "Val loss 0.0540\n",
            "Dice score : 0.02601608820259571\n",
            "Val loss 0.0536\n",
            "Dice score : 0.050416164100170135\n",
            "Val loss 0.0505\n",
            "Dice score : 0.054891712963581085\n",
            "Val loss 0.0490\n",
            "Dice score : 0.06922411173582077\n",
            "Val loss 0.0498\n",
            "Dice score : 0.0712059885263443\n",
            "Val loss 0.0523\n",
            "Dice score : 0.07628133147954941\n",
            "Val loss 0.0509\n",
            "Dice score : 0.030568767338991165\n",
            "Val loss 0.0530\n",
            "Dice score : 0.07117679715156555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 28/100 [34:11<1:28:07, 73.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0029 / 0100 | batch 0000 / 0050 | loss 0.0805\n",
            "Train: epoch 0029 / 0100 | batch 0001 / 0050 | loss 0.0730\n",
            "Train: epoch 0029 / 0100 | batch 0002 / 0050 | loss 0.0704\n",
            "Train: epoch 0029 / 0100 | batch 0003 / 0050 | loss 0.0675\n",
            "Train: epoch 0029 / 0100 | batch 0004 / 0050 | loss 0.0622\n",
            "Train: epoch 0029 / 0100 | batch 0005 / 0050 | loss 0.0650\n",
            "Train: epoch 0029 / 0100 | batch 0006 / 0050 | loss 0.0622\n",
            "Train: epoch 0029 / 0100 | batch 0007 / 0050 | loss 0.0604\n",
            "Train: epoch 0029 / 0100 | batch 0008 / 0050 | loss 0.0581\n",
            "Train: epoch 0029 / 0100 | batch 0009 / 0050 | loss 0.0567\n",
            "Train: epoch 0029 / 0100 | batch 0010 / 0050 | loss 0.0559\n",
            "Train: epoch 0029 / 0100 | batch 0011 / 0050 | loss 0.0540\n",
            "Train: epoch 0029 / 0100 | batch 0012 / 0050 | loss 0.0547\n",
            "Train: epoch 0029 / 0100 | batch 0013 / 0050 | loss 0.0533\n",
            "Train: epoch 0029 / 0100 | batch 0014 / 0050 | loss 0.0527\n",
            "Train: epoch 0029 / 0100 | batch 0015 / 0050 | loss 0.0519\n",
            "Train: epoch 0029 / 0100 | batch 0016 / 0050 | loss 0.0522\n",
            "Train: epoch 0029 / 0100 | batch 0017 / 0050 | loss 0.0518\n",
            "Train: epoch 0029 / 0100 | batch 0018 / 0050 | loss 0.0512\n",
            "Train: epoch 0029 / 0100 | batch 0019 / 0050 | loss 0.0512\n",
            "Train: epoch 0029 / 0100 | batch 0020 / 0050 | loss 0.0522\n",
            "Train: epoch 0029 / 0100 | batch 0021 / 0050 | loss 0.0516\n",
            "Train: epoch 0029 / 0100 | batch 0022 / 0050 | loss 0.0511\n",
            "Train: epoch 0029 / 0100 | batch 0023 / 0050 | loss 0.0505\n",
            "Train: epoch 0029 / 0100 | batch 0024 / 0050 | loss 0.0502\n",
            "Train: epoch 0029 / 0100 | batch 0025 / 0050 | loss 0.0504\n",
            "Train: epoch 0029 / 0100 | batch 0026 / 0050 | loss 0.0503\n",
            "Train: epoch 0029 / 0100 | batch 0027 / 0050 | loss 0.0500\n",
            "Train: epoch 0029 / 0100 | batch 0028 / 0050 | loss 0.0500\n",
            "Train: epoch 0029 / 0100 | batch 0029 / 0050 | loss 0.0498\n",
            "Train: epoch 0029 / 0100 | batch 0030 / 0050 | loss 0.0501\n",
            "Train: epoch 0029 / 0100 | batch 0031 / 0050 | loss 0.0500\n",
            "Train: epoch 0029 / 0100 | batch 0032 / 0050 | loss 0.0503\n",
            "Train: epoch 0029 / 0100 | batch 0033 / 0050 | loss 0.0504\n",
            "Train: epoch 0029 / 0100 | batch 0034 / 0050 | loss 0.0503\n",
            "Train: epoch 0029 / 0100 | batch 0035 / 0050 | loss 0.0501\n",
            "Train: epoch 0029 / 0100 | batch 0036 / 0050 | loss 0.0499\n",
            "Train: epoch 0029 / 0100 | batch 0037 / 0050 | loss 0.0502\n",
            "Train: epoch 0029 / 0100 | batch 0038 / 0050 | loss 0.0502\n",
            "Train: epoch 0029 / 0100 | batch 0039 / 0050 | loss 0.0506\n",
            "Train: epoch 0029 / 0100 | batch 0040 / 0050 | loss 0.0505\n",
            "Train: epoch 0029 / 0100 | batch 0041 / 0050 | loss 0.0501\n",
            "Train: epoch 0029 / 0100 | batch 0042 / 0050 | loss 0.0503\n",
            "Train: epoch 0029 / 0100 | batch 0043 / 0050 | loss 0.0510\n",
            "Train: epoch 0029 / 0100 | batch 0044 / 0050 | loss 0.0511\n",
            "Train: epoch 0029 / 0100 | batch 0045 / 0050 | loss 0.0513\n",
            "Train: epoch 0029 / 0100 | batch 0046 / 0050 | loss 0.0513\n",
            "Train: epoch 0029 / 0100 | batch 0047 / 0050 | loss 0.0509\n",
            "Train: epoch 0029 / 0100 | batch 0048 / 0050 | loss 0.0507\n",
            "Train: epoch 0029 / 0100 | batch 0049 / 0050 | loss 0.0504\n",
            "Val loss 0.0401\n",
            "Dice score : 0.02751513011753559\n",
            "Val loss 0.0396\n",
            "Dice score : 0.044582050293684006\n",
            "Val loss 0.0526\n",
            "Dice score : 0.07208152115345001\n",
            "Val loss 0.0497\n",
            "Dice score : 0.05569114908576012\n",
            "Val loss 0.0532\n",
            "Dice score : 0.07390390336513519\n",
            "Val loss 0.0525\n",
            "Dice score : 0.05253922939300537\n",
            "Val loss 0.0508\n",
            "Dice score : 0.04978395625948906\n",
            "Val loss 0.0504\n",
            "Dice score : 0.04651602357625961\n",
            "Val loss 0.0551\n",
            "Dice score : 0.07639487087726593\n",
            "Val loss 0.0543\n",
            "Dice score : 0.054855454713106155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 29/100 [35:23<1:26:15, 72.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0030 / 0100 | batch 0000 / 0050 | loss 0.0414\n",
            "Train: epoch 0030 / 0100 | batch 0001 / 0050 | loss 0.0463\n",
            "Train: epoch 0030 / 0100 | batch 0002 / 0050 | loss 0.0440\n",
            "Train: epoch 0030 / 0100 | batch 0003 / 0050 | loss 0.0450\n",
            "Train: epoch 0030 / 0100 | batch 0004 / 0050 | loss 0.0467\n",
            "Train: epoch 0030 / 0100 | batch 0005 / 0050 | loss 0.0482\n",
            "Train: epoch 0030 / 0100 | batch 0006 / 0050 | loss 0.0556\n",
            "Train: epoch 0030 / 0100 | batch 0007 / 0050 | loss 0.0561\n",
            "Train: epoch 0030 / 0100 | batch 0008 / 0050 | loss 0.0535\n",
            "Train: epoch 0030 / 0100 | batch 0009 / 0050 | loss 0.0532\n",
            "Train: epoch 0030 / 0100 | batch 0010 / 0050 | loss 0.0538\n",
            "Train: epoch 0030 / 0100 | batch 0011 / 0050 | loss 0.0543\n",
            "Train: epoch 0030 / 0100 | batch 0012 / 0050 | loss 0.0533\n",
            "Train: epoch 0030 / 0100 | batch 0013 / 0050 | loss 0.0528\n",
            "Train: epoch 0030 / 0100 | batch 0014 / 0050 | loss 0.0520\n",
            "Train: epoch 0030 / 0100 | batch 0015 / 0050 | loss 0.0516\n",
            "Train: epoch 0030 / 0100 | batch 0016 / 0050 | loss 0.0504\n",
            "Train: epoch 0030 / 0100 | batch 0017 / 0050 | loss 0.0502\n",
            "Train: epoch 0030 / 0100 | batch 0018 / 0050 | loss 0.0500\n",
            "Train: epoch 0030 / 0100 | batch 0019 / 0050 | loss 0.0506\n",
            "Train: epoch 0030 / 0100 | batch 0020 / 0050 | loss 0.0501\n",
            "Train: epoch 0030 / 0100 | batch 0021 / 0050 | loss 0.0497\n",
            "Train: epoch 0030 / 0100 | batch 0022 / 0050 | loss 0.0503\n",
            "Train: epoch 0030 / 0100 | batch 0023 / 0050 | loss 0.0510\n",
            "Train: epoch 0030 / 0100 | batch 0024 / 0050 | loss 0.0513\n",
            "Train: epoch 0030 / 0100 | batch 0025 / 0050 | loss 0.0523\n",
            "Train: epoch 0030 / 0100 | batch 0026 / 0050 | loss 0.0517\n",
            "Train: epoch 0030 / 0100 | batch 0027 / 0050 | loss 0.0522\n",
            "Train: epoch 0030 / 0100 | batch 0028 / 0050 | loss 0.0517\n",
            "Train: epoch 0030 / 0100 | batch 0029 / 0050 | loss 0.0513\n",
            "Train: epoch 0030 / 0100 | batch 0030 / 0050 | loss 0.0508\n",
            "Train: epoch 0030 / 0100 | batch 0031 / 0050 | loss 0.0510\n",
            "Train: epoch 0030 / 0100 | batch 0032 / 0050 | loss 0.0506\n",
            "Train: epoch 0030 / 0100 | batch 0033 / 0050 | loss 0.0510\n",
            "Train: epoch 0030 / 0100 | batch 0034 / 0050 | loss 0.0503\n",
            "Train: epoch 0030 / 0100 | batch 0035 / 0050 | loss 0.0501\n",
            "Train: epoch 0030 / 0100 | batch 0036 / 0050 | loss 0.0501\n",
            "Train: epoch 0030 / 0100 | batch 0037 / 0050 | loss 0.0500\n",
            "Train: epoch 0030 / 0100 | batch 0038 / 0050 | loss 0.0498\n",
            "Train: epoch 0030 / 0100 | batch 0039 / 0050 | loss 0.0499\n",
            "Train: epoch 0030 / 0100 | batch 0040 / 0050 | loss 0.0495\n",
            "Train: epoch 0030 / 0100 | batch 0041 / 0050 | loss 0.0495\n",
            "Train: epoch 0030 / 0100 | batch 0042 / 0050 | loss 0.0492\n",
            "Train: epoch 0030 / 0100 | batch 0043 / 0050 | loss 0.0493\n",
            "Train: epoch 0030 / 0100 | batch 0044 / 0050 | loss 0.0500\n",
            "Train: epoch 0030 / 0100 | batch 0045 / 0050 | loss 0.0498\n",
            "Train: epoch 0030 / 0100 | batch 0046 / 0050 | loss 0.0494\n",
            "Train: epoch 0030 / 0100 | batch 0047 / 0050 | loss 0.0498\n",
            "Train: epoch 0030 / 0100 | batch 0048 / 0050 | loss 0.0502\n",
            "Train: epoch 0030 / 0100 | batch 0049 / 0050 | loss 0.0503\n",
            "Val loss 0.0525\n",
            "Dice score : 0.05510317534208298\n",
            "Val loss 0.0474\n",
            "Dice score : 0.038912583142519\n",
            "Val loss 0.0479\n",
            "Dice score : 0.03621383383870125\n",
            "Val loss 0.0535\n",
            "Dice score : 0.05810253694653511\n",
            "Val loss 0.0537\n",
            "Dice score : 0.046502817422151566\n",
            "Val loss 0.0558\n",
            "Dice score : 0.06834916025400162\n",
            "Val loss 0.0538\n",
            "Dice score : 0.04416779428720474\n",
            "Val loss 0.0523\n",
            "Dice score : 0.02849605679512024\n",
            "Val loss 0.0528\n",
            "Dice score : 0.07643591612577438\n",
            "Val loss 0.0535\n",
            "Dice score : 0.04698682576417923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 30/100 [36:36<1:25:04, 72.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0031 / 0100 | batch 0000 / 0050 | loss 0.0364\n",
            "Train: epoch 0031 / 0100 | batch 0001 / 0050 | loss 0.0401\n",
            "Train: epoch 0031 / 0100 | batch 0002 / 0050 | loss 0.0435\n",
            "Train: epoch 0031 / 0100 | batch 0003 / 0050 | loss 0.0424\n",
            "Train: epoch 0031 / 0100 | batch 0004 / 0050 | loss 0.0445\n",
            "Train: epoch 0031 / 0100 | batch 0005 / 0050 | loss 0.0522\n",
            "Train: epoch 0031 / 0100 | batch 0006 / 0050 | loss 0.0525\n",
            "Train: epoch 0031 / 0100 | batch 0007 / 0050 | loss 0.0503\n",
            "Train: epoch 0031 / 0100 | batch 0008 / 0050 | loss 0.0502\n",
            "Train: epoch 0031 / 0100 | batch 0009 / 0050 | loss 0.0509\n",
            "Train: epoch 0031 / 0100 | batch 0010 / 0050 | loss 0.0497\n",
            "Train: epoch 0031 / 0100 | batch 0011 / 0050 | loss 0.0485\n",
            "Train: epoch 0031 / 0100 | batch 0012 / 0050 | loss 0.0500\n",
            "Train: epoch 0031 / 0100 | batch 0013 / 0050 | loss 0.0492\n",
            "Train: epoch 0031 / 0100 | batch 0014 / 0050 | loss 0.0494\n",
            "Train: epoch 0031 / 0100 | batch 0015 / 0050 | loss 0.0511\n",
            "Train: epoch 0031 / 0100 | batch 0016 / 0050 | loss 0.0507\n",
            "Train: epoch 0031 / 0100 | batch 0017 / 0050 | loss 0.0501\n",
            "Train: epoch 0031 / 0100 | batch 0018 / 0050 | loss 0.0506\n",
            "Train: epoch 0031 / 0100 | batch 0019 / 0050 | loss 0.0502\n",
            "Train: epoch 0031 / 0100 | batch 0020 / 0050 | loss 0.0501\n",
            "Train: epoch 0031 / 0100 | batch 0021 / 0050 | loss 0.0495\n",
            "Train: epoch 0031 / 0100 | batch 0022 / 0050 | loss 0.0490\n",
            "Train: epoch 0031 / 0100 | batch 0023 / 0050 | loss 0.0488\n",
            "Train: epoch 0031 / 0100 | batch 0024 / 0050 | loss 0.0495\n",
            "Train: epoch 0031 / 0100 | batch 0025 / 0050 | loss 0.0491\n",
            "Train: epoch 0031 / 0100 | batch 0026 / 0050 | loss 0.0486\n",
            "Train: epoch 0031 / 0100 | batch 0027 / 0050 | loss 0.0488\n",
            "Train: epoch 0031 / 0100 | batch 0028 / 0050 | loss 0.0487\n",
            "Train: epoch 0031 / 0100 | batch 0029 / 0050 | loss 0.0489\n",
            "Train: epoch 0031 / 0100 | batch 0030 / 0050 | loss 0.0498\n",
            "Train: epoch 0031 / 0100 | batch 0031 / 0050 | loss 0.0498\n",
            "Train: epoch 0031 / 0100 | batch 0032 / 0050 | loss 0.0498\n",
            "Train: epoch 0031 / 0100 | batch 0033 / 0050 | loss 0.0496\n",
            "Train: epoch 0031 / 0100 | batch 0034 / 0050 | loss 0.0497\n",
            "Train: epoch 0031 / 0100 | batch 0035 / 0050 | loss 0.0496\n",
            "Train: epoch 0031 / 0100 | batch 0036 / 0050 | loss 0.0495\n",
            "Train: epoch 0031 / 0100 | batch 0037 / 0050 | loss 0.0496\n",
            "Train: epoch 0031 / 0100 | batch 0038 / 0050 | loss 0.0501\n",
            "Train: epoch 0031 / 0100 | batch 0039 / 0050 | loss 0.0498\n",
            "Train: epoch 0031 / 0100 | batch 0040 / 0050 | loss 0.0496\n",
            "Train: epoch 0031 / 0100 | batch 0041 / 0050 | loss 0.0500\n",
            "Train: epoch 0031 / 0100 | batch 0042 / 0050 | loss 0.0498\n",
            "Train: epoch 0031 / 0100 | batch 0043 / 0050 | loss 0.0500\n",
            "Train: epoch 0031 / 0100 | batch 0044 / 0050 | loss 0.0497\n",
            "Train: epoch 0031 / 0100 | batch 0045 / 0050 | loss 0.0497\n",
            "Train: epoch 0031 / 0100 | batch 0046 / 0050 | loss 0.0497\n",
            "Train: epoch 0031 / 0100 | batch 0047 / 0050 | loss 0.0499\n",
            "Train: epoch 0031 / 0100 | batch 0048 / 0050 | loss 0.0502\n",
            "Train: epoch 0031 / 0100 | batch 0049 / 0050 | loss 0.0503\n",
            "Val loss 0.0379\n",
            "Dice score : 0.03370653837919235\n",
            "Val loss 0.0410\n",
            "Dice score : 0.053828440606594086\n",
            "Val loss 0.0410\n",
            "Dice score : 0.04969485476613045\n",
            "Val loss 0.0409\n",
            "Dice score : 0.062371380627155304\n",
            "Val loss 0.0410\n",
            "Dice score : 0.06394555419683456\n",
            "Val loss 0.0443\n",
            "Dice score : 0.07038555294275284\n",
            "Val loss 0.0464\n",
            "Dice score : 0.042782776057720184\n",
            "Val loss 0.0487\n",
            "Dice score : 0.053050924092531204\n",
            "Val loss 0.0500\n",
            "Dice score : 0.0713861957192421\n",
            "Val loss 0.0531\n",
            "Dice score : 0.0773695558309555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 31/100 [37:50<1:24:15, 73.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0032 / 0100 | batch 0000 / 0050 | loss 0.0733\n",
            "Train: epoch 0032 / 0100 | batch 0001 / 0050 | loss 0.0663\n",
            "Train: epoch 0032 / 0100 | batch 0002 / 0050 | loss 0.0562\n",
            "Train: epoch 0032 / 0100 | batch 0003 / 0050 | loss 0.0559\n",
            "Train: epoch 0032 / 0100 | batch 0004 / 0050 | loss 0.0551\n",
            "Train: epoch 0032 / 0100 | batch 0005 / 0050 | loss 0.0515\n",
            "Train: epoch 0032 / 0100 | batch 0006 / 0050 | loss 0.0532\n",
            "Train: epoch 0032 / 0100 | batch 0007 / 0050 | loss 0.0509\n",
            "Train: epoch 0032 / 0100 | batch 0008 / 0050 | loss 0.0521\n",
            "Train: epoch 0032 / 0100 | batch 0009 / 0050 | loss 0.0535\n",
            "Train: epoch 0032 / 0100 | batch 0010 / 0050 | loss 0.0523\n",
            "Train: epoch 0032 / 0100 | batch 0011 / 0050 | loss 0.0524\n",
            "Train: epoch 0032 / 0100 | batch 0012 / 0050 | loss 0.0512\n",
            "Train: epoch 0032 / 0100 | batch 0013 / 0050 | loss 0.0522\n",
            "Train: epoch 0032 / 0100 | batch 0014 / 0050 | loss 0.0526\n",
            "Train: epoch 0032 / 0100 | batch 0015 / 0050 | loss 0.0547\n",
            "Train: epoch 0032 / 0100 | batch 0016 / 0050 | loss 0.0559\n",
            "Train: epoch 0032 / 0100 | batch 0017 / 0050 | loss 0.0563\n",
            "Train: epoch 0032 / 0100 | batch 0018 / 0050 | loss 0.0553\n",
            "Train: epoch 0032 / 0100 | batch 0019 / 0050 | loss 0.0546\n",
            "Train: epoch 0032 / 0100 | batch 0020 / 0050 | loss 0.0545\n",
            "Train: epoch 0032 / 0100 | batch 0021 / 0050 | loss 0.0541\n",
            "Train: epoch 0032 / 0100 | batch 0022 / 0050 | loss 0.0544\n",
            "Train: epoch 0032 / 0100 | batch 0023 / 0050 | loss 0.0537\n",
            "Train: epoch 0032 / 0100 | batch 0024 / 0050 | loss 0.0531\n",
            "Train: epoch 0032 / 0100 | batch 0025 / 0050 | loss 0.0528\n",
            "Train: epoch 0032 / 0100 | batch 0026 / 0050 | loss 0.0523\n",
            "Train: epoch 0032 / 0100 | batch 0027 / 0050 | loss 0.0517\n",
            "Train: epoch 0032 / 0100 | batch 0028 / 0050 | loss 0.0521\n",
            "Train: epoch 0032 / 0100 | batch 0029 / 0050 | loss 0.0519\n",
            "Train: epoch 0032 / 0100 | batch 0030 / 0050 | loss 0.0511\n",
            "Train: epoch 0032 / 0100 | batch 0031 / 0050 | loss 0.0511\n",
            "Train: epoch 0032 / 0100 | batch 0032 / 0050 | loss 0.0513\n",
            "Train: epoch 0032 / 0100 | batch 0033 / 0050 | loss 0.0512\n",
            "Train: epoch 0032 / 0100 | batch 0034 / 0050 | loss 0.0513\n",
            "Train: epoch 0032 / 0100 | batch 0035 / 0050 | loss 0.0509\n",
            "Train: epoch 0032 / 0100 | batch 0036 / 0050 | loss 0.0506\n",
            "Train: epoch 0032 / 0100 | batch 0037 / 0050 | loss 0.0508\n",
            "Train: epoch 0032 / 0100 | batch 0038 / 0050 | loss 0.0507\n",
            "Train: epoch 0032 / 0100 | batch 0039 / 0050 | loss 0.0509\n",
            "Train: epoch 0032 / 0100 | batch 0040 / 0050 | loss 0.0507\n",
            "Train: epoch 0032 / 0100 | batch 0041 / 0050 | loss 0.0508\n",
            "Train: epoch 0032 / 0100 | batch 0042 / 0050 | loss 0.0505\n",
            "Train: epoch 0032 / 0100 | batch 0043 / 0050 | loss 0.0500\n",
            "Train: epoch 0032 / 0100 | batch 0044 / 0050 | loss 0.0497\n",
            "Train: epoch 0032 / 0100 | batch 0045 / 0050 | loss 0.0494\n",
            "Train: epoch 0032 / 0100 | batch 0046 / 0050 | loss 0.0493\n",
            "Train: epoch 0032 / 0100 | batch 0047 / 0050 | loss 0.0491\n",
            "Train: epoch 0032 / 0100 | batch 0048 / 0050 | loss 0.0489\n",
            "Train: epoch 0032 / 0100 | batch 0049 / 0050 | loss 0.0486\n",
            "Val loss 0.1053\n",
            "Dice score : 0.08203624188899994\n",
            "Val loss 0.0738\n",
            "Dice score : 0.04355273395776749\n",
            "Val loss 0.0652\n",
            "Dice score : 0.05750536546111107\n",
            "Val loss 0.0601\n",
            "Dice score : 0.03697185590863228\n",
            "Val loss 0.0628\n",
            "Dice score : 0.039855390787124634\n",
            "Val loss 0.0617\n",
            "Dice score : 0.04595325514674187\n",
            "Val loss 0.0594\n",
            "Dice score : 0.06024698540568352\n",
            "Val loss 0.0578\n",
            "Dice score : 0.04735241085290909\n",
            "Val loss 0.0581\n",
            "Dice score : 0.07585415989160538\n",
            "Val loss 0.0587\n",
            "Dice score : 0.06602156162261963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 32/100 [39:02<1:22:35, 72.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0033 / 0100 | batch 0000 / 0050 | loss 0.0456\n",
            "Train: epoch 0033 / 0100 | batch 0001 / 0050 | loss 0.0559\n",
            "Train: epoch 0033 / 0100 | batch 0002 / 0050 | loss 0.0511\n",
            "Train: epoch 0033 / 0100 | batch 0003 / 0050 | loss 0.0531\n",
            "Train: epoch 0033 / 0100 | batch 0004 / 0050 | loss 0.0549\n",
            "Train: epoch 0033 / 0100 | batch 0005 / 0050 | loss 0.0509\n",
            "Train: epoch 0033 / 0100 | batch 0006 / 0050 | loss 0.0495\n",
            "Train: epoch 0033 / 0100 | batch 0007 / 0050 | loss 0.0499\n",
            "Train: epoch 0033 / 0100 | batch 0008 / 0050 | loss 0.0480\n",
            "Train: epoch 0033 / 0100 | batch 0009 / 0050 | loss 0.0465\n",
            "Train: epoch 0033 / 0100 | batch 0010 / 0050 | loss 0.0452\n",
            "Train: epoch 0033 / 0100 | batch 0011 / 0050 | loss 0.0461\n",
            "Train: epoch 0033 / 0100 | batch 0012 / 0050 | loss 0.0478\n",
            "Train: epoch 0033 / 0100 | batch 0013 / 0050 | loss 0.0469\n",
            "Train: epoch 0033 / 0100 | batch 0014 / 0050 | loss 0.0476\n",
            "Train: epoch 0033 / 0100 | batch 0015 / 0050 | loss 0.0474\n",
            "Train: epoch 0033 / 0100 | batch 0016 / 0050 | loss 0.0469\n",
            "Train: epoch 0033 / 0100 | batch 0017 / 0050 | loss 0.0468\n",
            "Train: epoch 0033 / 0100 | batch 0018 / 0050 | loss 0.0463\n",
            "Train: epoch 0033 / 0100 | batch 0019 / 0050 | loss 0.0472\n",
            "Train: epoch 0033 / 0100 | batch 0020 / 0050 | loss 0.0475\n",
            "Train: epoch 0033 / 0100 | batch 0021 / 0050 | loss 0.0472\n",
            "Train: epoch 0033 / 0100 | batch 0022 / 0050 | loss 0.0479\n",
            "Train: epoch 0033 / 0100 | batch 0023 / 0050 | loss 0.0475\n",
            "Train: epoch 0033 / 0100 | batch 0024 / 0050 | loss 0.0480\n",
            "Train: epoch 0033 / 0100 | batch 0025 / 0050 | loss 0.0478\n",
            "Train: epoch 0033 / 0100 | batch 0026 / 0050 | loss 0.0475\n",
            "Train: epoch 0033 / 0100 | batch 0027 / 0050 | loss 0.0474\n",
            "Train: epoch 0033 / 0100 | batch 0028 / 0050 | loss 0.0478\n",
            "Train: epoch 0033 / 0100 | batch 0029 / 0050 | loss 0.0475\n",
            "Train: epoch 0033 / 0100 | batch 0030 / 0050 | loss 0.0474\n",
            "Train: epoch 0033 / 0100 | batch 0031 / 0050 | loss 0.0478\n",
            "Train: epoch 0033 / 0100 | batch 0032 / 0050 | loss 0.0474\n",
            "Train: epoch 0033 / 0100 | batch 0033 / 0050 | loss 0.0470\n",
            "Train: epoch 0033 / 0100 | batch 0034 / 0050 | loss 0.0473\n",
            "Train: epoch 0033 / 0100 | batch 0035 / 0050 | loss 0.0480\n",
            "Train: epoch 0033 / 0100 | batch 0036 / 0050 | loss 0.0480\n",
            "Train: epoch 0033 / 0100 | batch 0037 / 0050 | loss 0.0475\n",
            "Train: epoch 0033 / 0100 | batch 0038 / 0050 | loss 0.0471\n",
            "Train: epoch 0033 / 0100 | batch 0039 / 0050 | loss 0.0467\n",
            "Train: epoch 0033 / 0100 | batch 0040 / 0050 | loss 0.0466\n",
            "Train: epoch 0033 / 0100 | batch 0041 / 0050 | loss 0.0465\n",
            "Train: epoch 0033 / 0100 | batch 0042 / 0050 | loss 0.0464\n",
            "Train: epoch 0033 / 0100 | batch 0043 / 0050 | loss 0.0466\n",
            "Train: epoch 0033 / 0100 | batch 0044 / 0050 | loss 0.0464\n",
            "Train: epoch 0033 / 0100 | batch 0045 / 0050 | loss 0.0460\n",
            "Train: epoch 0033 / 0100 | batch 0046 / 0050 | loss 0.0462\n",
            "Train: epoch 0033 / 0100 | batch 0047 / 0050 | loss 0.0459\n",
            "Train: epoch 0033 / 0100 | batch 0048 / 0050 | loss 0.0458\n",
            "Train: epoch 0033 / 0100 | batch 0049 / 0050 | loss 0.0463\n",
            "Val loss 0.0452\n",
            "Dice score : 0.07593778520822525\n",
            "Val loss 0.0535\n",
            "Dice score : 0.07063751667737961\n",
            "Val loss 0.0518\n",
            "Dice score : 0.02364339865744114\n",
            "Val loss 0.0505\n",
            "Dice score : 0.05068845674395561\n",
            "Val loss 0.0506\n",
            "Dice score : 0.06950332969427109\n",
            "Val loss 0.0487\n",
            "Dice score : 0.053571127355098724\n",
            "Val loss 0.0480\n",
            "Dice score : 0.057410065084695816\n",
            "Val loss 0.0496\n",
            "Dice score : 0.06282522529363632\n",
            "Val loss 0.0511\n",
            "Dice score : 0.07966098189353943\n",
            "Val loss 0.0539\n",
            "Dice score : 0.07196219265460968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 33/100 [40:15<1:21:36, 73.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0034 / 0100 | batch 0000 / 0050 | loss 0.0388\n",
            "Train: epoch 0034 / 0100 | batch 0001 / 0050 | loss 0.0519\n",
            "Train: epoch 0034 / 0100 | batch 0002 / 0050 | loss 0.0467\n",
            "Train: epoch 0034 / 0100 | batch 0003 / 0050 | loss 0.0441\n",
            "Train: epoch 0034 / 0100 | batch 0004 / 0050 | loss 0.0433\n",
            "Train: epoch 0034 / 0100 | batch 0005 / 0050 | loss 0.0439\n",
            "Train: epoch 0034 / 0100 | batch 0006 / 0050 | loss 0.0428\n",
            "Train: epoch 0034 / 0100 | batch 0007 / 0050 | loss 0.0418\n",
            "Train: epoch 0034 / 0100 | batch 0008 / 0050 | loss 0.0436\n",
            "Train: epoch 0034 / 0100 | batch 0009 / 0050 | loss 0.0440\n",
            "Train: epoch 0034 / 0100 | batch 0010 / 0050 | loss 0.0431\n",
            "Train: epoch 0034 / 0100 | batch 0011 / 0050 | loss 0.0438\n",
            "Train: epoch 0034 / 0100 | batch 0012 / 0050 | loss 0.0440\n",
            "Train: epoch 0034 / 0100 | batch 0013 / 0050 | loss 0.0429\n",
            "Train: epoch 0034 / 0100 | batch 0014 / 0050 | loss 0.0441\n",
            "Train: epoch 0034 / 0100 | batch 0015 / 0050 | loss 0.0433\n",
            "Train: epoch 0034 / 0100 | batch 0016 / 0050 | loss 0.0430\n",
            "Train: epoch 0034 / 0100 | batch 0017 / 0050 | loss 0.0423\n",
            "Train: epoch 0034 / 0100 | batch 0018 / 0050 | loss 0.0433\n",
            "Train: epoch 0034 / 0100 | batch 0019 / 0050 | loss 0.0435\n",
            "Train: epoch 0034 / 0100 | batch 0020 / 0050 | loss 0.0433\n",
            "Train: epoch 0034 / 0100 | batch 0021 / 0050 | loss 0.0429\n",
            "Train: epoch 0034 / 0100 | batch 0022 / 0050 | loss 0.0431\n",
            "Train: epoch 0034 / 0100 | batch 0023 / 0050 | loss 0.0435\n",
            "Train: epoch 0034 / 0100 | batch 0024 / 0050 | loss 0.0438\n",
            "Train: epoch 0034 / 0100 | batch 0025 / 0050 | loss 0.0433\n",
            "Train: epoch 0034 / 0100 | batch 0026 / 0050 | loss 0.0433\n",
            "Train: epoch 0034 / 0100 | batch 0027 / 0050 | loss 0.0429\n",
            "Train: epoch 0034 / 0100 | batch 0028 / 0050 | loss 0.0430\n",
            "Train: epoch 0034 / 0100 | batch 0029 / 0050 | loss 0.0426\n",
            "Train: epoch 0034 / 0100 | batch 0030 / 0050 | loss 0.0422\n",
            "Train: epoch 0034 / 0100 | batch 0031 / 0050 | loss 0.0423\n",
            "Train: epoch 0034 / 0100 | batch 0032 / 0050 | loss 0.0429\n",
            "Train: epoch 0034 / 0100 | batch 0033 / 0050 | loss 0.0431\n",
            "Train: epoch 0034 / 0100 | batch 0034 / 0050 | loss 0.0427\n",
            "Train: epoch 0034 / 0100 | batch 0035 / 0050 | loss 0.0430\n",
            "Train: epoch 0034 / 0100 | batch 0036 / 0050 | loss 0.0428\n",
            "Train: epoch 0034 / 0100 | batch 0037 / 0050 | loss 0.0439\n",
            "Train: epoch 0034 / 0100 | batch 0038 / 0050 | loss 0.0447\n",
            "Train: epoch 0034 / 0100 | batch 0039 / 0050 | loss 0.0449\n",
            "Train: epoch 0034 / 0100 | batch 0040 / 0050 | loss 0.0450\n",
            "Train: epoch 0034 / 0100 | batch 0041 / 0050 | loss 0.0447\n",
            "Train: epoch 0034 / 0100 | batch 0042 / 0050 | loss 0.0450\n",
            "Train: epoch 0034 / 0100 | batch 0043 / 0050 | loss 0.0450\n",
            "Train: epoch 0034 / 0100 | batch 0044 / 0050 | loss 0.0450\n",
            "Train: epoch 0034 / 0100 | batch 0045 / 0050 | loss 0.0450\n",
            "Train: epoch 0034 / 0100 | batch 0046 / 0050 | loss 0.0451\n",
            "Train: epoch 0034 / 0100 | batch 0047 / 0050 | loss 0.0449\n",
            "Train: epoch 0034 / 0100 | batch 0048 / 0050 | loss 0.0447\n",
            "Train: epoch 0034 / 0100 | batch 0049 / 0050 | loss 0.0444\n",
            "Val loss 0.0761\n",
            "Dice score : 0.07764389365911484\n",
            "Val loss 0.0570\n",
            "Dice score : 0.061870865523815155\n",
            "Val loss 0.0576\n",
            "Dice score : 0.08737356215715408\n",
            "Val loss 0.0557\n",
            "Dice score : 0.0680374801158905\n",
            "Val loss 0.0563\n",
            "Dice score : 0.06440401822328568\n",
            "Val loss 0.0542\n",
            "Dice score : 0.07136636972427368\n",
            "Val loss 0.0518\n",
            "Dice score : 0.07460613548755646\n",
            "Val loss 0.0500\n",
            "Dice score : 0.014508354477584362\n",
            "Val loss 0.0480\n",
            "Dice score : 0.04325100779533386\n",
            "Val loss 0.0465\n",
            "Dice score : 0.046124741435050964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 34/100 [41:27<1:20:03, 72.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0035 / 0100 | batch 0000 / 0050 | loss 0.0269\n",
            "Train: epoch 0035 / 0100 | batch 0001 / 0050 | loss 0.0398\n",
            "Train: epoch 0035 / 0100 | batch 0002 / 0050 | loss 0.0418\n",
            "Train: epoch 0035 / 0100 | batch 0003 / 0050 | loss 0.0377\n",
            "Train: epoch 0035 / 0100 | batch 0004 / 0050 | loss 0.0375\n",
            "Train: epoch 0035 / 0100 | batch 0005 / 0050 | loss 0.0373\n",
            "Train: epoch 0035 / 0100 | batch 0006 / 0050 | loss 0.0368\n",
            "Train: epoch 0035 / 0100 | batch 0007 / 0050 | loss 0.0377\n",
            "Train: epoch 0035 / 0100 | batch 0008 / 0050 | loss 0.0371\n",
            "Train: epoch 0035 / 0100 | batch 0009 / 0050 | loss 0.0368\n",
            "Train: epoch 0035 / 0100 | batch 0010 / 0050 | loss 0.0375\n",
            "Train: epoch 0035 / 0100 | batch 0011 / 0050 | loss 0.0377\n",
            "Train: epoch 0035 / 0100 | batch 0012 / 0050 | loss 0.0373\n",
            "Train: epoch 0035 / 0100 | batch 0013 / 0050 | loss 0.0381\n",
            "Train: epoch 0035 / 0100 | batch 0014 / 0050 | loss 0.0397\n",
            "Train: epoch 0035 / 0100 | batch 0015 / 0050 | loss 0.0394\n",
            "Train: epoch 0035 / 0100 | batch 0016 / 0050 | loss 0.0390\n",
            "Train: epoch 0035 / 0100 | batch 0017 / 0050 | loss 0.0393\n",
            "Train: epoch 0035 / 0100 | batch 0018 / 0050 | loss 0.0397\n",
            "Train: epoch 0035 / 0100 | batch 0019 / 0050 | loss 0.0402\n",
            "Train: epoch 0035 / 0100 | batch 0020 / 0050 | loss 0.0400\n",
            "Train: epoch 0035 / 0100 | batch 0021 / 0050 | loss 0.0402\n",
            "Train: epoch 0035 / 0100 | batch 0022 / 0050 | loss 0.0399\n",
            "Train: epoch 0035 / 0100 | batch 0023 / 0050 | loss 0.0413\n",
            "Train: epoch 0035 / 0100 | batch 0024 / 0050 | loss 0.0409\n",
            "Train: epoch 0035 / 0100 | batch 0025 / 0050 | loss 0.0411\n",
            "Train: epoch 0035 / 0100 | batch 0026 / 0050 | loss 0.0409\n",
            "Train: epoch 0035 / 0100 | batch 0027 / 0050 | loss 0.0413\n",
            "Train: epoch 0035 / 0100 | batch 0028 / 0050 | loss 0.0409\n",
            "Train: epoch 0035 / 0100 | batch 0029 / 0050 | loss 0.0412\n",
            "Train: epoch 0035 / 0100 | batch 0030 / 0050 | loss 0.0414\n",
            "Train: epoch 0035 / 0100 | batch 0031 / 0050 | loss 0.0413\n",
            "Train: epoch 0035 / 0100 | batch 0032 / 0050 | loss 0.0412\n",
            "Train: epoch 0035 / 0100 | batch 0033 / 0050 | loss 0.0414\n",
            "Train: epoch 0035 / 0100 | batch 0034 / 0050 | loss 0.0414\n",
            "Train: epoch 0035 / 0100 | batch 0035 / 0050 | loss 0.0418\n",
            "Train: epoch 0035 / 0100 | batch 0036 / 0050 | loss 0.0420\n",
            "Train: epoch 0035 / 0100 | batch 0037 / 0050 | loss 0.0423\n",
            "Train: epoch 0035 / 0100 | batch 0038 / 0050 | loss 0.0422\n",
            "Train: epoch 0035 / 0100 | batch 0039 / 0050 | loss 0.0423\n",
            "Train: epoch 0035 / 0100 | batch 0040 / 0050 | loss 0.0426\n",
            "Train: epoch 0035 / 0100 | batch 0041 / 0050 | loss 0.0434\n",
            "Train: epoch 0035 / 0100 | batch 0042 / 0050 | loss 0.0432\n",
            "Train: epoch 0035 / 0100 | batch 0043 / 0050 | loss 0.0435\n",
            "Train: epoch 0035 / 0100 | batch 0044 / 0050 | loss 0.0434\n",
            "Train: epoch 0035 / 0100 | batch 0045 / 0050 | loss 0.0433\n",
            "Train: epoch 0035 / 0100 | batch 0046 / 0050 | loss 0.0432\n",
            "Train: epoch 0035 / 0100 | batch 0047 / 0050 | loss 0.0433\n",
            "Train: epoch 0035 / 0100 | batch 0048 / 0050 | loss 0.0432\n",
            "Train: epoch 0035 / 0100 | batch 0049 / 0050 | loss 0.0432\n",
            "Val loss 0.0281\n",
            "Dice score : 0.050818923860788345\n",
            "Val loss 0.0418\n",
            "Dice score : 0.08836370706558228\n",
            "Val loss 0.0472\n",
            "Dice score : 0.05669231712818146\n",
            "Val loss 0.0488\n",
            "Dice score : 0.0478169247508049\n",
            "Val loss 0.0458\n",
            "Dice score : 0.03907506540417671\n",
            "Val loss 0.0479\n",
            "Dice score : 0.09397681057453156\n",
            "Val loss 0.0463\n",
            "Dice score : 0.0529208779335022\n",
            "Val loss 0.0467\n",
            "Dice score : 0.06804695725440979\n",
            "Val loss 0.0479\n",
            "Dice score : 0.06013938784599304\n",
            "Val loss 0.0466\n",
            "Dice score : 0.03933160379528999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 35/100 [42:42<1:19:22, 73.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0036 / 0100 | batch 0000 / 0050 | loss 0.0321\n",
            "Train: epoch 0036 / 0100 | batch 0001 / 0050 | loss 0.0336\n",
            "Train: epoch 0036 / 0100 | batch 0002 / 0050 | loss 0.0350\n",
            "Train: epoch 0036 / 0100 | batch 0003 / 0050 | loss 0.0335\n",
            "Train: epoch 0036 / 0100 | batch 0004 / 0050 | loss 0.0396\n",
            "Train: epoch 0036 / 0100 | batch 0005 / 0050 | loss 0.0379\n",
            "Train: epoch 0036 / 0100 | batch 0006 / 0050 | loss 0.0403\n",
            "Train: epoch 0036 / 0100 | batch 0007 / 0050 | loss 0.0412\n",
            "Train: epoch 0036 / 0100 | batch 0008 / 0050 | loss 0.0423\n",
            "Train: epoch 0036 / 0100 | batch 0009 / 0050 | loss 0.0436\n",
            "Train: epoch 0036 / 0100 | batch 0010 / 0050 | loss 0.0426\n",
            "Train: epoch 0036 / 0100 | batch 0011 / 0050 | loss 0.0414\n",
            "Train: epoch 0036 / 0100 | batch 0012 / 0050 | loss 0.0411\n",
            "Train: epoch 0036 / 0100 | batch 0013 / 0050 | loss 0.0406\n",
            "Train: epoch 0036 / 0100 | batch 0014 / 0050 | loss 0.0398\n",
            "Train: epoch 0036 / 0100 | batch 0015 / 0050 | loss 0.0391\n",
            "Train: epoch 0036 / 0100 | batch 0016 / 0050 | loss 0.0394\n",
            "Train: epoch 0036 / 0100 | batch 0017 / 0050 | loss 0.0393\n",
            "Train: epoch 0036 / 0100 | batch 0018 / 0050 | loss 0.0388\n",
            "Train: epoch 0036 / 0100 | batch 0019 / 0050 | loss 0.0388\n",
            "Train: epoch 0036 / 0100 | batch 0020 / 0050 | loss 0.0401\n",
            "Train: epoch 0036 / 0100 | batch 0021 / 0050 | loss 0.0398\n",
            "Train: epoch 0036 / 0100 | batch 0022 / 0050 | loss 0.0407\n",
            "Train: epoch 0036 / 0100 | batch 0023 / 0050 | loss 0.0401\n",
            "Train: epoch 0036 / 0100 | batch 0024 / 0050 | loss 0.0400\n",
            "Train: epoch 0036 / 0100 | batch 0025 / 0050 | loss 0.0400\n",
            "Train: epoch 0036 / 0100 | batch 0026 / 0050 | loss 0.0399\n",
            "Train: epoch 0036 / 0100 | batch 0027 / 0050 | loss 0.0396\n",
            "Train: epoch 0036 / 0100 | batch 0028 / 0050 | loss 0.0401\n",
            "Train: epoch 0036 / 0100 | batch 0029 / 0050 | loss 0.0407\n",
            "Train: epoch 0036 / 0100 | batch 0030 / 0050 | loss 0.0410\n",
            "Train: epoch 0036 / 0100 | batch 0031 / 0050 | loss 0.0420\n",
            "Train: epoch 0036 / 0100 | batch 0032 / 0050 | loss 0.0428\n",
            "Train: epoch 0036 / 0100 | batch 0033 / 0050 | loss 0.0430\n",
            "Train: epoch 0036 / 0100 | batch 0034 / 0050 | loss 0.0426\n",
            "Train: epoch 0036 / 0100 | batch 0035 / 0050 | loss 0.0430\n",
            "Train: epoch 0036 / 0100 | batch 0036 / 0050 | loss 0.0427\n",
            "Train: epoch 0036 / 0100 | batch 0037 / 0050 | loss 0.0429\n",
            "Train: epoch 0036 / 0100 | batch 0038 / 0050 | loss 0.0428\n",
            "Train: epoch 0036 / 0100 | batch 0039 / 0050 | loss 0.0431\n",
            "Train: epoch 0036 / 0100 | batch 0040 / 0050 | loss 0.0434\n",
            "Train: epoch 0036 / 0100 | batch 0041 / 0050 | loss 0.0431\n",
            "Train: epoch 0036 / 0100 | batch 0042 / 0050 | loss 0.0430\n",
            "Train: epoch 0036 / 0100 | batch 0043 / 0050 | loss 0.0428\n",
            "Train: epoch 0036 / 0100 | batch 0044 / 0050 | loss 0.0426\n",
            "Train: epoch 0036 / 0100 | batch 0045 / 0050 | loss 0.0424\n",
            "Train: epoch 0036 / 0100 | batch 0046 / 0050 | loss 0.0423\n",
            "Train: epoch 0036 / 0100 | batch 0047 / 0050 | loss 0.0425\n",
            "Train: epoch 0036 / 0100 | batch 0048 / 0050 | loss 0.0426\n",
            "Train: epoch 0036 / 0100 | batch 0049 / 0050 | loss 0.0424\n",
            "Val loss 0.0524\n",
            "Dice score : 0.07918757945299149\n",
            "Val loss 0.0577\n",
            "Dice score : 0.059834640473127365\n",
            "Val loss 0.0500\n",
            "Dice score : 0.05024547874927521\n",
            "Val loss 0.0450\n",
            "Dice score : 0.042074792087078094\n",
            "Val loss 0.0439\n",
            "Dice score : 0.06645631045103073\n",
            "Val loss 0.0464\n",
            "Dice score : 0.09364721179008484\n",
            "Val loss 0.0471\n",
            "Dice score : 0.07266920804977417\n",
            "Val loss 0.0451\n",
            "Dice score : 0.07669764012098312\n",
            "Val loss 0.0444\n",
            "Dice score : 0.03607824072241783\n",
            "Val loss 0.0450\n",
            "Dice score : 0.052687134593725204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 36/100 [43:54<1:17:50, 72.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0037 / 0100 | batch 0000 / 0050 | loss 0.0309\n",
            "Train: epoch 0037 / 0100 | batch 0001 / 0050 | loss 0.0460\n",
            "Train: epoch 0037 / 0100 | batch 0002 / 0050 | loss 0.0435\n",
            "Train: epoch 0037 / 0100 | batch 0003 / 0050 | loss 0.0433\n",
            "Train: epoch 0037 / 0100 | batch 0004 / 0050 | loss 0.0422\n",
            "Train: epoch 0037 / 0100 | batch 0005 / 0050 | loss 0.0447\n",
            "Train: epoch 0037 / 0100 | batch 0006 / 0050 | loss 0.0432\n",
            "Train: epoch 0037 / 0100 | batch 0007 / 0050 | loss 0.0417\n",
            "Train: epoch 0037 / 0100 | batch 0008 / 0050 | loss 0.0411\n",
            "Train: epoch 0037 / 0100 | batch 0009 / 0050 | loss 0.0398\n",
            "Train: epoch 0037 / 0100 | batch 0010 / 0050 | loss 0.0393\n",
            "Train: epoch 0037 / 0100 | batch 0011 / 0050 | loss 0.0393\n",
            "Train: epoch 0037 / 0100 | batch 0012 / 0050 | loss 0.0400\n",
            "Train: epoch 0037 / 0100 | batch 0013 / 0050 | loss 0.0397\n",
            "Train: epoch 0037 / 0100 | batch 0014 / 0050 | loss 0.0392\n",
            "Train: epoch 0037 / 0100 | batch 0015 / 0050 | loss 0.0387\n",
            "Train: epoch 0037 / 0100 | batch 0016 / 0050 | loss 0.0393\n",
            "Train: epoch 0037 / 0100 | batch 0017 / 0050 | loss 0.0399\n",
            "Train: epoch 0037 / 0100 | batch 0018 / 0050 | loss 0.0412\n",
            "Train: epoch 0037 / 0100 | batch 0019 / 0050 | loss 0.0417\n",
            "Train: epoch 0037 / 0100 | batch 0020 / 0050 | loss 0.0410\n",
            "Train: epoch 0037 / 0100 | batch 0021 / 0050 | loss 0.0419\n",
            "Train: epoch 0037 / 0100 | batch 0022 / 0050 | loss 0.0416\n",
            "Train: epoch 0037 / 0100 | batch 0023 / 0050 | loss 0.0420\n",
            "Train: epoch 0037 / 0100 | batch 0024 / 0050 | loss 0.0422\n",
            "Train: epoch 0037 / 0100 | batch 0025 / 0050 | loss 0.0425\n",
            "Train: epoch 0037 / 0100 | batch 0026 / 0050 | loss 0.0425\n",
            "Train: epoch 0037 / 0100 | batch 0027 / 0050 | loss 0.0426\n",
            "Train: epoch 0037 / 0100 | batch 0028 / 0050 | loss 0.0425\n",
            "Train: epoch 0037 / 0100 | batch 0029 / 0050 | loss 0.0421\n",
            "Train: epoch 0037 / 0100 | batch 0030 / 0050 | loss 0.0418\n",
            "Train: epoch 0037 / 0100 | batch 0031 / 0050 | loss 0.0420\n",
            "Train: epoch 0037 / 0100 | batch 0032 / 0050 | loss 0.0421\n",
            "Train: epoch 0037 / 0100 | batch 0033 / 0050 | loss 0.0421\n",
            "Train: epoch 0037 / 0100 | batch 0034 / 0050 | loss 0.0420\n",
            "Train: epoch 0037 / 0100 | batch 0035 / 0050 | loss 0.0417\n",
            "Train: epoch 0037 / 0100 | batch 0036 / 0050 | loss 0.0415\n",
            "Train: epoch 0037 / 0100 | batch 0037 / 0050 | loss 0.0417\n",
            "Train: epoch 0037 / 0100 | batch 0038 / 0050 | loss 0.0415\n",
            "Train: epoch 0037 / 0100 | batch 0039 / 0050 | loss 0.0416\n",
            "Train: epoch 0037 / 0100 | batch 0040 / 0050 | loss 0.0415\n",
            "Train: epoch 0037 / 0100 | batch 0041 / 0050 | loss 0.0415\n",
            "Train: epoch 0037 / 0100 | batch 0042 / 0050 | loss 0.0417\n",
            "Train: epoch 0037 / 0100 | batch 0043 / 0050 | loss 0.0417\n",
            "Train: epoch 0037 / 0100 | batch 0044 / 0050 | loss 0.0424\n",
            "Train: epoch 0037 / 0100 | batch 0045 / 0050 | loss 0.0423\n",
            "Train: epoch 0037 / 0100 | batch 0046 / 0050 | loss 0.0423\n",
            "Train: epoch 0037 / 0100 | batch 0047 / 0050 | loss 0.0420\n",
            "Train: epoch 0037 / 0100 | batch 0048 / 0050 | loss 0.0422\n",
            "Train: epoch 0037 / 0100 | batch 0049 / 0050 | loss 0.0420\n",
            "Val loss 0.0533\n",
            "Dice score : 0.09619195014238358\n",
            "Val loss 0.0577\n",
            "Dice score : 0.07946723699569702\n",
            "Val loss 0.0499\n",
            "Dice score : 0.08072934299707413\n",
            "Val loss 0.0540\n",
            "Dice score : 0.05408921465277672\n",
            "Val loss 0.0516\n",
            "Dice score : 0.05948207899928093\n",
            "Val loss 0.0521\n",
            "Dice score : 0.08461891114711761\n",
            "Val loss 0.0496\n",
            "Dice score : 0.07313523441553116\n",
            "Val loss 0.0473\n",
            "Dice score : 0.05523104965686798\n",
            "Val loss 0.0458\n",
            "Dice score : 0.046101056039333344\n",
            "Val loss 0.0444\n",
            "Dice score : 0.07543440163135529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 37/100 [45:07<1:16:27, 72.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0038 / 0100 | batch 0000 / 0050 | loss 0.0337\n",
            "Train: epoch 0038 / 0100 | batch 0001 / 0050 | loss 0.0334\n",
            "Train: epoch 0038 / 0100 | batch 0002 / 0050 | loss 0.0362\n",
            "Train: epoch 0038 / 0100 | batch 0003 / 0050 | loss 0.0371\n",
            "Train: epoch 0038 / 0100 | batch 0004 / 0050 | loss 0.0406\n",
            "Train: epoch 0038 / 0100 | batch 0005 / 0050 | loss 0.0386\n",
            "Train: epoch 0038 / 0100 | batch 0006 / 0050 | loss 0.0414\n",
            "Train: epoch 0038 / 0100 | batch 0007 / 0050 | loss 0.0428\n",
            "Train: epoch 0038 / 0100 | batch 0008 / 0050 | loss 0.0420\n",
            "Train: epoch 0038 / 0100 | batch 0009 / 0050 | loss 0.0436\n",
            "Train: epoch 0038 / 0100 | batch 0010 / 0050 | loss 0.0428\n",
            "Train: epoch 0038 / 0100 | batch 0011 / 0050 | loss 0.0416\n",
            "Train: epoch 0038 / 0100 | batch 0012 / 0050 | loss 0.0425\n",
            "Train: epoch 0038 / 0100 | batch 0013 / 0050 | loss 0.0423\n",
            "Train: epoch 0038 / 0100 | batch 0014 / 0050 | loss 0.0433\n",
            "Train: epoch 0038 / 0100 | batch 0015 / 0050 | loss 0.0436\n",
            "Train: epoch 0038 / 0100 | batch 0016 / 0050 | loss 0.0435\n",
            "Train: epoch 0038 / 0100 | batch 0017 / 0050 | loss 0.0433\n",
            "Train: epoch 0038 / 0100 | batch 0018 / 0050 | loss 0.0430\n",
            "Train: epoch 0038 / 0100 | batch 0019 / 0050 | loss 0.0425\n",
            "Train: epoch 0038 / 0100 | batch 0020 / 0050 | loss 0.0418\n",
            "Train: epoch 0038 / 0100 | batch 0021 / 0050 | loss 0.0425\n",
            "Train: epoch 0038 / 0100 | batch 0022 / 0050 | loss 0.0420\n",
            "Train: epoch 0038 / 0100 | batch 0023 / 0050 | loss 0.0422\n",
            "Train: epoch 0038 / 0100 | batch 0024 / 0050 | loss 0.0424\n",
            "Train: epoch 0038 / 0100 | batch 0025 / 0050 | loss 0.0420\n",
            "Train: epoch 0038 / 0100 | batch 0026 / 0050 | loss 0.0427\n",
            "Train: epoch 0038 / 0100 | batch 0027 / 0050 | loss 0.0430\n",
            "Train: epoch 0038 / 0100 | batch 0028 / 0050 | loss 0.0425\n",
            "Train: epoch 0038 / 0100 | batch 0029 / 0050 | loss 0.0421\n",
            "Train: epoch 0038 / 0100 | batch 0030 / 0050 | loss 0.0419\n",
            "Train: epoch 0038 / 0100 | batch 0031 / 0050 | loss 0.0418\n",
            "Train: epoch 0038 / 0100 | batch 0032 / 0050 | loss 0.0415\n",
            "Train: epoch 0038 / 0100 | batch 0033 / 0050 | loss 0.0413\n",
            "Train: epoch 0038 / 0100 | batch 0034 / 0050 | loss 0.0411\n",
            "Train: epoch 0038 / 0100 | batch 0035 / 0050 | loss 0.0411\n",
            "Train: epoch 0038 / 0100 | batch 0036 / 0050 | loss 0.0409\n",
            "Train: epoch 0038 / 0100 | batch 0037 / 0050 | loss 0.0406\n",
            "Train: epoch 0038 / 0100 | batch 0038 / 0050 | loss 0.0409\n",
            "Train: epoch 0038 / 0100 | batch 0039 / 0050 | loss 0.0414\n",
            "Train: epoch 0038 / 0100 | batch 0040 / 0050 | loss 0.0412\n",
            "Train: epoch 0038 / 0100 | batch 0041 / 0050 | loss 0.0412\n",
            "Train: epoch 0038 / 0100 | batch 0042 / 0050 | loss 0.0412\n",
            "Train: epoch 0038 / 0100 | batch 0043 / 0050 | loss 0.0411\n",
            "Train: epoch 0038 / 0100 | batch 0044 / 0050 | loss 0.0411\n",
            "Train: epoch 0038 / 0100 | batch 0045 / 0050 | loss 0.0418\n",
            "Train: epoch 0038 / 0100 | batch 0046 / 0050 | loss 0.0421\n",
            "Train: epoch 0038 / 0100 | batch 0047 / 0050 | loss 0.0419\n",
            "Train: epoch 0038 / 0100 | batch 0048 / 0050 | loss 0.0418\n",
            "Train: epoch 0038 / 0100 | batch 0049 / 0050 | loss 0.0419\n",
            "Val loss 0.0414\n",
            "Dice score : 0.05471944436430931\n",
            "Val loss 0.0442\n",
            "Dice score : 0.06107553094625473\n",
            "Val loss 0.0425\n",
            "Dice score : 0.0711214616894722\n",
            "Val loss 0.0406\n",
            "Dice score : 0.04829582944512367\n",
            "Val loss 0.0450\n",
            "Dice score : 0.08672651648521423\n",
            "Val loss 0.0463\n",
            "Dice score : 0.07676640152931213\n",
            "Val loss 0.0442\n",
            "Dice score : 0.044046781957149506\n",
            "Val loss 0.0426\n",
            "Dice score : 0.037225205451250076\n",
            "Val loss 0.0417\n",
            "Dice score : 0.06462179869413376\n",
            "Val loss 0.0448\n",
            "Dice score : 0.08408939838409424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 38/100 [46:21<1:15:35, 73.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0039 / 0100 | batch 0000 / 0050 | loss 0.0551\n",
            "Train: epoch 0039 / 0100 | batch 0001 / 0050 | loss 0.0583\n",
            "Train: epoch 0039 / 0100 | batch 0002 / 0050 | loss 0.0492\n",
            "Train: epoch 0039 / 0100 | batch 0003 / 0050 | loss 0.0500\n",
            "Train: epoch 0039 / 0100 | batch 0004 / 0050 | loss 0.0496\n",
            "Train: epoch 0039 / 0100 | batch 0005 / 0050 | loss 0.0492\n",
            "Train: epoch 0039 / 0100 | batch 0006 / 0050 | loss 0.0468\n",
            "Train: epoch 0039 / 0100 | batch 0007 / 0050 | loss 0.0465\n",
            "Train: epoch 0039 / 0100 | batch 0008 / 0050 | loss 0.0461\n",
            "Train: epoch 0039 / 0100 | batch 0009 / 0050 | loss 0.0455\n",
            "Train: epoch 0039 / 0100 | batch 0010 / 0050 | loss 0.0448\n",
            "Train: epoch 0039 / 0100 | batch 0011 / 0050 | loss 0.0441\n",
            "Train: epoch 0039 / 0100 | batch 0012 / 0050 | loss 0.0430\n",
            "Train: epoch 0039 / 0100 | batch 0013 / 0050 | loss 0.0442\n",
            "Train: epoch 0039 / 0100 | batch 0014 / 0050 | loss 0.0446\n",
            "Train: epoch 0039 / 0100 | batch 0015 / 0050 | loss 0.0434\n",
            "Train: epoch 0039 / 0100 | batch 0016 / 0050 | loss 0.0437\n",
            "Train: epoch 0039 / 0100 | batch 0017 / 0050 | loss 0.0431\n",
            "Train: epoch 0039 / 0100 | batch 0018 / 0050 | loss 0.0429\n",
            "Train: epoch 0039 / 0100 | batch 0019 / 0050 | loss 0.0426\n",
            "Train: epoch 0039 / 0100 | batch 0020 / 0050 | loss 0.0420\n",
            "Train: epoch 0039 / 0100 | batch 0021 / 0050 | loss 0.0413\n",
            "Train: epoch 0039 / 0100 | batch 0022 / 0050 | loss 0.0417\n",
            "Train: epoch 0039 / 0100 | batch 0023 / 0050 | loss 0.0414\n",
            "Train: epoch 0039 / 0100 | batch 0024 / 0050 | loss 0.0410\n",
            "Train: epoch 0039 / 0100 | batch 0025 / 0050 | loss 0.0406\n",
            "Train: epoch 0039 / 0100 | batch 0026 / 0050 | loss 0.0406\n",
            "Train: epoch 0039 / 0100 | batch 0027 / 0050 | loss 0.0402\n",
            "Train: epoch 0039 / 0100 | batch 0028 / 0050 | loss 0.0409\n",
            "Train: epoch 0039 / 0100 | batch 0029 / 0050 | loss 0.0417\n",
            "Train: epoch 0039 / 0100 | batch 0030 / 0050 | loss 0.0417\n",
            "Train: epoch 0039 / 0100 | batch 0031 / 0050 | loss 0.0415\n",
            "Train: epoch 0039 / 0100 | batch 0032 / 0050 | loss 0.0415\n",
            "Train: epoch 0039 / 0100 | batch 0033 / 0050 | loss 0.0412\n",
            "Train: epoch 0039 / 0100 | batch 0034 / 0050 | loss 0.0411\n",
            "Train: epoch 0039 / 0100 | batch 0035 / 0050 | loss 0.0408\n",
            "Train: epoch 0039 / 0100 | batch 0036 / 0050 | loss 0.0404\n",
            "Train: epoch 0039 / 0100 | batch 0037 / 0050 | loss 0.0406\n",
            "Train: epoch 0039 / 0100 | batch 0038 / 0050 | loss 0.0403\n",
            "Train: epoch 0039 / 0100 | batch 0039 / 0050 | loss 0.0402\n",
            "Train: epoch 0039 / 0100 | batch 0040 / 0050 | loss 0.0404\n",
            "Train: epoch 0039 / 0100 | batch 0041 / 0050 | loss 0.0401\n",
            "Train: epoch 0039 / 0100 | batch 0042 / 0050 | loss 0.0401\n",
            "Train: epoch 0039 / 0100 | batch 0043 / 0050 | loss 0.0401\n",
            "Train: epoch 0039 / 0100 | batch 0044 / 0050 | loss 0.0400\n",
            "Train: epoch 0039 / 0100 | batch 0045 / 0050 | loss 0.0401\n",
            "Train: epoch 0039 / 0100 | batch 0046 / 0050 | loss 0.0402\n",
            "Train: epoch 0039 / 0100 | batch 0047 / 0050 | loss 0.0405\n",
            "Train: epoch 0039 / 0100 | batch 0048 / 0050 | loss 0.0403\n",
            "Train: epoch 0039 / 0100 | batch 0049 / 0050 | loss 0.0411\n",
            "Val loss 0.0495\n",
            "Dice score : 0.04759473726153374\n",
            "Val loss 0.0443\n",
            "Dice score : 0.0490267351269722\n",
            "Val loss 0.0536\n",
            "Dice score : 0.061898909509181976\n",
            "Val loss 0.0516\n",
            "Dice score : 0.01370153110474348\n",
            "Val loss 0.0521\n",
            "Dice score : 0.07962051033973694\n",
            "Val loss 0.0510\n",
            "Dice score : 0.07387224584817886\n",
            "Val loss 0.0486\n",
            "Dice score : 0.06328223645687103\n",
            "Val loss 0.0472\n",
            "Dice score : 0.06889744102954865\n",
            "Val loss 0.0459\n",
            "Dice score : 0.07748192548751831\n",
            "Val loss 0.0464\n",
            "Dice score : 0.050910584628582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 39/100 [47:32<1:13:57, 72.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0040 / 0100 | batch 0000 / 0050 | loss 0.0479\n",
            "Train: epoch 0040 / 0100 | batch 0001 / 0050 | loss 0.0545\n",
            "Train: epoch 0040 / 0100 | batch 0002 / 0050 | loss 0.0486\n",
            "Train: epoch 0040 / 0100 | batch 0003 / 0050 | loss 0.0443\n",
            "Train: epoch 0040 / 0100 | batch 0004 / 0050 | loss 0.0431\n",
            "Train: epoch 0040 / 0100 | batch 0005 / 0050 | loss 0.0418\n",
            "Train: epoch 0040 / 0100 | batch 0006 / 0050 | loss 0.0421\n",
            "Train: epoch 0040 / 0100 | batch 0007 / 0050 | loss 0.0413\n",
            "Train: epoch 0040 / 0100 | batch 0008 / 0050 | loss 0.0399\n",
            "Train: epoch 0040 / 0100 | batch 0009 / 0050 | loss 0.0395\n",
            "Train: epoch 0040 / 0100 | batch 0010 / 0050 | loss 0.0403\n",
            "Train: epoch 0040 / 0100 | batch 0011 / 0050 | loss 0.0410\n",
            "Train: epoch 0040 / 0100 | batch 0012 / 0050 | loss 0.0404\n",
            "Train: epoch 0040 / 0100 | batch 0013 / 0050 | loss 0.0399\n",
            "Train: epoch 0040 / 0100 | batch 0014 / 0050 | loss 0.0396\n",
            "Train: epoch 0040 / 0100 | batch 0015 / 0050 | loss 0.0401\n",
            "Train: epoch 0040 / 0100 | batch 0016 / 0050 | loss 0.0400\n",
            "Train: epoch 0040 / 0100 | batch 0017 / 0050 | loss 0.0402\n",
            "Train: epoch 0040 / 0100 | batch 0018 / 0050 | loss 0.0404\n",
            "Train: epoch 0040 / 0100 | batch 0019 / 0050 | loss 0.0421\n",
            "Train: epoch 0040 / 0100 | batch 0020 / 0050 | loss 0.0416\n",
            "Train: epoch 0040 / 0100 | batch 0021 / 0050 | loss 0.0409\n",
            "Train: epoch 0040 / 0100 | batch 0022 / 0050 | loss 0.0404\n",
            "Train: epoch 0040 / 0100 | batch 0023 / 0050 | loss 0.0400\n",
            "Train: epoch 0040 / 0100 | batch 0024 / 0050 | loss 0.0401\n",
            "Train: epoch 0040 / 0100 | batch 0025 / 0050 | loss 0.0397\n",
            "Train: epoch 0040 / 0100 | batch 0026 / 0050 | loss 0.0394\n",
            "Train: epoch 0040 / 0100 | batch 0027 / 0050 | loss 0.0389\n",
            "Train: epoch 0040 / 0100 | batch 0028 / 0050 | loss 0.0388\n",
            "Train: epoch 0040 / 0100 | batch 0029 / 0050 | loss 0.0389\n",
            "Train: epoch 0040 / 0100 | batch 0030 / 0050 | loss 0.0387\n",
            "Train: epoch 0040 / 0100 | batch 0031 / 0050 | loss 0.0382\n",
            "Train: epoch 0040 / 0100 | batch 0032 / 0050 | loss 0.0382\n",
            "Train: epoch 0040 / 0100 | batch 0033 / 0050 | loss 0.0381\n",
            "Train: epoch 0040 / 0100 | batch 0034 / 0050 | loss 0.0392\n",
            "Train: epoch 0040 / 0100 | batch 0035 / 0050 | loss 0.0392\n",
            "Train: epoch 0040 / 0100 | batch 0036 / 0050 | loss 0.0397\n",
            "Train: epoch 0040 / 0100 | batch 0037 / 0050 | loss 0.0404\n",
            "Train: epoch 0040 / 0100 | batch 0038 / 0050 | loss 0.0402\n",
            "Train: epoch 0040 / 0100 | batch 0039 / 0050 | loss 0.0401\n",
            "Train: epoch 0040 / 0100 | batch 0040 / 0050 | loss 0.0405\n",
            "Train: epoch 0040 / 0100 | batch 0041 / 0050 | loss 0.0406\n",
            "Train: epoch 0040 / 0100 | batch 0042 / 0050 | loss 0.0407\n",
            "Train: epoch 0040 / 0100 | batch 0043 / 0050 | loss 0.0406\n",
            "Train: epoch 0040 / 0100 | batch 0044 / 0050 | loss 0.0405\n",
            "Train: epoch 0040 / 0100 | batch 0045 / 0050 | loss 0.0404\n",
            "Train: epoch 0040 / 0100 | batch 0046 / 0050 | loss 0.0409\n",
            "Train: epoch 0040 / 0100 | batch 0047 / 0050 | loss 0.0410\n",
            "Train: epoch 0040 / 0100 | batch 0048 / 0050 | loss 0.0410\n",
            "Train: epoch 0040 / 0100 | batch 0049 / 0050 | loss 0.0413\n",
            "Val loss 0.0501\n",
            "Dice score : 0.06587610393762589\n",
            "Val loss 0.0440\n",
            "Dice score : 0.07194605469703674\n",
            "Val loss 0.0425\n",
            "Dice score : 0.035125236958265305\n",
            "Val loss 0.0395\n",
            "Dice score : 0.03982294350862503\n",
            "Val loss 0.0419\n",
            "Dice score : 0.07430833578109741\n",
            "Val loss 0.0424\n",
            "Dice score : 0.08323043584823608\n",
            "Val loss 0.0403\n",
            "Dice score : 0.06241154298186302\n",
            "Val loss 0.0424\n",
            "Dice score : 0.0978572741150856\n",
            "Val loss 0.0442\n",
            "Dice score : 0.09272947162389755\n",
            "Val loss 0.0428\n",
            "Dice score : 0.06720100343227386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 40/100 [48:43<1:12:00, 72.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0041 / 0100 | batch 0000 / 0050 | loss 0.0305\n",
            "Train: epoch 0041 / 0100 | batch 0001 / 0050 | loss 0.0314\n",
            "Train: epoch 0041 / 0100 | batch 0002 / 0050 | loss 0.0386\n",
            "Train: epoch 0041 / 0100 | batch 0003 / 0050 | loss 0.0427\n",
            "Train: epoch 0041 / 0100 | batch 0004 / 0050 | loss 0.0402\n",
            "Train: epoch 0041 / 0100 | batch 0005 / 0050 | loss 0.0388\n",
            "Train: epoch 0041 / 0100 | batch 0006 / 0050 | loss 0.0403\n",
            "Train: epoch 0041 / 0100 | batch 0007 / 0050 | loss 0.0419\n",
            "Train: epoch 0041 / 0100 | batch 0008 / 0050 | loss 0.0404\n",
            "Train: epoch 0041 / 0100 | batch 0009 / 0050 | loss 0.0431\n",
            "Train: epoch 0041 / 0100 | batch 0010 / 0050 | loss 0.0431\n",
            "Train: epoch 0041 / 0100 | batch 0011 / 0050 | loss 0.0414\n",
            "Train: epoch 0041 / 0100 | batch 0012 / 0050 | loss 0.0413\n",
            "Train: epoch 0041 / 0100 | batch 0013 / 0050 | loss 0.0410\n",
            "Train: epoch 0041 / 0100 | batch 0014 / 0050 | loss 0.0409\n",
            "Train: epoch 0041 / 0100 | batch 0015 / 0050 | loss 0.0422\n",
            "Train: epoch 0041 / 0100 | batch 0016 / 0050 | loss 0.0423\n",
            "Train: epoch 0041 / 0100 | batch 0017 / 0050 | loss 0.0418\n",
            "Train: epoch 0041 / 0100 | batch 0018 / 0050 | loss 0.0410\n",
            "Train: epoch 0041 / 0100 | batch 0019 / 0050 | loss 0.0410\n",
            "Train: epoch 0041 / 0100 | batch 0020 / 0050 | loss 0.0411\n",
            "Train: epoch 0041 / 0100 | batch 0021 / 0050 | loss 0.0413\n",
            "Train: epoch 0041 / 0100 | batch 0022 / 0050 | loss 0.0409\n",
            "Train: epoch 0041 / 0100 | batch 0023 / 0050 | loss 0.0407\n",
            "Train: epoch 0041 / 0100 | batch 0024 / 0050 | loss 0.0409\n",
            "Train: epoch 0041 / 0100 | batch 0025 / 0050 | loss 0.0407\n",
            "Train: epoch 0041 / 0100 | batch 0026 / 0050 | loss 0.0412\n",
            "Train: epoch 0041 / 0100 | batch 0027 / 0050 | loss 0.0411\n",
            "Train: epoch 0041 / 0100 | batch 0028 / 0050 | loss 0.0414\n",
            "Train: epoch 0041 / 0100 | batch 0029 / 0050 | loss 0.0411\n",
            "Train: epoch 0041 / 0100 | batch 0030 / 0050 | loss 0.0409\n",
            "Train: epoch 0041 / 0100 | batch 0031 / 0050 | loss 0.0408\n",
            "Train: epoch 0041 / 0100 | batch 0032 / 0050 | loss 0.0405\n",
            "Train: epoch 0041 / 0100 | batch 0033 / 0050 | loss 0.0403\n",
            "Train: epoch 0041 / 0100 | batch 0034 / 0050 | loss 0.0400\n",
            "Train: epoch 0041 / 0100 | batch 0035 / 0050 | loss 0.0398\n",
            "Train: epoch 0041 / 0100 | batch 0036 / 0050 | loss 0.0400\n",
            "Train: epoch 0041 / 0100 | batch 0037 / 0050 | loss 0.0399\n",
            "Train: epoch 0041 / 0100 | batch 0038 / 0050 | loss 0.0401\n",
            "Train: epoch 0041 / 0100 | batch 0039 / 0050 | loss 0.0398\n",
            "Train: epoch 0041 / 0100 | batch 0040 / 0050 | loss 0.0402\n",
            "Train: epoch 0041 / 0100 | batch 0041 / 0050 | loss 0.0407\n",
            "Train: epoch 0041 / 0100 | batch 0042 / 0050 | loss 0.0404\n",
            "Train: epoch 0041 / 0100 | batch 0043 / 0050 | loss 0.0410\n",
            "Train: epoch 0041 / 0100 | batch 0044 / 0050 | loss 0.0411\n",
            "Train: epoch 0041 / 0100 | batch 0045 / 0050 | loss 0.0415\n",
            "Train: epoch 0041 / 0100 | batch 0046 / 0050 | loss 0.0412\n",
            "Train: epoch 0041 / 0100 | batch 0047 / 0050 | loss 0.0411\n",
            "Train: epoch 0041 / 0100 | batch 0048 / 0050 | loss 0.0408\n",
            "Train: epoch 0041 / 0100 | batch 0049 / 0050 | loss 0.0408\n",
            "Val loss 0.0545\n",
            "Dice score : 0.061378125101327896\n",
            "Val loss 0.0430\n",
            "Dice score : 0.09466101229190826\n",
            "Val loss 0.0434\n",
            "Dice score : 0.058558810502290726\n",
            "Val loss 0.0449\n",
            "Dice score : 0.06700898706912994\n",
            "Val loss 0.0473\n",
            "Dice score : 0.08463872969150543\n",
            "Val loss 0.0498\n",
            "Dice score : 0.09687810391187668\n",
            "Val loss 0.0467\n",
            "Dice score : 0.0657755509018898\n",
            "Val loss 0.0450\n",
            "Dice score : 0.059745825827121735\n",
            "Val loss 0.0435\n",
            "Dice score : 0.06547343730926514\n",
            "Val loss 0.0431\n",
            "Dice score : 0.06128780171275139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 41/100 [49:55<1:10:54, 72.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0042 / 0100 | batch 0000 / 0050 | loss 0.0563\n",
            "Train: epoch 0042 / 0100 | batch 0001 / 0050 | loss 0.0420\n",
            "Train: epoch 0042 / 0100 | batch 0002 / 0050 | loss 0.0367\n",
            "Train: epoch 0042 / 0100 | batch 0003 / 0050 | loss 0.0388\n",
            "Train: epoch 0042 / 0100 | batch 0004 / 0050 | loss 0.0453\n",
            "Train: epoch 0042 / 0100 | batch 0005 / 0050 | loss 0.0435\n",
            "Train: epoch 0042 / 0100 | batch 0006 / 0050 | loss 0.0434\n",
            "Train: epoch 0042 / 0100 | batch 0007 / 0050 | loss 0.0421\n",
            "Train: epoch 0042 / 0100 | batch 0008 / 0050 | loss 0.0415\n",
            "Train: epoch 0042 / 0100 | batch 0009 / 0050 | loss 0.0409\n",
            "Train: epoch 0042 / 0100 | batch 0010 / 0050 | loss 0.0399\n",
            "Train: epoch 0042 / 0100 | batch 0011 / 0050 | loss 0.0392\n",
            "Train: epoch 0042 / 0100 | batch 0012 / 0050 | loss 0.0386\n",
            "Train: epoch 0042 / 0100 | batch 0013 / 0050 | loss 0.0379\n",
            "Train: epoch 0042 / 0100 | batch 0014 / 0050 | loss 0.0386\n",
            "Train: epoch 0042 / 0100 | batch 0015 / 0050 | loss 0.0385\n",
            "Train: epoch 0042 / 0100 | batch 0016 / 0050 | loss 0.0391\n",
            "Train: epoch 0042 / 0100 | batch 0017 / 0050 | loss 0.0385\n",
            "Train: epoch 0042 / 0100 | batch 0018 / 0050 | loss 0.0384\n",
            "Train: epoch 0042 / 0100 | batch 0019 / 0050 | loss 0.0388\n",
            "Train: epoch 0042 / 0100 | batch 0020 / 0050 | loss 0.0409\n",
            "Train: epoch 0042 / 0100 | batch 0021 / 0050 | loss 0.0407\n",
            "Train: epoch 0042 / 0100 | batch 0022 / 0050 | loss 0.0404\n",
            "Train: epoch 0042 / 0100 | batch 0023 / 0050 | loss 0.0409\n",
            "Train: epoch 0042 / 0100 | batch 0024 / 0050 | loss 0.0408\n",
            "Train: epoch 0042 / 0100 | batch 0025 / 0050 | loss 0.0403\n",
            "Train: epoch 0042 / 0100 | batch 0026 / 0050 | loss 0.0405\n",
            "Train: epoch 0042 / 0100 | batch 0027 / 0050 | loss 0.0419\n",
            "Train: epoch 0042 / 0100 | batch 0028 / 0050 | loss 0.0419\n",
            "Train: epoch 0042 / 0100 | batch 0029 / 0050 | loss 0.0416\n",
            "Train: epoch 0042 / 0100 | batch 0030 / 0050 | loss 0.0412\n",
            "Train: epoch 0042 / 0100 | batch 0031 / 0050 | loss 0.0415\n",
            "Train: epoch 0042 / 0100 | batch 0032 / 0050 | loss 0.0413\n",
            "Train: epoch 0042 / 0100 | batch 0033 / 0050 | loss 0.0416\n",
            "Train: epoch 0042 / 0100 | batch 0034 / 0050 | loss 0.0412\n",
            "Train: epoch 0042 / 0100 | batch 0035 / 0050 | loss 0.0415\n",
            "Train: epoch 0042 / 0100 | batch 0036 / 0050 | loss 0.0413\n",
            "Train: epoch 0042 / 0100 | batch 0037 / 0050 | loss 0.0417\n",
            "Train: epoch 0042 / 0100 | batch 0038 / 0050 | loss 0.0419\n",
            "Train: epoch 0042 / 0100 | batch 0039 / 0050 | loss 0.0417\n",
            "Train: epoch 0042 / 0100 | batch 0040 / 0050 | loss 0.0415\n",
            "Train: epoch 0042 / 0100 | batch 0041 / 0050 | loss 0.0411\n",
            "Train: epoch 0042 / 0100 | batch 0042 / 0050 | loss 0.0408\n",
            "Train: epoch 0042 / 0100 | batch 0043 / 0050 | loss 0.0406\n",
            "Train: epoch 0042 / 0100 | batch 0044 / 0050 | loss 0.0404\n",
            "Train: epoch 0042 / 0100 | batch 0045 / 0050 | loss 0.0408\n",
            "Train: epoch 0042 / 0100 | batch 0046 / 0050 | loss 0.0407\n",
            "Train: epoch 0042 / 0100 | batch 0047 / 0050 | loss 0.0404\n",
            "Train: epoch 0042 / 0100 | batch 0048 / 0050 | loss 0.0403\n",
            "Train: epoch 0042 / 0100 | batch 0049 / 0050 | loss 0.0402\n",
            "Val loss 0.0431\n",
            "Dice score : 0.050898317247629166\n",
            "Val loss 0.0376\n",
            "Dice score : 0.07359475642442703\n",
            "Val loss 0.0421\n",
            "Dice score : 0.05996077135205269\n",
            "Val loss 0.0387\n",
            "Dice score : 0.06586125493049622\n",
            "Val loss 0.0377\n",
            "Dice score : 0.07240861654281616\n",
            "Val loss 0.0393\n",
            "Dice score : 0.06150820851325989\n",
            "Val loss 0.0408\n",
            "Dice score : 0.07438170164823532\n",
            "Val loss 0.0398\n",
            "Dice score : 0.04448898509144783\n",
            "Val loss 0.0417\n",
            "Dice score : 0.051789119839668274\n",
            "Val loss 0.0431\n",
            "Dice score : 0.08911238610744476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 42/100 [51:06<1:09:22, 71.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0043 / 0100 | batch 0000 / 0050 | loss 0.0449\n",
            "Train: epoch 0043 / 0100 | batch 0001 / 0050 | loss 0.0370\n",
            "Train: epoch 0043 / 0100 | batch 0002 / 0050 | loss 0.0417\n",
            "Train: epoch 0043 / 0100 | batch 0003 / 0050 | loss 0.0459\n",
            "Train: epoch 0043 / 0100 | batch 0004 / 0050 | loss 0.0468\n",
            "Train: epoch 0043 / 0100 | batch 0005 / 0050 | loss 0.0438\n",
            "Train: epoch 0043 / 0100 | batch 0006 / 0050 | loss 0.0433\n",
            "Train: epoch 0043 / 0100 | batch 0007 / 0050 | loss 0.0412\n",
            "Train: epoch 0043 / 0100 | batch 0008 / 0050 | loss 0.0401\n",
            "Train: epoch 0043 / 0100 | batch 0009 / 0050 | loss 0.0390\n",
            "Train: epoch 0043 / 0100 | batch 0010 / 0050 | loss 0.0378\n",
            "Train: epoch 0043 / 0100 | batch 0011 / 0050 | loss 0.0374\n",
            "Train: epoch 0043 / 0100 | batch 0012 / 0050 | loss 0.0363\n",
            "Train: epoch 0043 / 0100 | batch 0013 / 0050 | loss 0.0384\n",
            "Train: epoch 0043 / 0100 | batch 0014 / 0050 | loss 0.0394\n",
            "Train: epoch 0043 / 0100 | batch 0015 / 0050 | loss 0.0394\n",
            "Train: epoch 0043 / 0100 | batch 0016 / 0050 | loss 0.0396\n",
            "Train: epoch 0043 / 0100 | batch 0017 / 0050 | loss 0.0405\n",
            "Train: epoch 0043 / 0100 | batch 0018 / 0050 | loss 0.0412\n",
            "Train: epoch 0043 / 0100 | batch 0019 / 0050 | loss 0.0415\n",
            "Train: epoch 0043 / 0100 | batch 0020 / 0050 | loss 0.0411\n",
            "Train: epoch 0043 / 0100 | batch 0021 / 0050 | loss 0.0411\n",
            "Train: epoch 0043 / 0100 | batch 0022 / 0050 | loss 0.0405\n",
            "Train: epoch 0043 / 0100 | batch 0023 / 0050 | loss 0.0410\n",
            "Train: epoch 0043 / 0100 | batch 0024 / 0050 | loss 0.0407\n",
            "Train: epoch 0043 / 0100 | batch 0025 / 0050 | loss 0.0406\n",
            "Train: epoch 0043 / 0100 | batch 0026 / 0050 | loss 0.0400\n",
            "Train: epoch 0043 / 0100 | batch 0027 / 0050 | loss 0.0401\n",
            "Train: epoch 0043 / 0100 | batch 0028 / 0050 | loss 0.0396\n",
            "Train: epoch 0043 / 0100 | batch 0029 / 0050 | loss 0.0392\n",
            "Train: epoch 0043 / 0100 | batch 0030 / 0050 | loss 0.0394\n",
            "Train: epoch 0043 / 0100 | batch 0031 / 0050 | loss 0.0397\n",
            "Train: epoch 0043 / 0100 | batch 0032 / 0050 | loss 0.0400\n",
            "Train: epoch 0043 / 0100 | batch 0033 / 0050 | loss 0.0397\n",
            "Train: epoch 0043 / 0100 | batch 0034 / 0050 | loss 0.0406\n",
            "Train: epoch 0043 / 0100 | batch 0035 / 0050 | loss 0.0404\n",
            "Train: epoch 0043 / 0100 | batch 0036 / 0050 | loss 0.0401\n",
            "Train: epoch 0043 / 0100 | batch 0037 / 0050 | loss 0.0399\n",
            "Train: epoch 0043 / 0100 | batch 0038 / 0050 | loss 0.0396\n",
            "Train: epoch 0043 / 0100 | batch 0039 / 0050 | loss 0.0395\n",
            "Train: epoch 0043 / 0100 | batch 0040 / 0050 | loss 0.0396\n",
            "Train: epoch 0043 / 0100 | batch 0041 / 0050 | loss 0.0396\n",
            "Train: epoch 0043 / 0100 | batch 0042 / 0050 | loss 0.0397\n",
            "Train: epoch 0043 / 0100 | batch 0043 / 0050 | loss 0.0396\n",
            "Train: epoch 0043 / 0100 | batch 0044 / 0050 | loss 0.0394\n",
            "Train: epoch 0043 / 0100 | batch 0045 / 0050 | loss 0.0393\n",
            "Train: epoch 0043 / 0100 | batch 0046 / 0050 | loss 0.0391\n",
            "Train: epoch 0043 / 0100 | batch 0047 / 0050 | loss 0.0388\n",
            "Train: epoch 0043 / 0100 | batch 0048 / 0050 | loss 0.0395\n",
            "Train: epoch 0043 / 0100 | batch 0049 / 0050 | loss 0.0395\n",
            "Val loss 0.0545\n",
            "Dice score : 0.06377357989549637\n",
            "Val loss 0.0519\n",
            "Dice score : 0.09836119413375854\n",
            "Val loss 0.0454\n",
            "Dice score : 0.04508000239729881\n",
            "Val loss 0.0463\n",
            "Dice score : 0.07032450288534164\n",
            "Val loss 0.0458\n",
            "Dice score : 0.0635499656200409\n",
            "Val loss 0.0428\n",
            "Dice score : 0.06561299413442612\n",
            "Val loss 0.0437\n",
            "Dice score : 0.06042643263936043\n",
            "Val loss 0.0415\n",
            "Dice score : 0.04907284304499626\n",
            "Val loss 0.0433\n",
            "Dice score : 0.08704893290996552\n",
            "Val loss 0.0420\n",
            "Dice score : 0.07607373595237732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 43/100 [52:18<1:08:20, 71.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0044 / 0100 | batch 0000 / 0050 | loss 0.0407\n",
            "Train: epoch 0044 / 0100 | batch 0001 / 0050 | loss 0.0427\n",
            "Train: epoch 0044 / 0100 | batch 0002 / 0050 | loss 0.0474\n",
            "Train: epoch 0044 / 0100 | batch 0003 / 0050 | loss 0.0437\n",
            "Train: epoch 0044 / 0100 | batch 0004 / 0050 | loss 0.0442\n",
            "Train: epoch 0044 / 0100 | batch 0005 / 0050 | loss 0.0439\n",
            "Train: epoch 0044 / 0100 | batch 0006 / 0050 | loss 0.0428\n",
            "Train: epoch 0044 / 0100 | batch 0007 / 0050 | loss 0.0431\n",
            "Train: epoch 0044 / 0100 | batch 0008 / 0050 | loss 0.0424\n",
            "Train: epoch 0044 / 0100 | batch 0009 / 0050 | loss 0.0410\n",
            "Train: epoch 0044 / 0100 | batch 0010 / 0050 | loss 0.0427\n",
            "Train: epoch 0044 / 0100 | batch 0011 / 0050 | loss 0.0418\n",
            "Train: epoch 0044 / 0100 | batch 0012 / 0050 | loss 0.0411\n",
            "Train: epoch 0044 / 0100 | batch 0013 / 0050 | loss 0.0400\n",
            "Train: epoch 0044 / 0100 | batch 0014 / 0050 | loss 0.0405\n",
            "Train: epoch 0044 / 0100 | batch 0015 / 0050 | loss 0.0399\n",
            "Train: epoch 0044 / 0100 | batch 0016 / 0050 | loss 0.0402\n",
            "Train: epoch 0044 / 0100 | batch 0017 / 0050 | loss 0.0400\n",
            "Train: epoch 0044 / 0100 | batch 0018 / 0050 | loss 0.0407\n",
            "Train: epoch 0044 / 0100 | batch 0019 / 0050 | loss 0.0401\n",
            "Train: epoch 0044 / 0100 | batch 0020 / 0050 | loss 0.0400\n",
            "Train: epoch 0044 / 0100 | batch 0021 / 0050 | loss 0.0400\n",
            "Train: epoch 0044 / 0100 | batch 0022 / 0050 | loss 0.0395\n",
            "Train: epoch 0044 / 0100 | batch 0023 / 0050 | loss 0.0398\n",
            "Train: epoch 0044 / 0100 | batch 0024 / 0050 | loss 0.0402\n",
            "Train: epoch 0044 / 0100 | batch 0025 / 0050 | loss 0.0400\n",
            "Train: epoch 0044 / 0100 | batch 0026 / 0050 | loss 0.0402\n",
            "Train: epoch 0044 / 0100 | batch 0027 / 0050 | loss 0.0402\n",
            "Train: epoch 0044 / 0100 | batch 0028 / 0050 | loss 0.0413\n",
            "Train: epoch 0044 / 0100 | batch 0029 / 0050 | loss 0.0413\n",
            "Train: epoch 0044 / 0100 | batch 0030 / 0050 | loss 0.0410\n",
            "Train: epoch 0044 / 0100 | batch 0031 / 0050 | loss 0.0406\n",
            "Train: epoch 0044 / 0100 | batch 0032 / 0050 | loss 0.0405\n",
            "Train: epoch 0044 / 0100 | batch 0033 / 0050 | loss 0.0402\n",
            "Train: epoch 0044 / 0100 | batch 0034 / 0050 | loss 0.0400\n",
            "Train: epoch 0044 / 0100 | batch 0035 / 0050 | loss 0.0401\n",
            "Train: epoch 0044 / 0100 | batch 0036 / 0050 | loss 0.0400\n",
            "Train: epoch 0044 / 0100 | batch 0037 / 0050 | loss 0.0398\n",
            "Train: epoch 0044 / 0100 | batch 0038 / 0050 | loss 0.0397\n",
            "Train: epoch 0044 / 0100 | batch 0039 / 0050 | loss 0.0397\n",
            "Train: epoch 0044 / 0100 | batch 0040 / 0050 | loss 0.0396\n",
            "Train: epoch 0044 / 0100 | batch 0041 / 0050 | loss 0.0394\n",
            "Train: epoch 0044 / 0100 | batch 0042 / 0050 | loss 0.0396\n",
            "Train: epoch 0044 / 0100 | batch 0043 / 0050 | loss 0.0394\n",
            "Train: epoch 0044 / 0100 | batch 0044 / 0050 | loss 0.0396\n",
            "Train: epoch 0044 / 0100 | batch 0045 / 0050 | loss 0.0398\n",
            "Train: epoch 0044 / 0100 | batch 0046 / 0050 | loss 0.0397\n",
            "Train: epoch 0044 / 0100 | batch 0047 / 0050 | loss 0.0395\n",
            "Train: epoch 0044 / 0100 | batch 0048 / 0050 | loss 0.0393\n",
            "Train: epoch 0044 / 0100 | batch 0049 / 0050 | loss 0.0393\n",
            "Val loss 0.0337\n",
            "Dice score : 0.05567522346973419\n",
            "Val loss 0.0358\n",
            "Dice score : 0.059218015521764755\n",
            "Val loss 0.0393\n",
            "Dice score : 0.08185558766126633\n",
            "Val loss 0.0391\n",
            "Dice score : 0.03295108303427696\n",
            "Val loss 0.0379\n",
            "Dice score : 0.06060212478041649\n",
            "Val loss 0.0392\n",
            "Dice score : 0.06390902400016785\n",
            "Val loss 0.0381\n",
            "Dice score : 0.07308149337768555\n",
            "Val loss 0.0392\n",
            "Dice score : 0.058185599744319916\n",
            "Val loss 0.0411\n",
            "Dice score : 0.08935476094484329\n",
            "Val loss 0.0420\n",
            "Dice score : 0.06884637475013733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 44/100 [53:30<1:07:01, 71.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0045 / 0100 | batch 0000 / 0050 | loss 0.0300\n",
            "Train: epoch 0045 / 0100 | batch 0001 / 0050 | loss 0.0305\n",
            "Train: epoch 0045 / 0100 | batch 0002 / 0050 | loss 0.0356\n",
            "Train: epoch 0045 / 0100 | batch 0003 / 0050 | loss 0.0430\n",
            "Train: epoch 0045 / 0100 | batch 0004 / 0050 | loss 0.0419\n",
            "Train: epoch 0045 / 0100 | batch 0005 / 0050 | loss 0.0403\n",
            "Train: epoch 0045 / 0100 | batch 0006 / 0050 | loss 0.0411\n",
            "Train: epoch 0045 / 0100 | batch 0007 / 0050 | loss 0.0396\n",
            "Train: epoch 0045 / 0100 | batch 0008 / 0050 | loss 0.0416\n",
            "Train: epoch 0045 / 0100 | batch 0009 / 0050 | loss 0.0423\n",
            "Train: epoch 0045 / 0100 | batch 0010 / 0050 | loss 0.0426\n",
            "Train: epoch 0045 / 0100 | batch 0011 / 0050 | loss 0.0453\n",
            "Train: epoch 0045 / 0100 | batch 0012 / 0050 | loss 0.0453\n",
            "Train: epoch 0045 / 0100 | batch 0013 / 0050 | loss 0.0444\n",
            "Train: epoch 0045 / 0100 | batch 0014 / 0050 | loss 0.0445\n",
            "Train: epoch 0045 / 0100 | batch 0015 / 0050 | loss 0.0439\n",
            "Train: epoch 0045 / 0100 | batch 0016 / 0050 | loss 0.0433\n",
            "Train: epoch 0045 / 0100 | batch 0017 / 0050 | loss 0.0424\n",
            "Train: epoch 0045 / 0100 | batch 0018 / 0050 | loss 0.0416\n",
            "Train: epoch 0045 / 0100 | batch 0019 / 0050 | loss 0.0414\n",
            "Train: epoch 0045 / 0100 | batch 0020 / 0050 | loss 0.0410\n",
            "Train: epoch 0045 / 0100 | batch 0021 / 0050 | loss 0.0416\n",
            "Train: epoch 0045 / 0100 | batch 0022 / 0050 | loss 0.0416\n",
            "Train: epoch 0045 / 0100 | batch 0023 / 0050 | loss 0.0412\n",
            "Train: epoch 0045 / 0100 | batch 0024 / 0050 | loss 0.0408\n",
            "Train: epoch 0045 / 0100 | batch 0025 / 0050 | loss 0.0404\n",
            "Train: epoch 0045 / 0100 | batch 0026 / 0050 | loss 0.0405\n",
            "Train: epoch 0045 / 0100 | batch 0027 / 0050 | loss 0.0400\n",
            "Train: epoch 0045 / 0100 | batch 0028 / 0050 | loss 0.0405\n",
            "Train: epoch 0045 / 0100 | batch 0029 / 0050 | loss 0.0405\n",
            "Train: epoch 0045 / 0100 | batch 0030 / 0050 | loss 0.0402\n",
            "Train: epoch 0045 / 0100 | batch 0031 / 0050 | loss 0.0404\n",
            "Train: epoch 0045 / 0100 | batch 0032 / 0050 | loss 0.0407\n",
            "Train: epoch 0045 / 0100 | batch 0033 / 0050 | loss 0.0405\n",
            "Train: epoch 0045 / 0100 | batch 0034 / 0050 | loss 0.0406\n",
            "Train: epoch 0045 / 0100 | batch 0035 / 0050 | loss 0.0405\n",
            "Train: epoch 0045 / 0100 | batch 0036 / 0050 | loss 0.0404\n",
            "Train: epoch 0045 / 0100 | batch 0037 / 0050 | loss 0.0401\n",
            "Train: epoch 0045 / 0100 | batch 0038 / 0050 | loss 0.0397\n",
            "Train: epoch 0045 / 0100 | batch 0039 / 0050 | loss 0.0394\n",
            "Train: epoch 0045 / 0100 | batch 0040 / 0050 | loss 0.0391\n",
            "Train: epoch 0045 / 0100 | batch 0041 / 0050 | loss 0.0393\n",
            "Train: epoch 0045 / 0100 | batch 0042 / 0050 | loss 0.0391\n",
            "Train: epoch 0045 / 0100 | batch 0043 / 0050 | loss 0.0389\n",
            "Train: epoch 0045 / 0100 | batch 0044 / 0050 | loss 0.0390\n",
            "Train: epoch 0045 / 0100 | batch 0045 / 0050 | loss 0.0389\n",
            "Train: epoch 0045 / 0100 | batch 0046 / 0050 | loss 0.0389\n",
            "Train: epoch 0045 / 0100 | batch 0047 / 0050 | loss 0.0390\n",
            "Train: epoch 0045 / 0100 | batch 0048 / 0050 | loss 0.0390\n",
            "Train: epoch 0045 / 0100 | batch 0049 / 0050 | loss 0.0390\n",
            "Val loss 0.0304\n",
            "Dice score : 0.09455449879169464\n",
            "Val loss 0.0372\n",
            "Dice score : 0.03638550639152527\n",
            "Val loss 0.0372\n",
            "Dice score : 0.09815135598182678\n",
            "Val loss 0.0425\n",
            "Dice score : 0.04489458352327347\n",
            "Val loss 0.0477\n",
            "Dice score : 0.11343013495206833\n",
            "Val loss 0.0504\n",
            "Dice score : 0.07470767199993134\n",
            "Val loss 0.0480\n",
            "Dice score : 0.06592407077550888\n",
            "Val loss 0.0491\n",
            "Dice score : 0.10114224255084991\n",
            "Val loss 0.0468\n",
            "Dice score : 0.04897172003984451\n",
            "Val loss 0.0464\n",
            "Dice score : 0.06858444213867188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 45/100 [54:44<1:06:24, 72.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0046 / 0100 | batch 0000 / 0050 | loss 0.0541\n",
            "Train: epoch 0046 / 0100 | batch 0001 / 0050 | loss 0.0502\n",
            "Train: epoch 0046 / 0100 | batch 0002 / 0050 | loss 0.0439\n",
            "Train: epoch 0046 / 0100 | batch 0003 / 0050 | loss 0.0415\n",
            "Train: epoch 0046 / 0100 | batch 0004 / 0050 | loss 0.0390\n",
            "Train: epoch 0046 / 0100 | batch 0005 / 0050 | loss 0.0371\n",
            "Train: epoch 0046 / 0100 | batch 0006 / 0050 | loss 0.0383\n",
            "Train: epoch 0046 / 0100 | batch 0007 / 0050 | loss 0.0381\n",
            "Train: epoch 0046 / 0100 | batch 0008 / 0050 | loss 0.0396\n",
            "Train: epoch 0046 / 0100 | batch 0009 / 0050 | loss 0.0392\n",
            "Train: epoch 0046 / 0100 | batch 0010 / 0050 | loss 0.0390\n",
            "Train: epoch 0046 / 0100 | batch 0011 / 0050 | loss 0.0390\n",
            "Train: epoch 0046 / 0100 | batch 0012 / 0050 | loss 0.0400\n",
            "Train: epoch 0046 / 0100 | batch 0013 / 0050 | loss 0.0391\n",
            "Train: epoch 0046 / 0100 | batch 0014 / 0050 | loss 0.0387\n",
            "Train: epoch 0046 / 0100 | batch 0015 / 0050 | loss 0.0383\n",
            "Train: epoch 0046 / 0100 | batch 0016 / 0050 | loss 0.0378\n",
            "Train: epoch 0046 / 0100 | batch 0017 / 0050 | loss 0.0373\n",
            "Train: epoch 0046 / 0100 | batch 0018 / 0050 | loss 0.0373\n",
            "Train: epoch 0046 / 0100 | batch 0019 / 0050 | loss 0.0370\n",
            "Train: epoch 0046 / 0100 | batch 0020 / 0050 | loss 0.0372\n",
            "Train: epoch 0046 / 0100 | batch 0021 / 0050 | loss 0.0369\n",
            "Train: epoch 0046 / 0100 | batch 0022 / 0050 | loss 0.0364\n",
            "Train: epoch 0046 / 0100 | batch 0023 / 0050 | loss 0.0371\n",
            "Train: epoch 0046 / 0100 | batch 0024 / 0050 | loss 0.0373\n",
            "Train: epoch 0046 / 0100 | batch 0025 / 0050 | loss 0.0369\n",
            "Train: epoch 0046 / 0100 | batch 0026 / 0050 | loss 0.0367\n",
            "Train: epoch 0046 / 0100 | batch 0027 / 0050 | loss 0.0374\n",
            "Train: epoch 0046 / 0100 | batch 0028 / 0050 | loss 0.0381\n",
            "Train: epoch 0046 / 0100 | batch 0029 / 0050 | loss 0.0380\n",
            "Train: epoch 0046 / 0100 | batch 0030 / 0050 | loss 0.0380\n",
            "Train: epoch 0046 / 0100 | batch 0031 / 0050 | loss 0.0378\n",
            "Train: epoch 0046 / 0100 | batch 0032 / 0050 | loss 0.0377\n",
            "Train: epoch 0046 / 0100 | batch 0033 / 0050 | loss 0.0377\n",
            "Train: epoch 0046 / 0100 | batch 0034 / 0050 | loss 0.0380\n",
            "Train: epoch 0046 / 0100 | batch 0035 / 0050 | loss 0.0383\n",
            "Train: epoch 0046 / 0100 | batch 0036 / 0050 | loss 0.0385\n",
            "Train: epoch 0046 / 0100 | batch 0037 / 0050 | loss 0.0394\n",
            "Train: epoch 0046 / 0100 | batch 0038 / 0050 | loss 0.0392\n",
            "Train: epoch 0046 / 0100 | batch 0039 / 0050 | loss 0.0390\n",
            "Train: epoch 0046 / 0100 | batch 0040 / 0050 | loss 0.0392\n",
            "Train: epoch 0046 / 0100 | batch 0041 / 0050 | loss 0.0391\n",
            "Train: epoch 0046 / 0100 | batch 0042 / 0050 | loss 0.0388\n",
            "Train: epoch 0046 / 0100 | batch 0043 / 0050 | loss 0.0387\n",
            "Train: epoch 0046 / 0100 | batch 0044 / 0050 | loss 0.0385\n",
            "Train: epoch 0046 / 0100 | batch 0045 / 0050 | loss 0.0387\n",
            "Train: epoch 0046 / 0100 | batch 0046 / 0050 | loss 0.0386\n",
            "Train: epoch 0046 / 0100 | batch 0047 / 0050 | loss 0.0385\n",
            "Train: epoch 0046 / 0100 | batch 0048 / 0050 | loss 0.0390\n",
            "Train: epoch 0046 / 0100 | batch 0049 / 0050 | loss 0.0388\n",
            "Val loss 0.0566\n",
            "Dice score : 0.08779381960630417\n",
            "Val loss 0.0524\n",
            "Dice score : 0.09718404710292816\n",
            "Val loss 0.0542\n",
            "Dice score : 0.07542116194963455\n",
            "Val loss 0.0533\n",
            "Dice score : 0.09636761993169785\n",
            "Val loss 0.0502\n",
            "Dice score : 0.10122093558311462\n",
            "Val loss 0.0463\n",
            "Dice score : 0.05458224192261696\n",
            "Val loss 0.0484\n",
            "Dice score : 0.08739522844552994\n",
            "Val loss 0.0459\n",
            "Dice score : 0.04579180106520653\n",
            "Val loss 0.0452\n",
            "Dice score : 0.042549509555101395\n",
            "Val loss 0.0441\n",
            "Dice score : 0.0670972689986229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 46/100 [55:55<1:04:54, 72.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0047 / 0100 | batch 0000 / 0050 | loss 0.0253\n",
            "Train: epoch 0047 / 0100 | batch 0001 / 0050 | loss 0.0275\n",
            "Train: epoch 0047 / 0100 | batch 0002 / 0050 | loss 0.0360\n",
            "Train: epoch 0047 / 0100 | batch 0003 / 0050 | loss 0.0390\n",
            "Train: epoch 0047 / 0100 | batch 0004 / 0050 | loss 0.0397\n",
            "Train: epoch 0047 / 0100 | batch 0005 / 0050 | loss 0.0396\n",
            "Train: epoch 0047 / 0100 | batch 0006 / 0050 | loss 0.0381\n",
            "Train: epoch 0047 / 0100 | batch 0007 / 0050 | loss 0.0375\n",
            "Train: epoch 0047 / 0100 | batch 0008 / 0050 | loss 0.0363\n",
            "Train: epoch 0047 / 0100 | batch 0009 / 0050 | loss 0.0382\n",
            "Train: epoch 0047 / 0100 | batch 0010 / 0050 | loss 0.0392\n",
            "Train: epoch 0047 / 0100 | batch 0011 / 0050 | loss 0.0393\n",
            "Train: epoch 0047 / 0100 | batch 0012 / 0050 | loss 0.0392\n",
            "Train: epoch 0047 / 0100 | batch 0013 / 0050 | loss 0.0391\n",
            "Train: epoch 0047 / 0100 | batch 0014 / 0050 | loss 0.0384\n",
            "Train: epoch 0047 / 0100 | batch 0015 / 0050 | loss 0.0385\n",
            "Train: epoch 0047 / 0100 | batch 0016 / 0050 | loss 0.0385\n",
            "Train: epoch 0047 / 0100 | batch 0017 / 0050 | loss 0.0384\n",
            "Train: epoch 0047 / 0100 | batch 0018 / 0050 | loss 0.0387\n",
            "Train: epoch 0047 / 0100 | batch 0019 / 0050 | loss 0.0392\n",
            "Train: epoch 0047 / 0100 | batch 0020 / 0050 | loss 0.0391\n",
            "Train: epoch 0047 / 0100 | batch 0021 / 0050 | loss 0.0384\n",
            "Train: epoch 0047 / 0100 | batch 0022 / 0050 | loss 0.0379\n",
            "Train: epoch 0047 / 0100 | batch 0023 / 0050 | loss 0.0378\n",
            "Train: epoch 0047 / 0100 | batch 0024 / 0050 | loss 0.0380\n",
            "Train: epoch 0047 / 0100 | batch 0025 / 0050 | loss 0.0385\n",
            "Train: epoch 0047 / 0100 | batch 0026 / 0050 | loss 0.0382\n",
            "Train: epoch 0047 / 0100 | batch 0027 / 0050 | loss 0.0386\n",
            "Train: epoch 0047 / 0100 | batch 0028 / 0050 | loss 0.0382\n",
            "Train: epoch 0047 / 0100 | batch 0029 / 0050 | loss 0.0379\n",
            "Train: epoch 0047 / 0100 | batch 0030 / 0050 | loss 0.0377\n",
            "Train: epoch 0047 / 0100 | batch 0031 / 0050 | loss 0.0376\n",
            "Train: epoch 0047 / 0100 | batch 0032 / 0050 | loss 0.0379\n",
            "Train: epoch 0047 / 0100 | batch 0033 / 0050 | loss 0.0377\n",
            "Train: epoch 0047 / 0100 | batch 0034 / 0050 | loss 0.0376\n",
            "Train: epoch 0047 / 0100 | batch 0035 / 0050 | loss 0.0374\n",
            "Train: epoch 0047 / 0100 | batch 0036 / 0050 | loss 0.0373\n",
            "Train: epoch 0047 / 0100 | batch 0037 / 0050 | loss 0.0375\n",
            "Train: epoch 0047 / 0100 | batch 0038 / 0050 | loss 0.0378\n",
            "Train: epoch 0047 / 0100 | batch 0039 / 0050 | loss 0.0379\n",
            "Train: epoch 0047 / 0100 | batch 0040 / 0050 | loss 0.0383\n",
            "Train: epoch 0047 / 0100 | batch 0041 / 0050 | loss 0.0381\n",
            "Train: epoch 0047 / 0100 | batch 0042 / 0050 | loss 0.0382\n",
            "Train: epoch 0047 / 0100 | batch 0043 / 0050 | loss 0.0382\n",
            "Train: epoch 0047 / 0100 | batch 0044 / 0050 | loss 0.0387\n",
            "Train: epoch 0047 / 0100 | batch 0045 / 0050 | loss 0.0389\n",
            "Train: epoch 0047 / 0100 | batch 0046 / 0050 | loss 0.0393\n",
            "Train: epoch 0047 / 0100 | batch 0047 / 0050 | loss 0.0390\n",
            "Train: epoch 0047 / 0100 | batch 0048 / 0050 | loss 0.0392\n",
            "Train: epoch 0047 / 0100 | batch 0049 / 0050 | loss 0.0390\n",
            "Val loss 0.0316\n",
            "Dice score : 0.06615330278873444\n",
            "Val loss 0.0339\n",
            "Dice score : 0.06504843384027481\n",
            "Val loss 0.0342\n",
            "Dice score : 0.0704410970211029\n",
            "Val loss 0.0386\n",
            "Dice score : 0.07077769190073013\n",
            "Val loss 0.0360\n",
            "Dice score : 0.06245620176196098\n",
            "Val loss 0.0351\n",
            "Dice score : 0.05513182282447815\n",
            "Val loss 0.0368\n",
            "Dice score : 0.10889549553394318\n",
            "Val loss 0.0389\n",
            "Dice score : 0.05826631560921669\n",
            "Val loss 0.0392\n",
            "Dice score : 0.07271707057952881\n",
            "Val loss 0.0402\n",
            "Dice score : 0.09013808518648148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 47/100 [57:08<1:03:58, 72.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0048 / 0100 | batch 0000 / 0050 | loss 0.0307\n",
            "Train: epoch 0048 / 0100 | batch 0001 / 0050 | loss 0.0305\n",
            "Train: epoch 0048 / 0100 | batch 0002 / 0050 | loss 0.0355\n",
            "Train: epoch 0048 / 0100 | batch 0003 / 0050 | loss 0.0342\n",
            "Train: epoch 0048 / 0100 | batch 0004 / 0050 | loss 0.0334\n",
            "Train: epoch 0048 / 0100 | batch 0005 / 0050 | loss 0.0336\n",
            "Train: epoch 0048 / 0100 | batch 0006 / 0050 | loss 0.0329\n",
            "Train: epoch 0048 / 0100 | batch 0007 / 0050 | loss 0.0347\n",
            "Train: epoch 0048 / 0100 | batch 0008 / 0050 | loss 0.0340\n",
            "Train: epoch 0048 / 0100 | batch 0009 / 0050 | loss 0.0347\n",
            "Train: epoch 0048 / 0100 | batch 0010 / 0050 | loss 0.0358\n",
            "Train: epoch 0048 / 0100 | batch 0011 / 0050 | loss 0.0353\n",
            "Train: epoch 0048 / 0100 | batch 0012 / 0050 | loss 0.0359\n",
            "Train: epoch 0048 / 0100 | batch 0013 / 0050 | loss 0.0374\n",
            "Train: epoch 0048 / 0100 | batch 0014 / 0050 | loss 0.0371\n",
            "Train: epoch 0048 / 0100 | batch 0015 / 0050 | loss 0.0365\n",
            "Train: epoch 0048 / 0100 | batch 0016 / 0050 | loss 0.0361\n",
            "Train: epoch 0048 / 0100 | batch 0017 / 0050 | loss 0.0367\n",
            "Train: epoch 0048 / 0100 | batch 0018 / 0050 | loss 0.0373\n",
            "Train: epoch 0048 / 0100 | batch 0019 / 0050 | loss 0.0377\n",
            "Train: epoch 0048 / 0100 | batch 0020 / 0050 | loss 0.0375\n",
            "Train: epoch 0048 / 0100 | batch 0021 / 0050 | loss 0.0372\n",
            "Train: epoch 0048 / 0100 | batch 0022 / 0050 | loss 0.0375\n",
            "Train: epoch 0048 / 0100 | batch 0023 / 0050 | loss 0.0374\n",
            "Train: epoch 0048 / 0100 | batch 0024 / 0050 | loss 0.0370\n",
            "Train: epoch 0048 / 0100 | batch 0025 / 0050 | loss 0.0372\n",
            "Train: epoch 0048 / 0100 | batch 0026 / 0050 | loss 0.0368\n",
            "Train: epoch 0048 / 0100 | batch 0027 / 0050 | loss 0.0367\n",
            "Train: epoch 0048 / 0100 | batch 0028 / 0050 | loss 0.0371\n",
            "Train: epoch 0048 / 0100 | batch 0029 / 0050 | loss 0.0370\n",
            "Train: epoch 0048 / 0100 | batch 0030 / 0050 | loss 0.0367\n",
            "Train: epoch 0048 / 0100 | batch 0031 / 0050 | loss 0.0371\n",
            "Train: epoch 0048 / 0100 | batch 0032 / 0050 | loss 0.0370\n",
            "Train: epoch 0048 / 0100 | batch 0033 / 0050 | loss 0.0374\n",
            "Train: epoch 0048 / 0100 | batch 0034 / 0050 | loss 0.0375\n",
            "Train: epoch 0048 / 0100 | batch 0035 / 0050 | loss 0.0375\n",
            "Train: epoch 0048 / 0100 | batch 0036 / 0050 | loss 0.0373\n",
            "Train: epoch 0048 / 0100 | batch 0037 / 0050 | loss 0.0386\n",
            "Train: epoch 0048 / 0100 | batch 0038 / 0050 | loss 0.0387\n",
            "Train: epoch 0048 / 0100 | batch 0039 / 0050 | loss 0.0385\n",
            "Train: epoch 0048 / 0100 | batch 0040 / 0050 | loss 0.0383\n",
            "Train: epoch 0048 / 0100 | batch 0041 / 0050 | loss 0.0381\n",
            "Train: epoch 0048 / 0100 | batch 0042 / 0050 | loss 0.0386\n",
            "Train: epoch 0048 / 0100 | batch 0043 / 0050 | loss 0.0384\n",
            "Train: epoch 0048 / 0100 | batch 0044 / 0050 | loss 0.0386\n",
            "Train: epoch 0048 / 0100 | batch 0045 / 0050 | loss 0.0387\n",
            "Train: epoch 0048 / 0100 | batch 0046 / 0050 | loss 0.0385\n",
            "Train: epoch 0048 / 0100 | batch 0047 / 0050 | loss 0.0384\n",
            "Train: epoch 0048 / 0100 | batch 0048 / 0050 | loss 0.0386\n",
            "Train: epoch 0048 / 0100 | batch 0049 / 0050 | loss 0.0384\n",
            "Val loss 0.0501\n",
            "Dice score : 0.09113045036792755\n",
            "Val loss 0.0409\n",
            "Dice score : 0.055610623210668564\n",
            "Val loss 0.0384\n",
            "Dice score : 0.061061158776283264\n",
            "Val loss 0.0352\n",
            "Dice score : 0.0585872121155262\n",
            "Val loss 0.0389\n",
            "Dice score : 0.07635767012834549\n",
            "Val loss 0.0414\n",
            "Dice score : 0.0854247659444809\n",
            "Val loss 0.0426\n",
            "Dice score : 0.05871418118476868\n",
            "Val loss 0.0442\n",
            "Dice score : 0.08002543449401855\n",
            "Val loss 0.0447\n",
            "Dice score : 0.0444897897541523\n",
            "Val loss 0.0432\n",
            "Dice score : 0.08267886191606522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 48/100 [58:22<1:03:01, 72.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0049 / 0100 | batch 0000 / 0050 | loss 0.0698\n",
            "Train: epoch 0049 / 0100 | batch 0001 / 0050 | loss 0.0484\n",
            "Train: epoch 0049 / 0100 | batch 0002 / 0050 | loss 0.0481\n",
            "Train: epoch 0049 / 0100 | batch 0003 / 0050 | loss 0.0507\n",
            "Train: epoch 0049 / 0100 | batch 0004 / 0050 | loss 0.0463\n",
            "Train: epoch 0049 / 0100 | batch 0005 / 0050 | loss 0.0464\n",
            "Train: epoch 0049 / 0100 | batch 0006 / 0050 | loss 0.0454\n",
            "Train: epoch 0049 / 0100 | batch 0007 / 0050 | loss 0.0441\n",
            "Train: epoch 0049 / 0100 | batch 0008 / 0050 | loss 0.0430\n",
            "Train: epoch 0049 / 0100 | batch 0009 / 0050 | loss 0.0432\n",
            "Train: epoch 0049 / 0100 | batch 0010 / 0050 | loss 0.0428\n",
            "Train: epoch 0049 / 0100 | batch 0011 / 0050 | loss 0.0430\n",
            "Train: epoch 0049 / 0100 | batch 0012 / 0050 | loss 0.0422\n",
            "Train: epoch 0049 / 0100 | batch 0013 / 0050 | loss 0.0412\n",
            "Train: epoch 0049 / 0100 | batch 0014 / 0050 | loss 0.0407\n",
            "Train: epoch 0049 / 0100 | batch 0015 / 0050 | loss 0.0402\n",
            "Train: epoch 0049 / 0100 | batch 0016 / 0050 | loss 0.0395\n",
            "Train: epoch 0049 / 0100 | batch 0017 / 0050 | loss 0.0398\n",
            "Train: epoch 0049 / 0100 | batch 0018 / 0050 | loss 0.0400\n",
            "Train: epoch 0049 / 0100 | batch 0019 / 0050 | loss 0.0394\n",
            "Train: epoch 0049 / 0100 | batch 0020 / 0050 | loss 0.0390\n",
            "Train: epoch 0049 / 0100 | batch 0021 / 0050 | loss 0.0391\n",
            "Train: epoch 0049 / 0100 | batch 0022 / 0050 | loss 0.0392\n",
            "Train: epoch 0049 / 0100 | batch 0023 / 0050 | loss 0.0389\n",
            "Train: epoch 0049 / 0100 | batch 0024 / 0050 | loss 0.0385\n",
            "Train: epoch 0049 / 0100 | batch 0025 / 0050 | loss 0.0382\n",
            "Train: epoch 0049 / 0100 | batch 0026 / 0050 | loss 0.0381\n",
            "Train: epoch 0049 / 0100 | batch 0027 / 0050 | loss 0.0384\n",
            "Train: epoch 0049 / 0100 | batch 0028 / 0050 | loss 0.0380\n",
            "Train: epoch 0049 / 0100 | batch 0029 / 0050 | loss 0.0380\n",
            "Train: epoch 0049 / 0100 | batch 0030 / 0050 | loss 0.0382\n",
            "Train: epoch 0049 / 0100 | batch 0031 / 0050 | loss 0.0382\n",
            "Train: epoch 0049 / 0100 | batch 0032 / 0050 | loss 0.0378\n",
            "Train: epoch 0049 / 0100 | batch 0033 / 0050 | loss 0.0375\n",
            "Train: epoch 0049 / 0100 | batch 0034 / 0050 | loss 0.0376\n",
            "Train: epoch 0049 / 0100 | batch 0035 / 0050 | loss 0.0376\n",
            "Train: epoch 0049 / 0100 | batch 0036 / 0050 | loss 0.0378\n",
            "Train: epoch 0049 / 0100 | batch 0037 / 0050 | loss 0.0375\n",
            "Train: epoch 0049 / 0100 | batch 0038 / 0050 | loss 0.0376\n",
            "Train: epoch 0049 / 0100 | batch 0039 / 0050 | loss 0.0376\n",
            "Train: epoch 0049 / 0100 | batch 0040 / 0050 | loss 0.0378\n",
            "Train: epoch 0049 / 0100 | batch 0041 / 0050 | loss 0.0376\n",
            "Train: epoch 0049 / 0100 | batch 0042 / 0050 | loss 0.0376\n",
            "Train: epoch 0049 / 0100 | batch 0043 / 0050 | loss 0.0372\n",
            "Train: epoch 0049 / 0100 | batch 0044 / 0050 | loss 0.0369\n",
            "Train: epoch 0049 / 0100 | batch 0045 / 0050 | loss 0.0374\n",
            "Train: epoch 0049 / 0100 | batch 0046 / 0050 | loss 0.0372\n",
            "Train: epoch 0049 / 0100 | batch 0047 / 0050 | loss 0.0379\n",
            "Train: epoch 0049 / 0100 | batch 0048 / 0050 | loss 0.0376\n",
            "Train: epoch 0049 / 0100 | batch 0049 / 0050 | loss 0.0376\n",
            "Val loss 0.0487\n",
            "Dice score : 0.07989317923784256\n",
            "Val loss 0.0412\n",
            "Dice score : 0.0561799593269825\n",
            "Val loss 0.0366\n",
            "Dice score : 0.05723527818918228\n",
            "Val loss 0.0421\n",
            "Dice score : 0.08407115191221237\n",
            "Val loss 0.0470\n",
            "Dice score : 0.049504730850458145\n",
            "Val loss 0.0442\n",
            "Dice score : 0.07836733013391495\n",
            "Val loss 0.0415\n",
            "Dice score : 0.05671593174338341\n",
            "Val loss 0.0416\n",
            "Dice score : 0.04917474836111069\n",
            "Val loss 0.0450\n",
            "Dice score : 0.11958655714988708\n",
            "Val loss 0.0448\n",
            "Dice score : 0.06731937825679779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 49/100 [59:33<1:01:25, 72.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0050 / 0100 | batch 0000 / 0050 | loss 0.0308\n",
            "Train: epoch 0050 / 0100 | batch 0001 / 0050 | loss 0.0326\n",
            "Train: epoch 0050 / 0100 | batch 0002 / 0050 | loss 0.0394\n",
            "Train: epoch 0050 / 0100 | batch 0003 / 0050 | loss 0.0392\n",
            "Train: epoch 0050 / 0100 | batch 0004 / 0050 | loss 0.0372\n",
            "Train: epoch 0050 / 0100 | batch 0005 / 0050 | loss 0.0404\n",
            "Train: epoch 0050 / 0100 | batch 0006 / 0050 | loss 0.0423\n",
            "Train: epoch 0050 / 0100 | batch 0007 / 0050 | loss 0.0420\n",
            "Train: epoch 0050 / 0100 | batch 0008 / 0050 | loss 0.0422\n",
            "Train: epoch 0050 / 0100 | batch 0009 / 0050 | loss 0.0405\n",
            "Train: epoch 0050 / 0100 | batch 0010 / 0050 | loss 0.0425\n",
            "Train: epoch 0050 / 0100 | batch 0011 / 0050 | loss 0.0422\n",
            "Train: epoch 0050 / 0100 | batch 0012 / 0050 | loss 0.0417\n",
            "Train: epoch 0050 / 0100 | batch 0013 / 0050 | loss 0.0412\n",
            "Train: epoch 0050 / 0100 | batch 0014 / 0050 | loss 0.0425\n",
            "Train: epoch 0050 / 0100 | batch 0015 / 0050 | loss 0.0421\n",
            "Train: epoch 0050 / 0100 | batch 0016 / 0050 | loss 0.0413\n",
            "Train: epoch 0050 / 0100 | batch 0017 / 0050 | loss 0.0412\n",
            "Train: epoch 0050 / 0100 | batch 0018 / 0050 | loss 0.0404\n",
            "Train: epoch 0050 / 0100 | batch 0019 / 0050 | loss 0.0402\n",
            "Train: epoch 0050 / 0100 | batch 0020 / 0050 | loss 0.0402\n",
            "Train: epoch 0050 / 0100 | batch 0021 / 0050 | loss 0.0395\n",
            "Train: epoch 0050 / 0100 | batch 0022 / 0050 | loss 0.0395\n",
            "Train: epoch 0050 / 0100 | batch 0023 / 0050 | loss 0.0398\n",
            "Train: epoch 0050 / 0100 | batch 0024 / 0050 | loss 0.0400\n",
            "Train: epoch 0050 / 0100 | batch 0025 / 0050 | loss 0.0398\n",
            "Train: epoch 0050 / 0100 | batch 0026 / 0050 | loss 0.0393\n",
            "Train: epoch 0050 / 0100 | batch 0027 / 0050 | loss 0.0391\n",
            "Train: epoch 0050 / 0100 | batch 0028 / 0050 | loss 0.0387\n",
            "Train: epoch 0050 / 0100 | batch 0029 / 0050 | loss 0.0383\n",
            "Train: epoch 0050 / 0100 | batch 0030 / 0050 | loss 0.0385\n",
            "Train: epoch 0050 / 0100 | batch 0031 / 0050 | loss 0.0381\n",
            "Train: epoch 0050 / 0100 | batch 0032 / 0050 | loss 0.0382\n",
            "Train: epoch 0050 / 0100 | batch 0033 / 0050 | loss 0.0380\n",
            "Train: epoch 0050 / 0100 | batch 0034 / 0050 | loss 0.0381\n",
            "Train: epoch 0050 / 0100 | batch 0035 / 0050 | loss 0.0382\n",
            "Train: epoch 0050 / 0100 | batch 0036 / 0050 | loss 0.0378\n",
            "Train: epoch 0050 / 0100 | batch 0037 / 0050 | loss 0.0375\n",
            "Train: epoch 0050 / 0100 | batch 0038 / 0050 | loss 0.0376\n",
            "Train: epoch 0050 / 0100 | batch 0039 / 0050 | loss 0.0374\n",
            "Train: epoch 0050 / 0100 | batch 0040 / 0050 | loss 0.0371\n",
            "Train: epoch 0050 / 0100 | batch 0041 / 0050 | loss 0.0381\n",
            "Train: epoch 0050 / 0100 | batch 0042 / 0050 | loss 0.0379\n",
            "Train: epoch 0050 / 0100 | batch 0043 / 0050 | loss 0.0377\n",
            "Train: epoch 0050 / 0100 | batch 0044 / 0050 | loss 0.0380\n",
            "Train: epoch 0050 / 0100 | batch 0045 / 0050 | loss 0.0378\n",
            "Train: epoch 0050 / 0100 | batch 0046 / 0050 | loss 0.0378\n",
            "Train: epoch 0050 / 0100 | batch 0047 / 0050 | loss 0.0376\n",
            "Train: epoch 0050 / 0100 | batch 0048 / 0050 | loss 0.0375\n",
            "Train: epoch 0050 / 0100 | batch 0049 / 0050 | loss 0.0375\n",
            "Val loss 0.0470\n",
            "Dice score : 0.11318977922201157\n",
            "Val loss 0.0371\n",
            "Dice score : 0.039703406393527985\n",
            "Val loss 0.0325\n",
            "Dice score : 0.06023668870329857\n",
            "Val loss 0.0467\n",
            "Dice score : 0.08109777420759201\n",
            "Val loss 0.0428\n",
            "Dice score : 0.04703781381249428\n",
            "Val loss 0.0422\n",
            "Dice score : 0.06285812705755234\n",
            "Val loss 0.0423\n",
            "Dice score : 0.08855787664651871\n",
            "Val loss 0.0429\n",
            "Dice score : 0.07455049455165863\n",
            "Val loss 0.0442\n",
            "Dice score : 0.10430768877267838\n",
            "Val loss 0.0427\n",
            "Dice score : 0.07456493377685547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 50/100 [1:00:46<1:00:33, 72.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0051 / 0100 | batch 0000 / 0050 | loss 0.0319\n",
            "Train: epoch 0051 / 0100 | batch 0001 / 0050 | loss 0.0358\n",
            "Train: epoch 0051 / 0100 | batch 0002 / 0050 | loss 0.0341\n",
            "Train: epoch 0051 / 0100 | batch 0003 / 0050 | loss 0.0358\n",
            "Train: epoch 0051 / 0100 | batch 0004 / 0050 | loss 0.0335\n",
            "Train: epoch 0051 / 0100 | batch 0005 / 0050 | loss 0.0340\n",
            "Train: epoch 0051 / 0100 | batch 0006 / 0050 | loss 0.0355\n",
            "Train: epoch 0051 / 0100 | batch 0007 / 0050 | loss 0.0367\n",
            "Train: epoch 0051 / 0100 | batch 0008 / 0050 | loss 0.0375\n",
            "Train: epoch 0051 / 0100 | batch 0009 / 0050 | loss 0.0366\n",
            "Train: epoch 0051 / 0100 | batch 0010 / 0050 | loss 0.0357\n",
            "Train: epoch 0051 / 0100 | batch 0011 / 0050 | loss 0.0355\n",
            "Train: epoch 0051 / 0100 | batch 0012 / 0050 | loss 0.0374\n",
            "Train: epoch 0051 / 0100 | batch 0013 / 0050 | loss 0.0370\n",
            "Train: epoch 0051 / 0100 | batch 0014 / 0050 | loss 0.0398\n",
            "Train: epoch 0051 / 0100 | batch 0015 / 0050 | loss 0.0396\n",
            "Train: epoch 0051 / 0100 | batch 0016 / 0050 | loss 0.0396\n",
            "Train: epoch 0051 / 0100 | batch 0017 / 0050 | loss 0.0408\n",
            "Train: epoch 0051 / 0100 | batch 0018 / 0050 | loss 0.0405\n",
            "Train: epoch 0051 / 0100 | batch 0019 / 0050 | loss 0.0403\n",
            "Train: epoch 0051 / 0100 | batch 0020 / 0050 | loss 0.0398\n",
            "Train: epoch 0051 / 0100 | batch 0021 / 0050 | loss 0.0398\n",
            "Train: epoch 0051 / 0100 | batch 0022 / 0050 | loss 0.0396\n",
            "Train: epoch 0051 / 0100 | batch 0023 / 0050 | loss 0.0405\n",
            "Train: epoch 0051 / 0100 | batch 0024 / 0050 | loss 0.0407\n",
            "Train: epoch 0051 / 0100 | batch 0025 / 0050 | loss 0.0402\n",
            "Train: epoch 0051 / 0100 | batch 0026 / 0050 | loss 0.0398\n",
            "Train: epoch 0051 / 0100 | batch 0027 / 0050 | loss 0.0402\n",
            "Train: epoch 0051 / 0100 | batch 0028 / 0050 | loss 0.0399\n",
            "Train: epoch 0051 / 0100 | batch 0029 / 0050 | loss 0.0394\n",
            "Train: epoch 0051 / 0100 | batch 0030 / 0050 | loss 0.0390\n",
            "Train: epoch 0051 / 0100 | batch 0031 / 0050 | loss 0.0386\n",
            "Train: epoch 0051 / 0100 | batch 0032 / 0050 | loss 0.0383\n",
            "Train: epoch 0051 / 0100 | batch 0033 / 0050 | loss 0.0381\n",
            "Train: epoch 0051 / 0100 | batch 0034 / 0050 | loss 0.0383\n",
            "Train: epoch 0051 / 0100 | batch 0035 / 0050 | loss 0.0386\n",
            "Train: epoch 0051 / 0100 | batch 0036 / 0050 | loss 0.0382\n",
            "Train: epoch 0051 / 0100 | batch 0037 / 0050 | loss 0.0380\n",
            "Train: epoch 0051 / 0100 | batch 0038 / 0050 | loss 0.0377\n",
            "Train: epoch 0051 / 0100 | batch 0039 / 0050 | loss 0.0375\n",
            "Train: epoch 0051 / 0100 | batch 0040 / 0050 | loss 0.0374\n",
            "Train: epoch 0051 / 0100 | batch 0041 / 0050 | loss 0.0373\n",
            "Train: epoch 0051 / 0100 | batch 0042 / 0050 | loss 0.0372\n",
            "Train: epoch 0051 / 0100 | batch 0043 / 0050 | loss 0.0370\n",
            "Train: epoch 0051 / 0100 | batch 0044 / 0050 | loss 0.0370\n",
            "Train: epoch 0051 / 0100 | batch 0045 / 0050 | loss 0.0368\n",
            "Train: epoch 0051 / 0100 | batch 0046 / 0050 | loss 0.0369\n",
            "Train: epoch 0051 / 0100 | batch 0047 / 0050 | loss 0.0367\n",
            "Train: epoch 0051 / 0100 | batch 0048 / 0050 | loss 0.0371\n",
            "Train: epoch 0051 / 0100 | batch 0049 / 0050 | loss 0.0372\n",
            "Val loss 0.0295\n",
            "Dice score : 0.0824575126171112\n",
            "Val loss 0.0384\n",
            "Dice score : 0.08545640856027603\n",
            "Val loss 0.0368\n",
            "Dice score : 0.049158357083797455\n",
            "Val loss 0.0360\n",
            "Dice score : 0.0842970460653305\n",
            "Val loss 0.0431\n",
            "Dice score : 0.09274516999721527\n",
            "Val loss 0.0418\n",
            "Dice score : 0.08751175552606583\n",
            "Val loss 0.0458\n",
            "Dice score : 0.09529804438352585\n",
            "Val loss 0.0447\n",
            "Dice score : 0.03275630250573158\n",
            "Val loss 0.0428\n",
            "Dice score : 0.04841027036309242\n",
            "Val loss 0.0437\n",
            "Dice score : 0.052669018507003784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 51/100 [1:01:59<59:16, 72.57s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0052 / 0100 | batch 0000 / 0050 | loss 0.0720\n",
            "Train: epoch 0052 / 0100 | batch 0001 / 0050 | loss 0.0597\n",
            "Train: epoch 0052 / 0100 | batch 0002 / 0050 | loss 0.0522\n",
            "Train: epoch 0052 / 0100 | batch 0003 / 0050 | loss 0.0498\n",
            "Train: epoch 0052 / 0100 | batch 0004 / 0050 | loss 0.0465\n",
            "Train: epoch 0052 / 0100 | batch 0005 / 0050 | loss 0.0452\n",
            "Train: epoch 0052 / 0100 | batch 0006 / 0050 | loss 0.0433\n",
            "Train: epoch 0052 / 0100 | batch 0007 / 0050 | loss 0.0428\n",
            "Train: epoch 0052 / 0100 | batch 0008 / 0050 | loss 0.0418\n",
            "Train: epoch 0052 / 0100 | batch 0009 / 0050 | loss 0.0409\n",
            "Train: epoch 0052 / 0100 | batch 0010 / 0050 | loss 0.0421\n",
            "Train: epoch 0052 / 0100 | batch 0011 / 0050 | loss 0.0409\n",
            "Train: epoch 0052 / 0100 | batch 0012 / 0050 | loss 0.0402\n",
            "Train: epoch 0052 / 0100 | batch 0013 / 0050 | loss 0.0395\n",
            "Train: epoch 0052 / 0100 | batch 0014 / 0050 | loss 0.0384\n",
            "Train: epoch 0052 / 0100 | batch 0015 / 0050 | loss 0.0380\n",
            "Train: epoch 0052 / 0100 | batch 0016 / 0050 | loss 0.0391\n",
            "Train: epoch 0052 / 0100 | batch 0017 / 0050 | loss 0.0385\n",
            "Train: epoch 0052 / 0100 | batch 0018 / 0050 | loss 0.0385\n",
            "Train: epoch 0052 / 0100 | batch 0019 / 0050 | loss 0.0381\n",
            "Train: epoch 0052 / 0100 | batch 0020 / 0050 | loss 0.0375\n",
            "Train: epoch 0052 / 0100 | batch 0021 / 0050 | loss 0.0377\n",
            "Train: epoch 0052 / 0100 | batch 0022 / 0050 | loss 0.0372\n",
            "Train: epoch 0052 / 0100 | batch 0023 / 0050 | loss 0.0373\n",
            "Train: epoch 0052 / 0100 | batch 0024 / 0050 | loss 0.0384\n",
            "Train: epoch 0052 / 0100 | batch 0025 / 0050 | loss 0.0378\n",
            "Train: epoch 0052 / 0100 | batch 0026 / 0050 | loss 0.0377\n",
            "Train: epoch 0052 / 0100 | batch 0027 / 0050 | loss 0.0373\n",
            "Train: epoch 0052 / 0100 | batch 0028 / 0050 | loss 0.0382\n",
            "Train: epoch 0052 / 0100 | batch 0029 / 0050 | loss 0.0383\n",
            "Train: epoch 0052 / 0100 | batch 0030 / 0050 | loss 0.0382\n",
            "Train: epoch 0052 / 0100 | batch 0031 / 0050 | loss 0.0387\n",
            "Train: epoch 0052 / 0100 | batch 0032 / 0050 | loss 0.0384\n",
            "Train: epoch 0052 / 0100 | batch 0033 / 0050 | loss 0.0387\n",
            "Train: epoch 0052 / 0100 | batch 0034 / 0050 | loss 0.0389\n",
            "Train: epoch 0052 / 0100 | batch 0035 / 0050 | loss 0.0385\n",
            "Train: epoch 0052 / 0100 | batch 0036 / 0050 | loss 0.0389\n",
            "Train: epoch 0052 / 0100 | batch 0037 / 0050 | loss 0.0388\n",
            "Train: epoch 0052 / 0100 | batch 0038 / 0050 | loss 0.0384\n",
            "Train: epoch 0052 / 0100 | batch 0039 / 0050 | loss 0.0384\n",
            "Train: epoch 0052 / 0100 | batch 0040 / 0050 | loss 0.0388\n",
            "Train: epoch 0052 / 0100 | batch 0041 / 0050 | loss 0.0392\n",
            "Train: epoch 0052 / 0100 | batch 0042 / 0050 | loss 0.0388\n",
            "Train: epoch 0052 / 0100 | batch 0043 / 0050 | loss 0.0389\n",
            "Train: epoch 0052 / 0100 | batch 0044 / 0050 | loss 0.0387\n",
            "Train: epoch 0052 / 0100 | batch 0045 / 0050 | loss 0.0384\n",
            "Train: epoch 0052 / 0100 | batch 0046 / 0050 | loss 0.0383\n",
            "Train: epoch 0052 / 0100 | batch 0047 / 0050 | loss 0.0383\n",
            "Train: epoch 0052 / 0100 | batch 0048 / 0050 | loss 0.0381\n",
            "Train: epoch 0052 / 0100 | batch 0049 / 0050 | loss 0.0379\n",
            "Val loss 0.0478\n",
            "Dice score : 0.11146138608455658\n",
            "Val loss 0.0407\n",
            "Dice score : 0.05214633792638779\n",
            "Val loss 0.0358\n",
            "Dice score : 0.057705074548721313\n",
            "Val loss 0.0384\n",
            "Dice score : 0.07221470773220062\n",
            "Val loss 0.0357\n",
            "Dice score : 0.0776154175400734\n",
            "Val loss 0.0359\n",
            "Dice score : 0.031919531524181366\n",
            "Val loss 0.0362\n",
            "Dice score : 0.0782504454255104\n",
            "Val loss 0.0386\n",
            "Dice score : 0.07762163132429123\n",
            "Val loss 0.0377\n",
            "Dice score : 0.08979719877243042\n",
            "Val loss 0.0390\n",
            "Dice score : 0.08476056903600693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 52/100 [1:03:11<57:53, 72.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0053 / 0100 | batch 0000 / 0050 | loss 0.0238\n",
            "Train: epoch 0053 / 0100 | batch 0001 / 0050 | loss 0.0401\n",
            "Train: epoch 0053 / 0100 | batch 0002 / 0050 | loss 0.0446\n",
            "Train: epoch 0053 / 0100 | batch 0003 / 0050 | loss 0.0441\n",
            "Train: epoch 0053 / 0100 | batch 0004 / 0050 | loss 0.0415\n",
            "Train: epoch 0053 / 0100 | batch 0005 / 0050 | loss 0.0415\n",
            "Train: epoch 0053 / 0100 | batch 0006 / 0050 | loss 0.0407\n",
            "Train: epoch 0053 / 0100 | batch 0007 / 0050 | loss 0.0383\n",
            "Train: epoch 0053 / 0100 | batch 0008 / 0050 | loss 0.0367\n",
            "Train: epoch 0053 / 0100 | batch 0009 / 0050 | loss 0.0367\n",
            "Train: epoch 0053 / 0100 | batch 0010 / 0050 | loss 0.0358\n",
            "Train: epoch 0053 / 0100 | batch 0011 / 0050 | loss 0.0387\n",
            "Train: epoch 0053 / 0100 | batch 0012 / 0050 | loss 0.0375\n",
            "Train: epoch 0053 / 0100 | batch 0013 / 0050 | loss 0.0372\n",
            "Train: epoch 0053 / 0100 | batch 0014 / 0050 | loss 0.0366\n",
            "Train: epoch 0053 / 0100 | batch 0015 / 0050 | loss 0.0363\n",
            "Train: epoch 0053 / 0100 | batch 0016 / 0050 | loss 0.0358\n",
            "Train: epoch 0053 / 0100 | batch 0017 / 0050 | loss 0.0364\n",
            "Train: epoch 0053 / 0100 | batch 0018 / 0050 | loss 0.0362\n",
            "Train: epoch 0053 / 0100 | batch 0019 / 0050 | loss 0.0377\n",
            "Train: epoch 0053 / 0100 | batch 0020 / 0050 | loss 0.0377\n",
            "Train: epoch 0053 / 0100 | batch 0021 / 0050 | loss 0.0370\n",
            "Train: epoch 0053 / 0100 | batch 0022 / 0050 | loss 0.0372\n",
            "Train: epoch 0053 / 0100 | batch 0023 / 0050 | loss 0.0368\n",
            "Train: epoch 0053 / 0100 | batch 0024 / 0050 | loss 0.0365\n",
            "Train: epoch 0053 / 0100 | batch 0025 / 0050 | loss 0.0366\n",
            "Train: epoch 0053 / 0100 | batch 0026 / 0050 | loss 0.0367\n",
            "Train: epoch 0053 / 0100 | batch 0027 / 0050 | loss 0.0372\n",
            "Train: epoch 0053 / 0100 | batch 0028 / 0050 | loss 0.0371\n",
            "Train: epoch 0053 / 0100 | batch 0029 / 0050 | loss 0.0369\n",
            "Train: epoch 0053 / 0100 | batch 0030 / 0050 | loss 0.0369\n",
            "Train: epoch 0053 / 0100 | batch 0031 / 0050 | loss 0.0370\n",
            "Train: epoch 0053 / 0100 | batch 0032 / 0050 | loss 0.0369\n",
            "Train: epoch 0053 / 0100 | batch 0033 / 0050 | loss 0.0367\n",
            "Train: epoch 0053 / 0100 | batch 0034 / 0050 | loss 0.0369\n",
            "Train: epoch 0053 / 0100 | batch 0035 / 0050 | loss 0.0368\n",
            "Train: epoch 0053 / 0100 | batch 0036 / 0050 | loss 0.0370\n",
            "Train: epoch 0053 / 0100 | batch 0037 / 0050 | loss 0.0373\n",
            "Train: epoch 0053 / 0100 | batch 0038 / 0050 | loss 0.0372\n",
            "Train: epoch 0053 / 0100 | batch 0039 / 0050 | loss 0.0369\n",
            "Train: epoch 0053 / 0100 | batch 0040 / 0050 | loss 0.0367\n",
            "Train: epoch 0053 / 0100 | batch 0041 / 0050 | loss 0.0365\n",
            "Train: epoch 0053 / 0100 | batch 0042 / 0050 | loss 0.0362\n",
            "Train: epoch 0053 / 0100 | batch 0043 / 0050 | loss 0.0364\n",
            "Train: epoch 0053 / 0100 | batch 0044 / 0050 | loss 0.0362\n",
            "Train: epoch 0053 / 0100 | batch 0045 / 0050 | loss 0.0363\n",
            "Train: epoch 0053 / 0100 | batch 0046 / 0050 | loss 0.0361\n",
            "Train: epoch 0053 / 0100 | batch 0047 / 0050 | loss 0.0360\n",
            "Train: epoch 0053 / 0100 | batch 0048 / 0050 | loss 0.0360\n",
            "Train: epoch 0053 / 0100 | batch 0049 / 0050 | loss 0.0361\n",
            "Val loss 0.0389\n",
            "Dice score : 0.06859461218118668\n",
            "Val loss 0.0466\n",
            "Dice score : 0.09013921767473221\n",
            "Val loss 0.0457\n",
            "Dice score : 0.06340383738279343\n",
            "Val loss 0.0412\n",
            "Dice score : 0.08457383513450623\n",
            "Val loss 0.0393\n",
            "Dice score : 0.06783657521009445\n",
            "Val loss 0.0373\n",
            "Dice score : 0.05000106990337372\n",
            "Val loss 0.0365\n",
            "Dice score : 0.10055774450302124\n",
            "Val loss 0.0384\n",
            "Dice score : 0.1148209273815155\n",
            "Val loss 0.0390\n",
            "Dice score : 0.06797296553850174\n",
            "Val loss 0.0397\n",
            "Dice score : 0.07824674248695374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 53/100 [1:04:23<56:46, 72.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0054 / 0100 | batch 0000 / 0050 | loss 0.0441\n",
            "Train: epoch 0054 / 0100 | batch 0001 / 0050 | loss 0.0489\n",
            "Train: epoch 0054 / 0100 | batch 0002 / 0050 | loss 0.0416\n",
            "Train: epoch 0054 / 0100 | batch 0003 / 0050 | loss 0.0395\n",
            "Train: epoch 0054 / 0100 | batch 0004 / 0050 | loss 0.0391\n",
            "Train: epoch 0054 / 0100 | batch 0005 / 0050 | loss 0.0393\n",
            "Train: epoch 0054 / 0100 | batch 0006 / 0050 | loss 0.0403\n",
            "Train: epoch 0054 / 0100 | batch 0007 / 0050 | loss 0.0384\n",
            "Train: epoch 0054 / 0100 | batch 0008 / 0050 | loss 0.0386\n",
            "Train: epoch 0054 / 0100 | batch 0009 / 0050 | loss 0.0385\n",
            "Train: epoch 0054 / 0100 | batch 0010 / 0050 | loss 0.0376\n",
            "Train: epoch 0054 / 0100 | batch 0011 / 0050 | loss 0.0376\n",
            "Train: epoch 0054 / 0100 | batch 0012 / 0050 | loss 0.0390\n",
            "Train: epoch 0054 / 0100 | batch 0013 / 0050 | loss 0.0386\n",
            "Train: epoch 0054 / 0100 | batch 0014 / 0050 | loss 0.0385\n",
            "Train: epoch 0054 / 0100 | batch 0015 / 0050 | loss 0.0381\n",
            "Train: epoch 0054 / 0100 | batch 0016 / 0050 | loss 0.0388\n",
            "Train: epoch 0054 / 0100 | batch 0017 / 0050 | loss 0.0380\n",
            "Train: epoch 0054 / 0100 | batch 0018 / 0050 | loss 0.0375\n",
            "Train: epoch 0054 / 0100 | batch 0019 / 0050 | loss 0.0370\n",
            "Train: epoch 0054 / 0100 | batch 0020 / 0050 | loss 0.0364\n",
            "Train: epoch 0054 / 0100 | batch 0021 / 0050 | loss 0.0366\n",
            "Train: epoch 0054 / 0100 | batch 0022 / 0050 | loss 0.0361\n",
            "Train: epoch 0054 / 0100 | batch 0023 / 0050 | loss 0.0356\n",
            "Train: epoch 0054 / 0100 | batch 0024 / 0050 | loss 0.0359\n",
            "Train: epoch 0054 / 0100 | batch 0025 / 0050 | loss 0.0368\n",
            "Train: epoch 0054 / 0100 | batch 0026 / 0050 | loss 0.0365\n",
            "Train: epoch 0054 / 0100 | batch 0027 / 0050 | loss 0.0371\n",
            "Train: epoch 0054 / 0100 | batch 0028 / 0050 | loss 0.0368\n",
            "Train: epoch 0054 / 0100 | batch 0029 / 0050 | loss 0.0366\n",
            "Train: epoch 0054 / 0100 | batch 0030 / 0050 | loss 0.0372\n",
            "Train: epoch 0054 / 0100 | batch 0031 / 0050 | loss 0.0368\n",
            "Train: epoch 0054 / 0100 | batch 0032 / 0050 | loss 0.0364\n",
            "Train: epoch 0054 / 0100 | batch 0033 / 0050 | loss 0.0362\n",
            "Train: epoch 0054 / 0100 | batch 0034 / 0050 | loss 0.0360\n",
            "Train: epoch 0054 / 0100 | batch 0035 / 0050 | loss 0.0367\n",
            "Train: epoch 0054 / 0100 | batch 0036 / 0050 | loss 0.0368\n",
            "Train: epoch 0054 / 0100 | batch 0037 / 0050 | loss 0.0365\n",
            "Train: epoch 0054 / 0100 | batch 0038 / 0050 | loss 0.0363\n",
            "Train: epoch 0054 / 0100 | batch 0039 / 0050 | loss 0.0362\n",
            "Train: epoch 0054 / 0100 | batch 0040 / 0050 | loss 0.0360\n",
            "Train: epoch 0054 / 0100 | batch 0041 / 0050 | loss 0.0358\n",
            "Train: epoch 0054 / 0100 | batch 0042 / 0050 | loss 0.0359\n",
            "Train: epoch 0054 / 0100 | batch 0043 / 0050 | loss 0.0357\n",
            "Train: epoch 0054 / 0100 | batch 0044 / 0050 | loss 0.0357\n",
            "Train: epoch 0054 / 0100 | batch 0045 / 0050 | loss 0.0357\n",
            "Train: epoch 0054 / 0100 | batch 0046 / 0050 | loss 0.0361\n",
            "Train: epoch 0054 / 0100 | batch 0047 / 0050 | loss 0.0361\n",
            "Train: epoch 0054 / 0100 | batch 0048 / 0050 | loss 0.0361\n",
            "Train: epoch 0054 / 0100 | batch 0049 / 0050 | loss 0.0361\n",
            "Val loss 0.0280\n",
            "Dice score : 0.06560550630092621\n",
            "Val loss 0.0443\n",
            "Dice score : 0.1140698492527008\n",
            "Val loss 0.0396\n",
            "Dice score : 0.0722137987613678\n",
            "Val loss 0.0365\n",
            "Dice score : 0.11414527893066406\n",
            "Val loss 0.0376\n",
            "Dice score : 0.0942307859659195\n",
            "Val loss 0.0368\n",
            "Dice score : 0.085993692278862\n",
            "Val loss 0.0427\n",
            "Dice score : 0.10335473716259003\n",
            "Val loss 0.0411\n",
            "Dice score : 0.051432497799396515\n",
            "Val loss 0.0393\n",
            "Dice score : 0.08439699560403824\n",
            "Val loss 0.0398\n",
            "Dice score : 0.077296182513237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 54/100 [1:05:36<55:33, 72.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0055 / 0100 | batch 0000 / 0050 | loss 0.0309\n",
            "Train: epoch 0055 / 0100 | batch 0001 / 0050 | loss 0.0308\n",
            "Train: epoch 0055 / 0100 | batch 0002 / 0050 | loss 0.0305\n",
            "Train: epoch 0055 / 0100 | batch 0003 / 0050 | loss 0.0310\n",
            "Train: epoch 0055 / 0100 | batch 0004 / 0050 | loss 0.0308\n",
            "Train: epoch 0055 / 0100 | batch 0005 / 0050 | loss 0.0335\n",
            "Train: epoch 0055 / 0100 | batch 0006 / 0050 | loss 0.0359\n",
            "Train: epoch 0055 / 0100 | batch 0007 / 0050 | loss 0.0348\n",
            "Train: epoch 0055 / 0100 | batch 0008 / 0050 | loss 0.0361\n",
            "Train: epoch 0055 / 0100 | batch 0009 / 0050 | loss 0.0362\n",
            "Train: epoch 0055 / 0100 | batch 0010 / 0050 | loss 0.0363\n",
            "Train: epoch 0055 / 0100 | batch 0011 / 0050 | loss 0.0364\n",
            "Train: epoch 0055 / 0100 | batch 0012 / 0050 | loss 0.0356\n",
            "Train: epoch 0055 / 0100 | batch 0013 / 0050 | loss 0.0351\n",
            "Train: epoch 0055 / 0100 | batch 0014 / 0050 | loss 0.0344\n",
            "Train: epoch 0055 / 0100 | batch 0015 / 0050 | loss 0.0339\n",
            "Train: epoch 0055 / 0100 | batch 0016 / 0050 | loss 0.0342\n",
            "Train: epoch 0055 / 0100 | batch 0017 / 0050 | loss 0.0362\n",
            "Train: epoch 0055 / 0100 | batch 0018 / 0050 | loss 0.0361\n",
            "Train: epoch 0055 / 0100 | batch 0019 / 0050 | loss 0.0367\n",
            "Train: epoch 0055 / 0100 | batch 0020 / 0050 | loss 0.0372\n",
            "Train: epoch 0055 / 0100 | batch 0021 / 0050 | loss 0.0374\n",
            "Train: epoch 0055 / 0100 | batch 0022 / 0050 | loss 0.0367\n",
            "Train: epoch 0055 / 0100 | batch 0023 / 0050 | loss 0.0362\n",
            "Train: epoch 0055 / 0100 | batch 0024 / 0050 | loss 0.0358\n",
            "Train: epoch 0055 / 0100 | batch 0025 / 0050 | loss 0.0365\n",
            "Train: epoch 0055 / 0100 | batch 0026 / 0050 | loss 0.0360\n",
            "Train: epoch 0055 / 0100 | batch 0027 / 0050 | loss 0.0368\n",
            "Train: epoch 0055 / 0100 | batch 0028 / 0050 | loss 0.0366\n",
            "Train: epoch 0055 / 0100 | batch 0029 / 0050 | loss 0.0368\n",
            "Train: epoch 0055 / 0100 | batch 0030 / 0050 | loss 0.0369\n",
            "Train: epoch 0055 / 0100 | batch 0031 / 0050 | loss 0.0367\n",
            "Train: epoch 0055 / 0100 | batch 0032 / 0050 | loss 0.0364\n",
            "Train: epoch 0055 / 0100 | batch 0033 / 0050 | loss 0.0367\n",
            "Train: epoch 0055 / 0100 | batch 0034 / 0050 | loss 0.0366\n",
            "Train: epoch 0055 / 0100 | batch 0035 / 0050 | loss 0.0362\n",
            "Train: epoch 0055 / 0100 | batch 0036 / 0050 | loss 0.0360\n",
            "Train: epoch 0055 / 0100 | batch 0037 / 0050 | loss 0.0366\n",
            "Train: epoch 0055 / 0100 | batch 0038 / 0050 | loss 0.0366\n",
            "Train: epoch 0055 / 0100 | batch 0039 / 0050 | loss 0.0364\n",
            "Train: epoch 0055 / 0100 | batch 0040 / 0050 | loss 0.0368\n",
            "Train: epoch 0055 / 0100 | batch 0041 / 0050 | loss 0.0366\n",
            "Train: epoch 0055 / 0100 | batch 0042 / 0050 | loss 0.0367\n",
            "Train: epoch 0055 / 0100 | batch 0043 / 0050 | loss 0.0364\n",
            "Train: epoch 0055 / 0100 | batch 0044 / 0050 | loss 0.0362\n",
            "Train: epoch 0055 / 0100 | batch 0045 / 0050 | loss 0.0361\n",
            "Train: epoch 0055 / 0100 | batch 0046 / 0050 | loss 0.0359\n",
            "Train: epoch 0055 / 0100 | batch 0047 / 0050 | loss 0.0357\n",
            "Train: epoch 0055 / 0100 | batch 0048 / 0050 | loss 0.0358\n",
            "Train: epoch 0055 / 0100 | batch 0049 / 0050 | loss 0.0357\n",
            "Val loss 0.0532\n",
            "Dice score : 0.09596037864685059\n",
            "Val loss 0.0500\n",
            "Dice score : 0.09956273436546326\n",
            "Val loss 0.0419\n",
            "Dice score : 0.06069181486964226\n",
            "Val loss 0.0421\n",
            "Dice score : 0.08345000445842743\n",
            "Val loss 0.0391\n",
            "Dice score : 0.09823574125766754\n",
            "Val loss 0.0413\n",
            "Dice score : 0.10309675335884094\n",
            "Val loss 0.0441\n",
            "Dice score : 0.05707088112831116\n",
            "Val loss 0.0435\n",
            "Dice score : 0.08556828647851944\n",
            "Val loss 0.0434\n",
            "Dice score : 0.09319327771663666\n",
            "Val loss 0.0416\n",
            "Dice score : 0.06668244302272797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 55/100 [1:06:49<54:36, 72.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0056 / 0100 | batch 0000 / 0050 | loss 0.0568\n",
            "Train: epoch 0056 / 0100 | batch 0001 / 0050 | loss 0.0581\n",
            "Train: epoch 0056 / 0100 | batch 0002 / 0050 | loss 0.0470\n",
            "Train: epoch 0056 / 0100 | batch 0003 / 0050 | loss 0.0412\n",
            "Train: epoch 0056 / 0100 | batch 0004 / 0050 | loss 0.0448\n",
            "Train: epoch 0056 / 0100 | batch 0005 / 0050 | loss 0.0449\n",
            "Train: epoch 0056 / 0100 | batch 0006 / 0050 | loss 0.0451\n",
            "Train: epoch 0056 / 0100 | batch 0007 / 0050 | loss 0.0425\n",
            "Train: epoch 0056 / 0100 | batch 0008 / 0050 | loss 0.0424\n",
            "Train: epoch 0056 / 0100 | batch 0009 / 0050 | loss 0.0408\n",
            "Train: epoch 0056 / 0100 | batch 0010 / 0050 | loss 0.0401\n",
            "Train: epoch 0056 / 0100 | batch 0011 / 0050 | loss 0.0388\n",
            "Train: epoch 0056 / 0100 | batch 0012 / 0050 | loss 0.0387\n",
            "Train: epoch 0056 / 0100 | batch 0013 / 0050 | loss 0.0377\n",
            "Train: epoch 0056 / 0100 | batch 0014 / 0050 | loss 0.0372\n",
            "Train: epoch 0056 / 0100 | batch 0015 / 0050 | loss 0.0365\n",
            "Train: epoch 0056 / 0100 | batch 0016 / 0050 | loss 0.0358\n",
            "Train: epoch 0056 / 0100 | batch 0017 / 0050 | loss 0.0357\n",
            "Train: epoch 0056 / 0100 | batch 0018 / 0050 | loss 0.0355\n",
            "Train: epoch 0056 / 0100 | batch 0019 / 0050 | loss 0.0357\n",
            "Train: epoch 0056 / 0100 | batch 0020 / 0050 | loss 0.0351\n",
            "Train: epoch 0056 / 0100 | batch 0021 / 0050 | loss 0.0351\n",
            "Train: epoch 0056 / 0100 | batch 0022 / 0050 | loss 0.0347\n",
            "Train: epoch 0056 / 0100 | batch 0023 / 0050 | loss 0.0354\n",
            "Train: epoch 0056 / 0100 | batch 0024 / 0050 | loss 0.0357\n",
            "Train: epoch 0056 / 0100 | batch 0025 / 0050 | loss 0.0358\n",
            "Train: epoch 0056 / 0100 | batch 0026 / 0050 | loss 0.0352\n",
            "Train: epoch 0056 / 0100 | batch 0027 / 0050 | loss 0.0353\n",
            "Train: epoch 0056 / 0100 | batch 0028 / 0050 | loss 0.0358\n",
            "Train: epoch 0056 / 0100 | batch 0029 / 0050 | loss 0.0358\n",
            "Train: epoch 0056 / 0100 | batch 0030 / 0050 | loss 0.0356\n",
            "Train: epoch 0056 / 0100 | batch 0031 / 0050 | loss 0.0356\n",
            "Train: epoch 0056 / 0100 | batch 0032 / 0050 | loss 0.0357\n",
            "Train: epoch 0056 / 0100 | batch 0033 / 0050 | loss 0.0354\n",
            "Train: epoch 0056 / 0100 | batch 0034 / 0050 | loss 0.0351\n",
            "Train: epoch 0056 / 0100 | batch 0035 / 0050 | loss 0.0353\n",
            "Train: epoch 0056 / 0100 | batch 0036 / 0050 | loss 0.0353\n",
            "Train: epoch 0056 / 0100 | batch 0037 / 0050 | loss 0.0350\n",
            "Train: epoch 0056 / 0100 | batch 0038 / 0050 | loss 0.0348\n",
            "Train: epoch 0056 / 0100 | batch 0039 / 0050 | loss 0.0350\n",
            "Train: epoch 0056 / 0100 | batch 0040 / 0050 | loss 0.0349\n",
            "Train: epoch 0056 / 0100 | batch 0041 / 0050 | loss 0.0352\n",
            "Train: epoch 0056 / 0100 | batch 0042 / 0050 | loss 0.0350\n",
            "Train: epoch 0056 / 0100 | batch 0043 / 0050 | loss 0.0348\n",
            "Train: epoch 0056 / 0100 | batch 0044 / 0050 | loss 0.0350\n",
            "Train: epoch 0056 / 0100 | batch 0045 / 0050 | loss 0.0349\n",
            "Train: epoch 0056 / 0100 | batch 0046 / 0050 | loss 0.0348\n",
            "Train: epoch 0056 / 0100 | batch 0047 / 0050 | loss 0.0353\n",
            "Train: epoch 0056 / 0100 | batch 0048 / 0050 | loss 0.0351\n",
            "Train: epoch 0056 / 0100 | batch 0049 / 0050 | loss 0.0350\n",
            "Val loss 0.0370\n",
            "Dice score : 0.06503346562385559\n",
            "Val loss 0.0422\n",
            "Dice score : 0.09656860679388046\n",
            "Val loss 0.0461\n",
            "Dice score : 0.10421305894851685\n",
            "Val loss 0.0414\n",
            "Dice score : 0.07369072735309601\n",
            "Val loss 0.0415\n",
            "Dice score : 0.08601754158735275\n",
            "Val loss 0.0386\n",
            "Dice score : 0.08223980665206909\n",
            "Val loss 0.0406\n",
            "Dice score : 0.06524401158094406\n",
            "Val loss 0.0389\n",
            "Dice score : 0.048033103346824646\n",
            "Val loss 0.0394\n",
            "Dice score : 0.10112911462783813\n",
            "Val loss 0.0386\n",
            "Dice score : 0.037951260805130005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 56/100 [1:08:01<53:02, 72.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0057 / 0100 | batch 0000 / 0050 | loss 0.0293\n",
            "Train: epoch 0057 / 0100 | batch 0001 / 0050 | loss 0.0267\n",
            "Train: epoch 0057 / 0100 | batch 0002 / 0050 | loss 0.0372\n",
            "Train: epoch 0057 / 0100 | batch 0003 / 0050 | loss 0.0347\n",
            "Train: epoch 0057 / 0100 | batch 0004 / 0050 | loss 0.0351\n",
            "Train: epoch 0057 / 0100 | batch 0005 / 0050 | loss 0.0373\n",
            "Train: epoch 0057 / 0100 | batch 0006 / 0050 | loss 0.0374\n",
            "Train: epoch 0057 / 0100 | batch 0007 / 0050 | loss 0.0384\n",
            "Train: epoch 0057 / 0100 | batch 0008 / 0050 | loss 0.0370\n",
            "Train: epoch 0057 / 0100 | batch 0009 / 0050 | loss 0.0367\n",
            "Train: epoch 0057 / 0100 | batch 0010 / 0050 | loss 0.0388\n",
            "Train: epoch 0057 / 0100 | batch 0011 / 0050 | loss 0.0375\n",
            "Train: epoch 0057 / 0100 | batch 0012 / 0050 | loss 0.0363\n",
            "Train: epoch 0057 / 0100 | batch 0013 / 0050 | loss 0.0363\n",
            "Train: epoch 0057 / 0100 | batch 0014 / 0050 | loss 0.0365\n",
            "Train: epoch 0057 / 0100 | batch 0015 / 0050 | loss 0.0367\n",
            "Train: epoch 0057 / 0100 | batch 0016 / 0050 | loss 0.0367\n",
            "Train: epoch 0057 / 0100 | batch 0017 / 0050 | loss 0.0361\n",
            "Train: epoch 0057 / 0100 | batch 0018 / 0050 | loss 0.0355\n",
            "Train: epoch 0057 / 0100 | batch 0019 / 0050 | loss 0.0353\n",
            "Train: epoch 0057 / 0100 | batch 0020 / 0050 | loss 0.0360\n",
            "Train: epoch 0057 / 0100 | batch 0021 / 0050 | loss 0.0356\n",
            "Train: epoch 0057 / 0100 | batch 0022 / 0050 | loss 0.0356\n",
            "Train: epoch 0057 / 0100 | batch 0023 / 0050 | loss 0.0352\n",
            "Train: epoch 0057 / 0100 | batch 0024 / 0050 | loss 0.0349\n",
            "Train: epoch 0057 / 0100 | batch 0025 / 0050 | loss 0.0353\n",
            "Train: epoch 0057 / 0100 | batch 0026 / 0050 | loss 0.0349\n",
            "Train: epoch 0057 / 0100 | batch 0027 / 0050 | loss 0.0345\n",
            "Train: epoch 0057 / 0100 | batch 0028 / 0050 | loss 0.0341\n",
            "Train: epoch 0057 / 0100 | batch 0029 / 0050 | loss 0.0341\n",
            "Train: epoch 0057 / 0100 | batch 0030 / 0050 | loss 0.0343\n",
            "Train: epoch 0057 / 0100 | batch 0031 / 0050 | loss 0.0349\n",
            "Train: epoch 0057 / 0100 | batch 0032 / 0050 | loss 0.0356\n",
            "Train: epoch 0057 / 0100 | batch 0033 / 0050 | loss 0.0355\n",
            "Train: epoch 0057 / 0100 | batch 0034 / 0050 | loss 0.0353\n",
            "Train: epoch 0057 / 0100 | batch 0035 / 0050 | loss 0.0353\n",
            "Train: epoch 0057 / 0100 | batch 0036 / 0050 | loss 0.0358\n",
            "Train: epoch 0057 / 0100 | batch 0037 / 0050 | loss 0.0356\n",
            "Train: epoch 0057 / 0100 | batch 0038 / 0050 | loss 0.0354\n",
            "Train: epoch 0057 / 0100 | batch 0039 / 0050 | loss 0.0354\n",
            "Train: epoch 0057 / 0100 | batch 0040 / 0050 | loss 0.0356\n",
            "Train: epoch 0057 / 0100 | batch 0041 / 0050 | loss 0.0355\n",
            "Train: epoch 0057 / 0100 | batch 0042 / 0050 | loss 0.0356\n",
            "Train: epoch 0057 / 0100 | batch 0043 / 0050 | loss 0.0354\n",
            "Train: epoch 0057 / 0100 | batch 0044 / 0050 | loss 0.0351\n",
            "Train: epoch 0057 / 0100 | batch 0045 / 0050 | loss 0.0350\n",
            "Train: epoch 0057 / 0100 | batch 0046 / 0050 | loss 0.0348\n",
            "Train: epoch 0057 / 0100 | batch 0047 / 0050 | loss 0.0350\n",
            "Train: epoch 0057 / 0100 | batch 0048 / 0050 | loss 0.0350\n",
            "Train: epoch 0057 / 0100 | batch 0049 / 0050 | loss 0.0348\n",
            "Val loss 0.0261\n",
            "Dice score : 0.09226871281862259\n",
            "Val loss 0.0405\n",
            "Dice score : 0.04867510870099068\n",
            "Val loss 0.0350\n",
            "Dice score : 0.06848268955945969\n",
            "Val loss 0.0423\n",
            "Dice score : 0.1336110383272171\n",
            "Val loss 0.0435\n",
            "Dice score : 0.09746900945901871\n",
            "Val loss 0.0413\n",
            "Dice score : 0.058204974979162216\n",
            "Val loss 0.0394\n",
            "Dice score : 0.08598963171243668\n",
            "Val loss 0.0404\n",
            "Dice score : 0.06515415012836456\n",
            "Val loss 0.0389\n",
            "Dice score : 0.10769020020961761\n",
            "Val loss 0.0379\n",
            "Dice score : 0.06169954687356949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 57/100 [1:09:15<52:11, 72.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0058 / 0100 | batch 0000 / 0050 | loss 0.0534\n",
            "Train: epoch 0058 / 0100 | batch 0001 / 0050 | loss 0.0403\n",
            "Train: epoch 0058 / 0100 | batch 0002 / 0050 | loss 0.0367\n",
            "Train: epoch 0058 / 0100 | batch 0003 / 0050 | loss 0.0374\n",
            "Train: epoch 0058 / 0100 | batch 0004 / 0050 | loss 0.0348\n",
            "Train: epoch 0058 / 0100 | batch 0005 / 0050 | loss 0.0337\n",
            "Train: epoch 0058 / 0100 | batch 0006 / 0050 | loss 0.0333\n",
            "Train: epoch 0058 / 0100 | batch 0007 / 0050 | loss 0.0341\n",
            "Train: epoch 0058 / 0100 | batch 0008 / 0050 | loss 0.0388\n",
            "Train: epoch 0058 / 0100 | batch 0009 / 0050 | loss 0.0374\n",
            "Train: epoch 0058 / 0100 | batch 0010 / 0050 | loss 0.0378\n",
            "Train: epoch 0058 / 0100 | batch 0011 / 0050 | loss 0.0368\n",
            "Train: epoch 0058 / 0100 | batch 0012 / 0050 | loss 0.0366\n",
            "Train: epoch 0058 / 0100 | batch 0013 / 0050 | loss 0.0358\n",
            "Train: epoch 0058 / 0100 | batch 0014 / 0050 | loss 0.0359\n",
            "Train: epoch 0058 / 0100 | batch 0015 / 0050 | loss 0.0355\n",
            "Train: epoch 0058 / 0100 | batch 0016 / 0050 | loss 0.0348\n",
            "Train: epoch 0058 / 0100 | batch 0017 / 0050 | loss 0.0351\n",
            "Train: epoch 0058 / 0100 | batch 0018 / 0050 | loss 0.0348\n",
            "Train: epoch 0058 / 0100 | batch 0019 / 0050 | loss 0.0349\n",
            "Train: epoch 0058 / 0100 | batch 0020 / 0050 | loss 0.0342\n",
            "Train: epoch 0058 / 0100 | batch 0021 / 0050 | loss 0.0338\n",
            "Train: epoch 0058 / 0100 | batch 0022 / 0050 | loss 0.0346\n",
            "Train: epoch 0058 / 0100 | batch 0023 / 0050 | loss 0.0339\n",
            "Train: epoch 0058 / 0100 | batch 0024 / 0050 | loss 0.0341\n",
            "Train: epoch 0058 / 0100 | batch 0025 / 0050 | loss 0.0338\n",
            "Train: epoch 0058 / 0100 | batch 0026 / 0050 | loss 0.0334\n",
            "Train: epoch 0058 / 0100 | batch 0027 / 0050 | loss 0.0337\n",
            "Train: epoch 0058 / 0100 | batch 0028 / 0050 | loss 0.0334\n",
            "Train: epoch 0058 / 0100 | batch 0029 / 0050 | loss 0.0332\n",
            "Train: epoch 0058 / 0100 | batch 0030 / 0050 | loss 0.0330\n",
            "Train: epoch 0058 / 0100 | batch 0031 / 0050 | loss 0.0327\n",
            "Train: epoch 0058 / 0100 | batch 0032 / 0050 | loss 0.0330\n",
            "Train: epoch 0058 / 0100 | batch 0033 / 0050 | loss 0.0331\n",
            "Train: epoch 0058 / 0100 | batch 0034 / 0050 | loss 0.0344\n",
            "Train: epoch 0058 / 0100 | batch 0035 / 0050 | loss 0.0343\n",
            "Train: epoch 0058 / 0100 | batch 0036 / 0050 | loss 0.0342\n",
            "Train: epoch 0058 / 0100 | batch 0037 / 0050 | loss 0.0346\n",
            "Train: epoch 0058 / 0100 | batch 0038 / 0050 | loss 0.0343\n",
            "Train: epoch 0058 / 0100 | batch 0039 / 0050 | loss 0.0342\n",
            "Train: epoch 0058 / 0100 | batch 0040 / 0050 | loss 0.0346\n",
            "Train: epoch 0058 / 0100 | batch 0041 / 0050 | loss 0.0348\n",
            "Train: epoch 0058 / 0100 | batch 0042 / 0050 | loss 0.0347\n",
            "Train: epoch 0058 / 0100 | batch 0043 / 0050 | loss 0.0345\n",
            "Train: epoch 0058 / 0100 | batch 0044 / 0050 | loss 0.0346\n",
            "Train: epoch 0058 / 0100 | batch 0045 / 0050 | loss 0.0347\n",
            "Train: epoch 0058 / 0100 | batch 0046 / 0050 | loss 0.0345\n",
            "Train: epoch 0058 / 0100 | batch 0047 / 0050 | loss 0.0343\n",
            "Train: epoch 0058 / 0100 | batch 0048 / 0050 | loss 0.0344\n",
            "Train: epoch 0058 / 0100 | batch 0049 / 0050 | loss 0.0345\n",
            "Val loss 0.0492\n",
            "Dice score : 0.09621451795101166\n",
            "Val loss 0.0401\n",
            "Dice score : 0.07358238846063614\n",
            "Val loss 0.0411\n",
            "Dice score : 0.10383566468954086\n",
            "Val loss 0.0378\n",
            "Dice score : 0.07031946629285812\n",
            "Val loss 0.0374\n",
            "Dice score : 0.10293672233819962\n",
            "Val loss 0.0359\n",
            "Dice score : 0.07032991945743561\n",
            "Val loss 0.0375\n",
            "Dice score : 0.09067472070455551\n",
            "Val loss 0.0359\n",
            "Dice score : 0.09862346202135086\n",
            "Val loss 0.0381\n",
            "Dice score : 0.07165627926588058\n",
            "Val loss 0.0379\n",
            "Dice score : 0.11536937952041626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 58/100 [1:10:28<50:59, 72.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0059 / 0100 | batch 0000 / 0050 | loss 0.0534\n",
            "Train: epoch 0059 / 0100 | batch 0001 / 0050 | loss 0.0388\n",
            "Train: epoch 0059 / 0100 | batch 0002 / 0050 | loss 0.0413\n",
            "Train: epoch 0059 / 0100 | batch 0003 / 0050 | loss 0.0369\n",
            "Train: epoch 0059 / 0100 | batch 0004 / 0050 | loss 0.0371\n",
            "Train: epoch 0059 / 0100 | batch 0005 / 0050 | loss 0.0362\n",
            "Train: epoch 0059 / 0100 | batch 0006 / 0050 | loss 0.0366\n",
            "Train: epoch 0059 / 0100 | batch 0007 / 0050 | loss 0.0384\n",
            "Train: epoch 0059 / 0100 | batch 0008 / 0050 | loss 0.0369\n",
            "Train: epoch 0059 / 0100 | batch 0009 / 0050 | loss 0.0367\n",
            "Train: epoch 0059 / 0100 | batch 0010 / 0050 | loss 0.0353\n",
            "Train: epoch 0059 / 0100 | batch 0011 / 0050 | loss 0.0344\n",
            "Train: epoch 0059 / 0100 | batch 0012 / 0050 | loss 0.0335\n",
            "Train: epoch 0059 / 0100 | batch 0013 / 0050 | loss 0.0329\n",
            "Train: epoch 0059 / 0100 | batch 0014 / 0050 | loss 0.0331\n",
            "Train: epoch 0059 / 0100 | batch 0015 / 0050 | loss 0.0332\n",
            "Train: epoch 0059 / 0100 | batch 0016 / 0050 | loss 0.0344\n",
            "Train: epoch 0059 / 0100 | batch 0017 / 0050 | loss 0.0350\n",
            "Train: epoch 0059 / 0100 | batch 0018 / 0050 | loss 0.0344\n",
            "Train: epoch 0059 / 0100 | batch 0019 / 0050 | loss 0.0346\n",
            "Train: epoch 0059 / 0100 | batch 0020 / 0050 | loss 0.0344\n",
            "Train: epoch 0059 / 0100 | batch 0021 / 0050 | loss 0.0349\n",
            "Train: epoch 0059 / 0100 | batch 0022 / 0050 | loss 0.0355\n",
            "Train: epoch 0059 / 0100 | batch 0023 / 0050 | loss 0.0353\n",
            "Train: epoch 0059 / 0100 | batch 0024 / 0050 | loss 0.0350\n",
            "Train: epoch 0059 / 0100 | batch 0025 / 0050 | loss 0.0346\n",
            "Train: epoch 0059 / 0100 | batch 0026 / 0050 | loss 0.0354\n",
            "Train: epoch 0059 / 0100 | batch 0027 / 0050 | loss 0.0351\n",
            "Train: epoch 0059 / 0100 | batch 0028 / 0050 | loss 0.0348\n",
            "Train: epoch 0059 / 0100 | batch 0029 / 0050 | loss 0.0345\n",
            "Train: epoch 0059 / 0100 | batch 0030 / 0050 | loss 0.0342\n",
            "Train: epoch 0059 / 0100 | batch 0031 / 0050 | loss 0.0341\n",
            "Train: epoch 0059 / 0100 | batch 0032 / 0050 | loss 0.0340\n",
            "Train: epoch 0059 / 0100 | batch 0033 / 0050 | loss 0.0342\n",
            "Train: epoch 0059 / 0100 | batch 0034 / 0050 | loss 0.0347\n",
            "Train: epoch 0059 / 0100 | batch 0035 / 0050 | loss 0.0344\n",
            "Train: epoch 0059 / 0100 | batch 0036 / 0050 | loss 0.0343\n",
            "Train: epoch 0059 / 0100 | batch 0037 / 0050 | loss 0.0344\n",
            "Train: epoch 0059 / 0100 | batch 0038 / 0050 | loss 0.0342\n",
            "Train: epoch 0059 / 0100 | batch 0039 / 0050 | loss 0.0346\n",
            "Train: epoch 0059 / 0100 | batch 0040 / 0050 | loss 0.0343\n",
            "Train: epoch 0059 / 0100 | batch 0041 / 0050 | loss 0.0342\n",
            "Train: epoch 0059 / 0100 | batch 0042 / 0050 | loss 0.0348\n",
            "Train: epoch 0059 / 0100 | batch 0043 / 0050 | loss 0.0347\n",
            "Train: epoch 0059 / 0100 | batch 0044 / 0050 | loss 0.0346\n",
            "Train: epoch 0059 / 0100 | batch 0045 / 0050 | loss 0.0347\n",
            "Train: epoch 0059 / 0100 | batch 0046 / 0050 | loss 0.0347\n",
            "Train: epoch 0059 / 0100 | batch 0047 / 0050 | loss 0.0347\n",
            "Train: epoch 0059 / 0100 | batch 0048 / 0050 | loss 0.0344\n",
            "Train: epoch 0059 / 0100 | batch 0049 / 0050 | loss 0.0342\n",
            "Val loss 0.0516\n",
            "Dice score : 0.09686574339866638\n",
            "Val loss 0.0449\n",
            "Dice score : 0.07192428410053253\n",
            "Val loss 0.0393\n",
            "Dice score : 0.0751909464597702\n",
            "Val loss 0.0379\n",
            "Dice score : 0.05292576178908348\n",
            "Val loss 0.0352\n",
            "Dice score : 0.10192728042602539\n",
            "Val loss 0.0352\n",
            "Dice score : 0.10650601983070374\n",
            "Val loss 0.0371\n",
            "Dice score : 0.12150947749614716\n",
            "Val loss 0.0385\n",
            "Dice score : 0.07430999726057053\n",
            "Val loss 0.0365\n",
            "Dice score : 0.04542681947350502\n",
            "Val loss 0.0370\n",
            "Dice score : 0.10619328916072845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 59/100 [1:11:38<49:22, 72.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0060 / 0100 | batch 0000 / 0050 | loss 0.0272\n",
            "Train: epoch 0060 / 0100 | batch 0001 / 0050 | loss 0.0312\n",
            "Train: epoch 0060 / 0100 | batch 0002 / 0050 | loss 0.0402\n",
            "Train: epoch 0060 / 0100 | batch 0003 / 0050 | loss 0.0421\n",
            "Train: epoch 0060 / 0100 | batch 0004 / 0050 | loss 0.0385\n",
            "Train: epoch 0060 / 0100 | batch 0005 / 0050 | loss 0.0380\n",
            "Train: epoch 0060 / 0100 | batch 0006 / 0050 | loss 0.0358\n",
            "Train: epoch 0060 / 0100 | batch 0007 / 0050 | loss 0.0344\n",
            "Train: epoch 0060 / 0100 | batch 0008 / 0050 | loss 0.0348\n",
            "Train: epoch 0060 / 0100 | batch 0009 / 0050 | loss 0.0356\n",
            "Train: epoch 0060 / 0100 | batch 0010 / 0050 | loss 0.0353\n",
            "Train: epoch 0060 / 0100 | batch 0011 / 0050 | loss 0.0342\n",
            "Train: epoch 0060 / 0100 | batch 0012 / 0050 | loss 0.0341\n",
            "Train: epoch 0060 / 0100 | batch 0013 / 0050 | loss 0.0345\n",
            "Train: epoch 0060 / 0100 | batch 0014 / 0050 | loss 0.0339\n",
            "Train: epoch 0060 / 0100 | batch 0015 / 0050 | loss 0.0332\n",
            "Train: epoch 0060 / 0100 | batch 0016 / 0050 | loss 0.0338\n",
            "Train: epoch 0060 / 0100 | batch 0017 / 0050 | loss 0.0331\n",
            "Train: epoch 0060 / 0100 | batch 0018 / 0050 | loss 0.0328\n",
            "Train: epoch 0060 / 0100 | batch 0019 / 0050 | loss 0.0322\n",
            "Train: epoch 0060 / 0100 | batch 0020 / 0050 | loss 0.0320\n",
            "Train: epoch 0060 / 0100 | batch 0021 / 0050 | loss 0.0330\n",
            "Train: epoch 0060 / 0100 | batch 0022 / 0050 | loss 0.0333\n",
            "Train: epoch 0060 / 0100 | batch 0023 / 0050 | loss 0.0333\n",
            "Train: epoch 0060 / 0100 | batch 0024 / 0050 | loss 0.0329\n",
            "Train: epoch 0060 / 0100 | batch 0025 / 0050 | loss 0.0328\n",
            "Train: epoch 0060 / 0100 | batch 0026 / 0050 | loss 0.0331\n",
            "Train: epoch 0060 / 0100 | batch 0027 / 0050 | loss 0.0337\n",
            "Train: epoch 0060 / 0100 | batch 0028 / 0050 | loss 0.0334\n",
            "Train: epoch 0060 / 0100 | batch 0029 / 0050 | loss 0.0337\n",
            "Train: epoch 0060 / 0100 | batch 0030 / 0050 | loss 0.0339\n",
            "Train: epoch 0060 / 0100 | batch 0031 / 0050 | loss 0.0343\n",
            "Train: epoch 0060 / 0100 | batch 0032 / 0050 | loss 0.0342\n",
            "Train: epoch 0060 / 0100 | batch 0033 / 0050 | loss 0.0341\n",
            "Train: epoch 0060 / 0100 | batch 0034 / 0050 | loss 0.0339\n",
            "Train: epoch 0060 / 0100 | batch 0035 / 0050 | loss 0.0338\n",
            "Train: epoch 0060 / 0100 | batch 0036 / 0050 | loss 0.0348\n",
            "Train: epoch 0060 / 0100 | batch 0037 / 0050 | loss 0.0351\n",
            "Train: epoch 0060 / 0100 | batch 0038 / 0050 | loss 0.0350\n",
            "Train: epoch 0060 / 0100 | batch 0039 / 0050 | loss 0.0349\n",
            "Train: epoch 0060 / 0100 | batch 0040 / 0050 | loss 0.0349\n",
            "Train: epoch 0060 / 0100 | batch 0041 / 0050 | loss 0.0347\n",
            "Train: epoch 0060 / 0100 | batch 0042 / 0050 | loss 0.0348\n",
            "Train: epoch 0060 / 0100 | batch 0043 / 0050 | loss 0.0349\n",
            "Train: epoch 0060 / 0100 | batch 0044 / 0050 | loss 0.0347\n",
            "Train: epoch 0060 / 0100 | batch 0045 / 0050 | loss 0.0346\n",
            "Train: epoch 0060 / 0100 | batch 0046 / 0050 | loss 0.0344\n",
            "Train: epoch 0060 / 0100 | batch 0047 / 0050 | loss 0.0342\n",
            "Train: epoch 0060 / 0100 | batch 0048 / 0050 | loss 0.0339\n",
            "Train: epoch 0060 / 0100 | batch 0049 / 0050 | loss 0.0338\n",
            "Val loss 0.0268\n",
            "Dice score : 0.058467015624046326\n",
            "Val loss 0.0356\n",
            "Dice score : 0.09217248111963272\n",
            "Val loss 0.0333\n",
            "Dice score : 0.08637318015098572\n",
            "Val loss 0.0360\n",
            "Dice score : 0.10543199628591537\n",
            "Val loss 0.0368\n",
            "Dice score : 0.07694368809461594\n",
            "Val loss 0.0356\n",
            "Dice score : 0.0666126236319542\n",
            "Val loss 0.0355\n",
            "Dice score : 0.04996170103549957\n",
            "Val loss 0.0377\n",
            "Dice score : 0.05469505116343498\n",
            "Val loss 0.0361\n",
            "Dice score : 0.07436999678611755\n",
            "Val loss 0.0376\n",
            "Dice score : 0.10118143260478973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 60/100 [1:12:51<48:16, 72.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0061 / 0100 | batch 0000 / 0050 | loss 0.0214\n",
            "Train: epoch 0061 / 0100 | batch 0001 / 0050 | loss 0.0237\n",
            "Train: epoch 0061 / 0100 | batch 0002 / 0050 | loss 0.0340\n",
            "Train: epoch 0061 / 0100 | batch 0003 / 0050 | loss 0.0369\n",
            "Train: epoch 0061 / 0100 | batch 0004 / 0050 | loss 0.0378\n",
            "Train: epoch 0061 / 0100 | batch 0005 / 0050 | loss 0.0400\n",
            "Train: epoch 0061 / 0100 | batch 0006 / 0050 | loss 0.0380\n",
            "Train: epoch 0061 / 0100 | batch 0007 / 0050 | loss 0.0376\n",
            "Train: epoch 0061 / 0100 | batch 0008 / 0050 | loss 0.0360\n",
            "Train: epoch 0061 / 0100 | batch 0009 / 0050 | loss 0.0348\n",
            "Train: epoch 0061 / 0100 | batch 0010 / 0050 | loss 0.0337\n",
            "Train: epoch 0061 / 0100 | batch 0011 / 0050 | loss 0.0332\n",
            "Train: epoch 0061 / 0100 | batch 0012 / 0050 | loss 0.0325\n",
            "Train: epoch 0061 / 0100 | batch 0013 / 0050 | loss 0.0322\n",
            "Train: epoch 0061 / 0100 | batch 0014 / 0050 | loss 0.0324\n",
            "Train: epoch 0061 / 0100 | batch 0015 / 0050 | loss 0.0334\n",
            "Train: epoch 0061 / 0100 | batch 0016 / 0050 | loss 0.0346\n",
            "Train: epoch 0061 / 0100 | batch 0017 / 0050 | loss 0.0340\n",
            "Train: epoch 0061 / 0100 | batch 0018 / 0050 | loss 0.0343\n",
            "Train: epoch 0061 / 0100 | batch 0019 / 0050 | loss 0.0343\n",
            "Train: epoch 0061 / 0100 | batch 0020 / 0050 | loss 0.0337\n",
            "Train: epoch 0061 / 0100 | batch 0021 / 0050 | loss 0.0340\n",
            "Train: epoch 0061 / 0100 | batch 0022 / 0050 | loss 0.0337\n",
            "Train: epoch 0061 / 0100 | batch 0023 / 0050 | loss 0.0335\n",
            "Train: epoch 0061 / 0100 | batch 0024 / 0050 | loss 0.0334\n",
            "Train: epoch 0061 / 0100 | batch 0025 / 0050 | loss 0.0331\n",
            "Train: epoch 0061 / 0100 | batch 0026 / 0050 | loss 0.0336\n",
            "Train: epoch 0061 / 0100 | batch 0027 / 0050 | loss 0.0341\n",
            "Train: epoch 0061 / 0100 | batch 0028 / 0050 | loss 0.0338\n",
            "Train: epoch 0061 / 0100 | batch 0029 / 0050 | loss 0.0334\n",
            "Train: epoch 0061 / 0100 | batch 0030 / 0050 | loss 0.0333\n",
            "Train: epoch 0061 / 0100 | batch 0031 / 0050 | loss 0.0330\n",
            "Train: epoch 0061 / 0100 | batch 0032 / 0050 | loss 0.0333\n",
            "Train: epoch 0061 / 0100 | batch 0033 / 0050 | loss 0.0335\n",
            "Train: epoch 0061 / 0100 | batch 0034 / 0050 | loss 0.0340\n",
            "Train: epoch 0061 / 0100 | batch 0035 / 0050 | loss 0.0337\n",
            "Train: epoch 0061 / 0100 | batch 0036 / 0050 | loss 0.0334\n",
            "Train: epoch 0061 / 0100 | batch 0037 / 0050 | loss 0.0332\n",
            "Train: epoch 0061 / 0100 | batch 0038 / 0050 | loss 0.0334\n",
            "Train: epoch 0061 / 0100 | batch 0039 / 0050 | loss 0.0334\n",
            "Train: epoch 0061 / 0100 | batch 0040 / 0050 | loss 0.0338\n",
            "Train: epoch 0061 / 0100 | batch 0041 / 0050 | loss 0.0336\n",
            "Train: epoch 0061 / 0100 | batch 0042 / 0050 | loss 0.0337\n",
            "Train: epoch 0061 / 0100 | batch 0043 / 0050 | loss 0.0337\n",
            "Train: epoch 0061 / 0100 | batch 0044 / 0050 | loss 0.0336\n",
            "Train: epoch 0061 / 0100 | batch 0045 / 0050 | loss 0.0333\n",
            "Train: epoch 0061 / 0100 | batch 0046 / 0050 | loss 0.0335\n",
            "Train: epoch 0061 / 0100 | batch 0047 / 0050 | loss 0.0333\n",
            "Train: epoch 0061 / 0100 | batch 0048 / 0050 | loss 0.0335\n",
            "Train: epoch 0061 / 0100 | batch 0049 / 0050 | loss 0.0335\n",
            "Val loss 0.0318\n",
            "Dice score : 0.039389077574014664\n",
            "Val loss 0.0468\n",
            "Dice score : 0.1085609570145607\n",
            "Val loss 0.0391\n",
            "Dice score : 0.06726334989070892\n",
            "Val loss 0.0350\n",
            "Dice score : 0.0618915855884552\n",
            "Val loss 0.0359\n",
            "Dice score : 0.11791404336690903\n",
            "Val loss 0.0392\n",
            "Dice score : 0.09033960103988647\n",
            "Val loss 0.0373\n",
            "Dice score : 0.09093783795833588\n",
            "Val loss 0.0391\n",
            "Dice score : 0.05965734273195267\n",
            "Val loss 0.0382\n",
            "Dice score : 0.0666736364364624\n",
            "Val loss 0.0369\n",
            "Dice score : 0.06416700035333633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 61/100 [1:14:05<47:17, 72.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0062 / 0100 | batch 0000 / 0050 | loss 0.0269\n",
            "Train: epoch 0062 / 0100 | batch 0001 / 0050 | loss 0.0343\n",
            "Train: epoch 0062 / 0100 | batch 0002 / 0050 | loss 0.0317\n",
            "Train: epoch 0062 / 0100 | batch 0003 / 0050 | loss 0.0318\n",
            "Train: epoch 0062 / 0100 | batch 0004 / 0050 | loss 0.0305\n",
            "Train: epoch 0062 / 0100 | batch 0005 / 0050 | loss 0.0308\n",
            "Train: epoch 0062 / 0100 | batch 0006 / 0050 | loss 0.0295\n",
            "Train: epoch 0062 / 0100 | batch 0007 / 0050 | loss 0.0313\n",
            "Train: epoch 0062 / 0100 | batch 0008 / 0050 | loss 0.0305\n",
            "Train: epoch 0062 / 0100 | batch 0009 / 0050 | loss 0.0298\n",
            "Train: epoch 0062 / 0100 | batch 0010 / 0050 | loss 0.0303\n",
            "Train: epoch 0062 / 0100 | batch 0011 / 0050 | loss 0.0302\n",
            "Train: epoch 0062 / 0100 | batch 0012 / 0050 | loss 0.0307\n",
            "Train: epoch 0062 / 0100 | batch 0013 / 0050 | loss 0.0318\n",
            "Train: epoch 0062 / 0100 | batch 0014 / 0050 | loss 0.0316\n",
            "Train: epoch 0062 / 0100 | batch 0015 / 0050 | loss 0.0315\n",
            "Train: epoch 0062 / 0100 | batch 0016 / 0050 | loss 0.0311\n",
            "Train: epoch 0062 / 0100 | batch 0017 / 0050 | loss 0.0309\n",
            "Train: epoch 0062 / 0100 | batch 0018 / 0050 | loss 0.0307\n",
            "Train: epoch 0062 / 0100 | batch 0019 / 0050 | loss 0.0310\n",
            "Train: epoch 0062 / 0100 | batch 0020 / 0050 | loss 0.0312\n",
            "Train: epoch 0062 / 0100 | batch 0021 / 0050 | loss 0.0314\n",
            "Train: epoch 0062 / 0100 | batch 0022 / 0050 | loss 0.0317\n",
            "Train: epoch 0062 / 0100 | batch 0023 / 0050 | loss 0.0322\n",
            "Train: epoch 0062 / 0100 | batch 0024 / 0050 | loss 0.0327\n",
            "Train: epoch 0062 / 0100 | batch 0025 / 0050 | loss 0.0333\n",
            "Train: epoch 0062 / 0100 | batch 0026 / 0050 | loss 0.0334\n",
            "Train: epoch 0062 / 0100 | batch 0027 / 0050 | loss 0.0334\n",
            "Train: epoch 0062 / 0100 | batch 0028 / 0050 | loss 0.0331\n",
            "Train: epoch 0062 / 0100 | batch 0029 / 0050 | loss 0.0331\n",
            "Train: epoch 0062 / 0100 | batch 0030 / 0050 | loss 0.0328\n",
            "Train: epoch 0062 / 0100 | batch 0031 / 0050 | loss 0.0329\n",
            "Train: epoch 0062 / 0100 | batch 0032 / 0050 | loss 0.0333\n",
            "Train: epoch 0062 / 0100 | batch 0033 / 0050 | loss 0.0331\n",
            "Train: epoch 0062 / 0100 | batch 0034 / 0050 | loss 0.0329\n",
            "Train: epoch 0062 / 0100 | batch 0035 / 0050 | loss 0.0325\n",
            "Train: epoch 0062 / 0100 | batch 0036 / 0050 | loss 0.0322\n",
            "Train: epoch 0062 / 0100 | batch 0037 / 0050 | loss 0.0320\n",
            "Train: epoch 0062 / 0100 | batch 0038 / 0050 | loss 0.0321\n",
            "Train: epoch 0062 / 0100 | batch 0039 / 0050 | loss 0.0319\n",
            "Train: epoch 0062 / 0100 | batch 0040 / 0050 | loss 0.0323\n",
            "Train: epoch 0062 / 0100 | batch 0041 / 0050 | loss 0.0322\n",
            "Train: epoch 0062 / 0100 | batch 0042 / 0050 | loss 0.0320\n",
            "Train: epoch 0062 / 0100 | batch 0043 / 0050 | loss 0.0322\n",
            "Train: epoch 0062 / 0100 | batch 0044 / 0050 | loss 0.0322\n",
            "Train: epoch 0062 / 0100 | batch 0045 / 0050 | loss 0.0323\n",
            "Train: epoch 0062 / 0100 | batch 0046 / 0050 | loss 0.0321\n",
            "Train: epoch 0062 / 0100 | batch 0047 / 0050 | loss 0.0333\n",
            "Train: epoch 0062 / 0100 | batch 0048 / 0050 | loss 0.0331\n",
            "Train: epoch 0062 / 0100 | batch 0049 / 0050 | loss 0.0331\n",
            "Val loss 0.0428\n",
            "Dice score : 0.06605690717697144\n",
            "Val loss 0.0448\n",
            "Dice score : 0.08617965877056122\n",
            "Val loss 0.0431\n",
            "Dice score : 0.04178144410252571\n",
            "Val loss 0.0463\n",
            "Dice score : 0.07563651353120804\n",
            "Val loss 0.0417\n",
            "Dice score : 0.05748268589377403\n",
            "Val loss 0.0476\n",
            "Dice score : 0.07925227284431458\n",
            "Val loss 0.0488\n",
            "Dice score : 0.08060064166784286\n",
            "Val loss 0.0460\n",
            "Dice score : 0.07599467784166336\n",
            "Val loss 0.0434\n",
            "Dice score : 0.07993996888399124\n",
            "Val loss 0.0418\n",
            "Dice score : 0.0752435103058815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 62/100 [1:15:18<46:15, 73.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0063 / 0100 | batch 0000 / 0050 | loss 0.0282\n",
            "Train: epoch 0063 / 0100 | batch 0001 / 0050 | loss 0.0319\n",
            "Train: epoch 0063 / 0100 | batch 0002 / 0050 | loss 0.0297\n",
            "Train: epoch 0063 / 0100 | batch 0003 / 0050 | loss 0.0279\n",
            "Train: epoch 0063 / 0100 | batch 0004 / 0050 | loss 0.0304\n",
            "Train: epoch 0063 / 0100 | batch 0005 / 0050 | loss 0.0289\n",
            "Train: epoch 0063 / 0100 | batch 0006 / 0050 | loss 0.0304\n",
            "Train: epoch 0063 / 0100 | batch 0007 / 0050 | loss 0.0298\n",
            "Train: epoch 0063 / 0100 | batch 0008 / 0050 | loss 0.0315\n",
            "Train: epoch 0063 / 0100 | batch 0009 / 0050 | loss 0.0327\n",
            "Train: epoch 0063 / 0100 | batch 0010 / 0050 | loss 0.0320\n",
            "Train: epoch 0063 / 0100 | batch 0011 / 0050 | loss 0.0318\n",
            "Train: epoch 0063 / 0100 | batch 0012 / 0050 | loss 0.0341\n",
            "Train: epoch 0063 / 0100 | batch 0013 / 0050 | loss 0.0366\n",
            "Train: epoch 0063 / 0100 | batch 0014 / 0050 | loss 0.0360\n",
            "Train: epoch 0063 / 0100 | batch 0015 / 0050 | loss 0.0352\n",
            "Train: epoch 0063 / 0100 | batch 0016 / 0050 | loss 0.0355\n",
            "Train: epoch 0063 / 0100 | batch 0017 / 0050 | loss 0.0354\n",
            "Train: epoch 0063 / 0100 | batch 0018 / 0050 | loss 0.0353\n",
            "Train: epoch 0063 / 0100 | batch 0019 / 0050 | loss 0.0347\n",
            "Train: epoch 0063 / 0100 | batch 0020 / 0050 | loss 0.0355\n",
            "Train: epoch 0063 / 0100 | batch 0021 / 0050 | loss 0.0350\n",
            "Train: epoch 0063 / 0100 | batch 0022 / 0050 | loss 0.0353\n",
            "Train: epoch 0063 / 0100 | batch 0023 / 0050 | loss 0.0347\n",
            "Train: epoch 0063 / 0100 | batch 0024 / 0050 | loss 0.0344\n",
            "Train: epoch 0063 / 0100 | batch 0025 / 0050 | loss 0.0339\n",
            "Train: epoch 0063 / 0100 | batch 0026 / 0050 | loss 0.0338\n",
            "Train: epoch 0063 / 0100 | batch 0027 / 0050 | loss 0.0342\n",
            "Train: epoch 0063 / 0100 | batch 0028 / 0050 | loss 0.0338\n",
            "Train: epoch 0063 / 0100 | batch 0029 / 0050 | loss 0.0340\n",
            "Train: epoch 0063 / 0100 | batch 0030 / 0050 | loss 0.0337\n",
            "Train: epoch 0063 / 0100 | batch 0031 / 0050 | loss 0.0335\n",
            "Train: epoch 0063 / 0100 | batch 0032 / 0050 | loss 0.0334\n",
            "Train: epoch 0063 / 0100 | batch 0033 / 0050 | loss 0.0332\n",
            "Train: epoch 0063 / 0100 | batch 0034 / 0050 | loss 0.0334\n",
            "Train: epoch 0063 / 0100 | batch 0035 / 0050 | loss 0.0332\n",
            "Train: epoch 0063 / 0100 | batch 0036 / 0050 | loss 0.0333\n",
            "Train: epoch 0063 / 0100 | batch 0037 / 0050 | loss 0.0333\n",
            "Train: epoch 0063 / 0100 | batch 0038 / 0050 | loss 0.0331\n",
            "Train: epoch 0063 / 0100 | batch 0039 / 0050 | loss 0.0329\n",
            "Train: epoch 0063 / 0100 | batch 0040 / 0050 | loss 0.0328\n",
            "Train: epoch 0063 / 0100 | batch 0041 / 0050 | loss 0.0332\n",
            "Train: epoch 0063 / 0100 | batch 0042 / 0050 | loss 0.0332\n",
            "Train: epoch 0063 / 0100 | batch 0043 / 0050 | loss 0.0334\n",
            "Train: epoch 0063 / 0100 | batch 0044 / 0050 | loss 0.0332\n",
            "Train: epoch 0063 / 0100 | batch 0045 / 0050 | loss 0.0331\n",
            "Train: epoch 0063 / 0100 | batch 0046 / 0050 | loss 0.0335\n",
            "Train: epoch 0063 / 0100 | batch 0047 / 0050 | loss 0.0335\n",
            "Train: epoch 0063 / 0100 | batch 0048 / 0050 | loss 0.0333\n",
            "Train: epoch 0063 / 0100 | batch 0049 / 0050 | loss 0.0332\n",
            "Val loss 0.0464\n",
            "Dice score : 0.09648465365171432\n",
            "Val loss 0.0416\n",
            "Dice score : 0.06529240310192108\n",
            "Val loss 0.0379\n",
            "Dice score : 0.05528073012828827\n",
            "Val loss 0.0345\n",
            "Dice score : 0.08554773777723312\n",
            "Val loss 0.0320\n",
            "Dice score : 0.07274946570396423\n",
            "Val loss 0.0366\n",
            "Dice score : 0.0614171028137207\n",
            "Val loss 0.0363\n",
            "Dice score : 0.07161717861890793\n",
            "Val loss 0.0371\n",
            "Dice score : 0.11889103800058365\n",
            "Val loss 0.0357\n",
            "Dice score : 0.07846418768167496\n",
            "Val loss 0.0376\n",
            "Dice score : 0.09298655390739441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 63/100 [1:16:32<45:07, 73.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0064 / 0100 | batch 0000 / 0050 | loss 0.0286\n",
            "Train: epoch 0064 / 0100 | batch 0001 / 0050 | loss 0.0285\n",
            "Train: epoch 0064 / 0100 | batch 0002 / 0050 | loss 0.0340\n",
            "Train: epoch 0064 / 0100 | batch 0003 / 0050 | loss 0.0366\n",
            "Train: epoch 0064 / 0100 | batch 0004 / 0050 | loss 0.0365\n",
            "Train: epoch 0064 / 0100 | batch 0005 / 0050 | loss 0.0345\n",
            "Train: epoch 0064 / 0100 | batch 0006 / 0050 | loss 0.0342\n",
            "Train: epoch 0064 / 0100 | batch 0007 / 0050 | loss 0.0329\n",
            "Train: epoch 0064 / 0100 | batch 0008 / 0050 | loss 0.0325\n",
            "Train: epoch 0064 / 0100 | batch 0009 / 0050 | loss 0.0331\n",
            "Train: epoch 0064 / 0100 | batch 0010 / 0050 | loss 0.0326\n",
            "Train: epoch 0064 / 0100 | batch 0011 / 0050 | loss 0.0315\n",
            "Train: epoch 0064 / 0100 | batch 0012 / 0050 | loss 0.0307\n",
            "Train: epoch 0064 / 0100 | batch 0013 / 0050 | loss 0.0305\n",
            "Train: epoch 0064 / 0100 | batch 0014 / 0050 | loss 0.0316\n",
            "Train: epoch 0064 / 0100 | batch 0015 / 0050 | loss 0.0327\n",
            "Train: epoch 0064 / 0100 | batch 0016 / 0050 | loss 0.0330\n",
            "Train: epoch 0064 / 0100 | batch 0017 / 0050 | loss 0.0326\n",
            "Train: epoch 0064 / 0100 | batch 0018 / 0050 | loss 0.0321\n",
            "Train: epoch 0064 / 0100 | batch 0019 / 0050 | loss 0.0316\n",
            "Train: epoch 0064 / 0100 | batch 0020 / 0050 | loss 0.0313\n",
            "Train: epoch 0064 / 0100 | batch 0021 / 0050 | loss 0.0310\n",
            "Train: epoch 0064 / 0100 | batch 0022 / 0050 | loss 0.0312\n",
            "Train: epoch 0064 / 0100 | batch 0023 / 0050 | loss 0.0315\n",
            "Train: epoch 0064 / 0100 | batch 0024 / 0050 | loss 0.0313\n",
            "Train: epoch 0064 / 0100 | batch 0025 / 0050 | loss 0.0315\n",
            "Train: epoch 0064 / 0100 | batch 0026 / 0050 | loss 0.0321\n",
            "Train: epoch 0064 / 0100 | batch 0027 / 0050 | loss 0.0321\n",
            "Train: epoch 0064 / 0100 | batch 0028 / 0050 | loss 0.0321\n",
            "Train: epoch 0064 / 0100 | batch 0029 / 0050 | loss 0.0322\n",
            "Train: epoch 0064 / 0100 | batch 0030 / 0050 | loss 0.0322\n",
            "Train: epoch 0064 / 0100 | batch 0031 / 0050 | loss 0.0324\n",
            "Train: epoch 0064 / 0100 | batch 0032 / 0050 | loss 0.0326\n",
            "Train: epoch 0064 / 0100 | batch 0033 / 0050 | loss 0.0325\n",
            "Train: epoch 0064 / 0100 | batch 0034 / 0050 | loss 0.0323\n",
            "Train: epoch 0064 / 0100 | batch 0035 / 0050 | loss 0.0327\n",
            "Train: epoch 0064 / 0100 | batch 0036 / 0050 | loss 0.0328\n",
            "Train: epoch 0064 / 0100 | batch 0037 / 0050 | loss 0.0329\n",
            "Train: epoch 0064 / 0100 | batch 0038 / 0050 | loss 0.0328\n",
            "Train: epoch 0064 / 0100 | batch 0039 / 0050 | loss 0.0327\n",
            "Train: epoch 0064 / 0100 | batch 0040 / 0050 | loss 0.0328\n",
            "Train: epoch 0064 / 0100 | batch 0041 / 0050 | loss 0.0324\n",
            "Train: epoch 0064 / 0100 | batch 0042 / 0050 | loss 0.0323\n",
            "Train: epoch 0064 / 0100 | batch 0043 / 0050 | loss 0.0322\n",
            "Train: epoch 0064 / 0100 | batch 0044 / 0050 | loss 0.0324\n",
            "Train: epoch 0064 / 0100 | batch 0045 / 0050 | loss 0.0324\n",
            "Train: epoch 0064 / 0100 | batch 0046 / 0050 | loss 0.0322\n",
            "Train: epoch 0064 / 0100 | batch 0047 / 0050 | loss 0.0323\n",
            "Train: epoch 0064 / 0100 | batch 0048 / 0050 | loss 0.0324\n",
            "Train: epoch 0064 / 0100 | batch 0049 / 0050 | loss 0.0323\n",
            "Val loss 0.0240\n",
            "Dice score : 0.08325424045324326\n",
            "Val loss 0.0369\n",
            "Dice score : 0.09558982402086258\n",
            "Val loss 0.0411\n",
            "Dice score : 0.09602933377027512\n",
            "Val loss 0.0389\n",
            "Dice score : 0.055246368050575256\n",
            "Val loss 0.0366\n",
            "Dice score : 0.06660300493240356\n",
            "Val loss 0.0352\n",
            "Dice score : 0.07793904840946198\n",
            "Val loss 0.0397\n",
            "Dice score : 0.12150793522596359\n",
            "Val loss 0.0378\n",
            "Dice score : 0.05899563804268837\n",
            "Val loss 0.0367\n",
            "Dice score : 0.09593556076288223\n",
            "Val loss 0.0376\n",
            "Dice score : 0.036071933805942535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 64/100 [1:17:45<43:56, 73.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0065 / 0100 | batch 0000 / 0050 | loss 0.0224\n",
            "Train: epoch 0065 / 0100 | batch 0001 / 0050 | loss 0.0285\n",
            "Train: epoch 0065 / 0100 | batch 0002 / 0050 | loss 0.0304\n",
            "Train: epoch 0065 / 0100 | batch 0003 / 0050 | loss 0.0298\n",
            "Train: epoch 0065 / 0100 | batch 0004 / 0050 | loss 0.0311\n",
            "Train: epoch 0065 / 0100 | batch 0005 / 0050 | loss 0.0319\n",
            "Train: epoch 0065 / 0100 | batch 0006 / 0050 | loss 0.0335\n",
            "Train: epoch 0065 / 0100 | batch 0007 / 0050 | loss 0.0330\n",
            "Train: epoch 0065 / 0100 | batch 0008 / 0050 | loss 0.0355\n",
            "Train: epoch 0065 / 0100 | batch 0009 / 0050 | loss 0.0351\n",
            "Train: epoch 0065 / 0100 | batch 0010 / 0050 | loss 0.0347\n",
            "Train: epoch 0065 / 0100 | batch 0011 / 0050 | loss 0.0346\n",
            "Train: epoch 0065 / 0100 | batch 0012 / 0050 | loss 0.0343\n",
            "Train: epoch 0065 / 0100 | batch 0013 / 0050 | loss 0.0340\n",
            "Train: epoch 0065 / 0100 | batch 0014 / 0050 | loss 0.0346\n",
            "Train: epoch 0065 / 0100 | batch 0015 / 0050 | loss 0.0339\n",
            "Train: epoch 0065 / 0100 | batch 0016 / 0050 | loss 0.0333\n",
            "Train: epoch 0065 / 0100 | batch 0017 / 0050 | loss 0.0331\n",
            "Train: epoch 0065 / 0100 | batch 0018 / 0050 | loss 0.0328\n",
            "Train: epoch 0065 / 0100 | batch 0019 / 0050 | loss 0.0326\n",
            "Train: epoch 0065 / 0100 | batch 0020 / 0050 | loss 0.0333\n",
            "Train: epoch 0065 / 0100 | batch 0021 / 0050 | loss 0.0334\n",
            "Train: epoch 0065 / 0100 | batch 0022 / 0050 | loss 0.0330\n",
            "Train: epoch 0065 / 0100 | batch 0023 / 0050 | loss 0.0330\n",
            "Train: epoch 0065 / 0100 | batch 0024 / 0050 | loss 0.0331\n",
            "Train: epoch 0065 / 0100 | batch 0025 / 0050 | loss 0.0334\n",
            "Train: epoch 0065 / 0100 | batch 0026 / 0050 | loss 0.0331\n",
            "Train: epoch 0065 / 0100 | batch 0027 / 0050 | loss 0.0332\n",
            "Train: epoch 0065 / 0100 | batch 0028 / 0050 | loss 0.0333\n",
            "Train: epoch 0065 / 0100 | batch 0029 / 0050 | loss 0.0333\n",
            "Train: epoch 0065 / 0100 | batch 0030 / 0050 | loss 0.0329\n",
            "Train: epoch 0065 / 0100 | batch 0031 / 0050 | loss 0.0328\n",
            "Train: epoch 0065 / 0100 | batch 0032 / 0050 | loss 0.0324\n",
            "Train: epoch 0065 / 0100 | batch 0033 / 0050 | loss 0.0327\n",
            "Train: epoch 0065 / 0100 | batch 0034 / 0050 | loss 0.0332\n",
            "Train: epoch 0065 / 0100 | batch 0035 / 0050 | loss 0.0331\n",
            "Train: epoch 0065 / 0100 | batch 0036 / 0050 | loss 0.0336\n",
            "Train: epoch 0065 / 0100 | batch 0037 / 0050 | loss 0.0333\n",
            "Train: epoch 0065 / 0100 | batch 0038 / 0050 | loss 0.0333\n",
            "Train: epoch 0065 / 0100 | batch 0039 / 0050 | loss 0.0336\n",
            "Train: epoch 0065 / 0100 | batch 0040 / 0050 | loss 0.0334\n",
            "Train: epoch 0065 / 0100 | batch 0041 / 0050 | loss 0.0331\n",
            "Train: epoch 0065 / 0100 | batch 0042 / 0050 | loss 0.0331\n",
            "Train: epoch 0065 / 0100 | batch 0043 / 0050 | loss 0.0328\n",
            "Train: epoch 0065 / 0100 | batch 0044 / 0050 | loss 0.0326\n",
            "Train: epoch 0065 / 0100 | batch 0045 / 0050 | loss 0.0330\n",
            "Train: epoch 0065 / 0100 | batch 0046 / 0050 | loss 0.0333\n",
            "Train: epoch 0065 / 0100 | batch 0047 / 0050 | loss 0.0331\n",
            "Train: epoch 0065 / 0100 | batch 0048 / 0050 | loss 0.0329\n",
            "Train: epoch 0065 / 0100 | batch 0049 / 0050 | loss 0.0327\n",
            "Val loss 0.0269\n",
            "Dice score : 0.07077008485794067\n",
            "Val loss 0.0259\n",
            "Dice score : 0.0602128766477108\n",
            "Val loss 0.0316\n",
            "Dice score : 0.08713062107563019\n",
            "Val loss 0.0295\n",
            "Dice score : 0.05591998249292374\n",
            "Val loss 0.0287\n",
            "Dice score : 0.03955774009227753\n",
            "Val loss 0.0279\n",
            "Dice score : 0.06620455533266068\n",
            "Val loss 0.0286\n",
            "Dice score : 0.08065343648195267\n",
            "Val loss 0.0317\n",
            "Dice score : 0.10151364654302597\n",
            "Val loss 0.0318\n",
            "Dice score : 0.05071265250444412\n",
            "Val loss 0.0348\n",
            "Dice score : 0.12888562679290771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 65/100 [1:18:58<42:38, 73.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0066 / 0100 | batch 0000 / 0050 | loss 0.0283\n",
            "Train: epoch 0066 / 0100 | batch 0001 / 0050 | loss 0.0242\n",
            "Train: epoch 0066 / 0100 | batch 0002 / 0050 | loss 0.0227\n",
            "Train: epoch 0066 / 0100 | batch 0003 / 0050 | loss 0.0246\n",
            "Train: epoch 0066 / 0100 | batch 0004 / 0050 | loss 0.0280\n",
            "Train: epoch 0066 / 0100 | batch 0005 / 0050 | loss 0.0311\n",
            "Train: epoch 0066 / 0100 | batch 0006 / 0050 | loss 0.0296\n",
            "Train: epoch 0066 / 0100 | batch 0007 / 0050 | loss 0.0284\n",
            "Train: epoch 0066 / 0100 | batch 0008 / 0050 | loss 0.0281\n",
            "Train: epoch 0066 / 0100 | batch 0009 / 0050 | loss 0.0273\n",
            "Train: epoch 0066 / 0100 | batch 0010 / 0050 | loss 0.0270\n",
            "Train: epoch 0066 / 0100 | batch 0011 / 0050 | loss 0.0282\n",
            "Train: epoch 0066 / 0100 | batch 0012 / 0050 | loss 0.0283\n",
            "Train: epoch 0066 / 0100 | batch 0013 / 0050 | loss 0.0284\n",
            "Train: epoch 0066 / 0100 | batch 0014 / 0050 | loss 0.0289\n",
            "Train: epoch 0066 / 0100 | batch 0015 / 0050 | loss 0.0286\n",
            "Train: epoch 0066 / 0100 | batch 0016 / 0050 | loss 0.0286\n",
            "Train: epoch 0066 / 0100 | batch 0017 / 0050 | loss 0.0297\n",
            "Train: epoch 0066 / 0100 | batch 0018 / 0050 | loss 0.0301\n",
            "Train: epoch 0066 / 0100 | batch 0019 / 0050 | loss 0.0302\n",
            "Train: epoch 0066 / 0100 | batch 0020 / 0050 | loss 0.0299\n",
            "Train: epoch 0066 / 0100 | batch 0021 / 0050 | loss 0.0298\n",
            "Train: epoch 0066 / 0100 | batch 0022 / 0050 | loss 0.0301\n",
            "Train: epoch 0066 / 0100 | batch 0023 / 0050 | loss 0.0297\n",
            "Train: epoch 0066 / 0100 | batch 0024 / 0050 | loss 0.0297\n",
            "Train: epoch 0066 / 0100 | batch 0025 / 0050 | loss 0.0299\n",
            "Train: epoch 0066 / 0100 | batch 0026 / 0050 | loss 0.0298\n",
            "Train: epoch 0066 / 0100 | batch 0027 / 0050 | loss 0.0297\n",
            "Train: epoch 0066 / 0100 | batch 0028 / 0050 | loss 0.0315\n",
            "Train: epoch 0066 / 0100 | batch 0029 / 0050 | loss 0.0314\n",
            "Train: epoch 0066 / 0100 | batch 0030 / 0050 | loss 0.0311\n",
            "Train: epoch 0066 / 0100 | batch 0031 / 0050 | loss 0.0313\n",
            "Train: epoch 0066 / 0100 | batch 0032 / 0050 | loss 0.0314\n",
            "Train: epoch 0066 / 0100 | batch 0033 / 0050 | loss 0.0311\n",
            "Train: epoch 0066 / 0100 | batch 0034 / 0050 | loss 0.0310\n",
            "Train: epoch 0066 / 0100 | batch 0035 / 0050 | loss 0.0312\n",
            "Train: epoch 0066 / 0100 | batch 0036 / 0050 | loss 0.0312\n",
            "Train: epoch 0066 / 0100 | batch 0037 / 0050 | loss 0.0308\n",
            "Train: epoch 0066 / 0100 | batch 0038 / 0050 | loss 0.0308\n",
            "Train: epoch 0066 / 0100 | batch 0039 / 0050 | loss 0.0305\n",
            "Train: epoch 0066 / 0100 | batch 0040 / 0050 | loss 0.0308\n",
            "Train: epoch 0066 / 0100 | batch 0041 / 0050 | loss 0.0309\n",
            "Train: epoch 0066 / 0100 | batch 0042 / 0050 | loss 0.0309\n",
            "Train: epoch 0066 / 0100 | batch 0043 / 0050 | loss 0.0311\n",
            "Train: epoch 0066 / 0100 | batch 0044 / 0050 | loss 0.0310\n",
            "Train: epoch 0066 / 0100 | batch 0045 / 0050 | loss 0.0311\n",
            "Train: epoch 0066 / 0100 | batch 0046 / 0050 | loss 0.0312\n",
            "Train: epoch 0066 / 0100 | batch 0047 / 0050 | loss 0.0311\n",
            "Train: epoch 0066 / 0100 | batch 0048 / 0050 | loss 0.0314\n",
            "Train: epoch 0066 / 0100 | batch 0049 / 0050 | loss 0.0316\n",
            "Val loss 0.0585\n",
            "Dice score : 0.0828128531575203\n",
            "Val loss 0.0532\n",
            "Dice score : 0.0705513060092926\n",
            "Val loss 0.0489\n",
            "Dice score : 0.14100788533687592\n",
            "Val loss 0.0425\n",
            "Dice score : 0.08047308772802353\n",
            "Val loss 0.0394\n",
            "Dice score : 0.05549822375178337\n",
            "Val loss 0.0372\n",
            "Dice score : 0.08184204250574112\n",
            "Val loss 0.0387\n",
            "Dice score : 0.05750175938010216\n",
            "Val loss 0.0381\n",
            "Dice score : 0.06780219823122025\n",
            "Val loss 0.0364\n",
            "Dice score : 0.05640251934528351\n",
            "Val loss 0.0373\n",
            "Dice score : 0.09443873167037964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 66/100 [1:20:11<41:25, 73.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0067 / 0100 | batch 0000 / 0050 | loss 0.0287\n",
            "Train: epoch 0067 / 0100 | batch 0001 / 0050 | loss 0.0261\n",
            "Train: epoch 0067 / 0100 | batch 0002 / 0050 | loss 0.0284\n",
            "Train: epoch 0067 / 0100 | batch 0003 / 0050 | loss 0.0294\n",
            "Train: epoch 0067 / 0100 | batch 0004 / 0050 | loss 0.0310\n",
            "Train: epoch 0067 / 0100 | batch 0005 / 0050 | loss 0.0310\n",
            "Train: epoch 0067 / 0100 | batch 0006 / 0050 | loss 0.0295\n",
            "Train: epoch 0067 / 0100 | batch 0007 / 0050 | loss 0.0285\n",
            "Train: epoch 0067 / 0100 | batch 0008 / 0050 | loss 0.0296\n",
            "Train: epoch 0067 / 0100 | batch 0009 / 0050 | loss 0.0311\n",
            "Train: epoch 0067 / 0100 | batch 0010 / 0050 | loss 0.0326\n",
            "Train: epoch 0067 / 0100 | batch 0011 / 0050 | loss 0.0322\n",
            "Train: epoch 0067 / 0100 | batch 0012 / 0050 | loss 0.0316\n",
            "Train: epoch 0067 / 0100 | batch 0013 / 0050 | loss 0.0313\n",
            "Train: epoch 0067 / 0100 | batch 0014 / 0050 | loss 0.0307\n",
            "Train: epoch 0067 / 0100 | batch 0015 / 0050 | loss 0.0301\n",
            "Train: epoch 0067 / 0100 | batch 0016 / 0050 | loss 0.0300\n",
            "Train: epoch 0067 / 0100 | batch 0017 / 0050 | loss 0.0296\n",
            "Train: epoch 0067 / 0100 | batch 0018 / 0050 | loss 0.0304\n",
            "Train: epoch 0067 / 0100 | batch 0019 / 0050 | loss 0.0301\n",
            "Train: epoch 0067 / 0100 | batch 0020 / 0050 | loss 0.0298\n",
            "Train: epoch 0067 / 0100 | batch 0021 / 0050 | loss 0.0300\n",
            "Train: epoch 0067 / 0100 | batch 0022 / 0050 | loss 0.0297\n",
            "Train: epoch 0067 / 0100 | batch 0023 / 0050 | loss 0.0300\n",
            "Train: epoch 0067 / 0100 | batch 0024 / 0050 | loss 0.0300\n",
            "Train: epoch 0067 / 0100 | batch 0025 / 0050 | loss 0.0299\n",
            "Train: epoch 0067 / 0100 | batch 0026 / 0050 | loss 0.0299\n",
            "Train: epoch 0067 / 0100 | batch 0027 / 0050 | loss 0.0298\n",
            "Train: epoch 0067 / 0100 | batch 0028 / 0050 | loss 0.0294\n",
            "Train: epoch 0067 / 0100 | batch 0029 / 0050 | loss 0.0299\n",
            "Train: epoch 0067 / 0100 | batch 0030 / 0050 | loss 0.0298\n",
            "Train: epoch 0067 / 0100 | batch 0031 / 0050 | loss 0.0297\n",
            "Train: epoch 0067 / 0100 | batch 0032 / 0050 | loss 0.0302\n",
            "Train: epoch 0067 / 0100 | batch 0033 / 0050 | loss 0.0302\n",
            "Train: epoch 0067 / 0100 | batch 0034 / 0050 | loss 0.0301\n",
            "Train: epoch 0067 / 0100 | batch 0035 / 0050 | loss 0.0300\n",
            "Train: epoch 0067 / 0100 | batch 0036 / 0050 | loss 0.0299\n",
            "Train: epoch 0067 / 0100 | batch 0037 / 0050 | loss 0.0297\n",
            "Train: epoch 0067 / 0100 | batch 0038 / 0050 | loss 0.0298\n",
            "Train: epoch 0067 / 0100 | batch 0039 / 0050 | loss 0.0299\n",
            "Train: epoch 0067 / 0100 | batch 0040 / 0050 | loss 0.0297\n",
            "Train: epoch 0067 / 0100 | batch 0041 / 0050 | loss 0.0295\n",
            "Train: epoch 0067 / 0100 | batch 0042 / 0050 | loss 0.0302\n",
            "Train: epoch 0067 / 0100 | batch 0043 / 0050 | loss 0.0305\n",
            "Train: epoch 0067 / 0100 | batch 0044 / 0050 | loss 0.0306\n",
            "Train: epoch 0067 / 0100 | batch 0045 / 0050 | loss 0.0308\n",
            "Train: epoch 0067 / 0100 | batch 0046 / 0050 | loss 0.0312\n",
            "Train: epoch 0067 / 0100 | batch 0047 / 0050 | loss 0.0314\n",
            "Train: epoch 0067 / 0100 | batch 0048 / 0050 | loss 0.0313\n",
            "Train: epoch 0067 / 0100 | batch 0049 / 0050 | loss 0.0314\n",
            "Val loss 0.0245\n",
            "Dice score : 0.09187167882919312\n",
            "Val loss 0.0245\n",
            "Dice score : 0.09139945358037949\n",
            "Val loss 0.0309\n",
            "Dice score : 0.07371459156274796\n",
            "Val loss 0.0317\n",
            "Dice score : 0.07089576870203018\n",
            "Val loss 0.0297\n",
            "Dice score : 0.05913892760872841\n",
            "Val loss 0.0305\n",
            "Dice score : 0.06166885793209076\n",
            "Val loss 0.0358\n",
            "Dice score : 0.08284761756658554\n",
            "Val loss 0.0380\n",
            "Dice score : 0.07736726850271225\n",
            "Val loss 0.0388\n",
            "Dice score : 0.07294774055480957\n",
            "Val loss 0.0381\n",
            "Dice score : 0.06307093799114227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 67/100 [1:21:24<40:13, 73.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0068 / 0100 | batch 0000 / 0050 | loss 0.0430\n",
            "Train: epoch 0068 / 0100 | batch 0001 / 0050 | loss 0.0379\n",
            "Train: epoch 0068 / 0100 | batch 0002 / 0050 | loss 0.0351\n",
            "Train: epoch 0068 / 0100 | batch 0003 / 0050 | loss 0.0351\n",
            "Train: epoch 0068 / 0100 | batch 0004 / 0050 | loss 0.0369\n",
            "Train: epoch 0068 / 0100 | batch 0005 / 0050 | loss 0.0353\n",
            "Train: epoch 0068 / 0100 | batch 0006 / 0050 | loss 0.0331\n",
            "Train: epoch 0068 / 0100 | batch 0007 / 0050 | loss 0.0316\n",
            "Train: epoch 0068 / 0100 | batch 0008 / 0050 | loss 0.0321\n",
            "Train: epoch 0068 / 0100 | batch 0009 / 0050 | loss 0.0312\n",
            "Train: epoch 0068 / 0100 | batch 0010 / 0050 | loss 0.0328\n",
            "Train: epoch 0068 / 0100 | batch 0011 / 0050 | loss 0.0332\n",
            "Train: epoch 0068 / 0100 | batch 0012 / 0050 | loss 0.0324\n",
            "Train: epoch 0068 / 0100 | batch 0013 / 0050 | loss 0.0328\n",
            "Train: epoch 0068 / 0100 | batch 0014 / 0050 | loss 0.0323\n",
            "Train: epoch 0068 / 0100 | batch 0015 / 0050 | loss 0.0319\n",
            "Train: epoch 0068 / 0100 | batch 0016 / 0050 | loss 0.0316\n",
            "Train: epoch 0068 / 0100 | batch 0017 / 0050 | loss 0.0320\n",
            "Train: epoch 0068 / 0100 | batch 0018 / 0050 | loss 0.0322\n",
            "Train: epoch 0068 / 0100 | batch 0019 / 0050 | loss 0.0319\n",
            "Train: epoch 0068 / 0100 | batch 0020 / 0050 | loss 0.0316\n",
            "Train: epoch 0068 / 0100 | batch 0021 / 0050 | loss 0.0316\n",
            "Train: epoch 0068 / 0100 | batch 0022 / 0050 | loss 0.0318\n",
            "Train: epoch 0068 / 0100 | batch 0023 / 0050 | loss 0.0317\n",
            "Train: epoch 0068 / 0100 | batch 0024 / 0050 | loss 0.0313\n",
            "Train: epoch 0068 / 0100 | batch 0025 / 0050 | loss 0.0322\n",
            "Train: epoch 0068 / 0100 | batch 0026 / 0050 | loss 0.0328\n",
            "Train: epoch 0068 / 0100 | batch 0027 / 0050 | loss 0.0324\n",
            "Train: epoch 0068 / 0100 | batch 0028 / 0050 | loss 0.0323\n",
            "Train: epoch 0068 / 0100 | batch 0029 / 0050 | loss 0.0321\n",
            "Train: epoch 0068 / 0100 | batch 0030 / 0050 | loss 0.0319\n",
            "Train: epoch 0068 / 0100 | batch 0031 / 0050 | loss 0.0316\n",
            "Train: epoch 0068 / 0100 | batch 0032 / 0050 | loss 0.0313\n",
            "Train: epoch 0068 / 0100 | batch 0033 / 0050 | loss 0.0310\n",
            "Train: epoch 0068 / 0100 | batch 0034 / 0050 | loss 0.0311\n",
            "Train: epoch 0068 / 0100 | batch 0035 / 0050 | loss 0.0316\n",
            "Train: epoch 0068 / 0100 | batch 0036 / 0050 | loss 0.0312\n",
            "Train: epoch 0068 / 0100 | batch 0037 / 0050 | loss 0.0312\n",
            "Train: epoch 0068 / 0100 | batch 0038 / 0050 | loss 0.0309\n",
            "Train: epoch 0068 / 0100 | batch 0039 / 0050 | loss 0.0312\n",
            "Train: epoch 0068 / 0100 | batch 0040 / 0050 | loss 0.0309\n",
            "Train: epoch 0068 / 0100 | batch 0041 / 0050 | loss 0.0307\n",
            "Train: epoch 0068 / 0100 | batch 0042 / 0050 | loss 0.0307\n",
            "Train: epoch 0068 / 0100 | batch 0043 / 0050 | loss 0.0307\n",
            "Train: epoch 0068 / 0100 | batch 0044 / 0050 | loss 0.0306\n",
            "Train: epoch 0068 / 0100 | batch 0045 / 0050 | loss 0.0314\n",
            "Train: epoch 0068 / 0100 | batch 0046 / 0050 | loss 0.0315\n",
            "Train: epoch 0068 / 0100 | batch 0047 / 0050 | loss 0.0314\n",
            "Train: epoch 0068 / 0100 | batch 0048 / 0050 | loss 0.0313\n",
            "Train: epoch 0068 / 0100 | batch 0049 / 0050 | loss 0.0311\n",
            "Val loss 0.0372\n",
            "Dice score : 0.052874449640512466\n",
            "Val loss 0.0372\n",
            "Dice score : 0.12419416755437851\n",
            "Val loss 0.0326\n",
            "Dice score : 0.05642913654446602\n",
            "Val loss 0.0301\n",
            "Dice score : 0.0635981559753418\n",
            "Val loss 0.0394\n",
            "Dice score : 0.10099493712186813\n",
            "Val loss 0.0409\n",
            "Dice score : 0.08879246562719345\n",
            "Val loss 0.0389\n",
            "Dice score : 0.09713024646043777\n",
            "Val loss 0.0386\n",
            "Dice score : 0.07784945517778397\n",
            "Val loss 0.0376\n",
            "Dice score : 0.06303362548351288\n",
            "Val loss 0.0361\n",
            "Dice score : 0.04817778989672661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 68/100 [1:22:37<38:53, 72.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0069 / 0100 | batch 0000 / 0050 | loss 0.0191\n",
            "Train: epoch 0069 / 0100 | batch 0001 / 0050 | loss 0.0207\n",
            "Train: epoch 0069 / 0100 | batch 0002 / 0050 | loss 0.0204\n",
            "Train: epoch 0069 / 0100 | batch 0003 / 0050 | loss 0.0227\n",
            "Train: epoch 0069 / 0100 | batch 0004 / 0050 | loss 0.0241\n",
            "Train: epoch 0069 / 0100 | batch 0005 / 0050 | loss 0.0274\n",
            "Train: epoch 0069 / 0100 | batch 0006 / 0050 | loss 0.0267\n",
            "Train: epoch 0069 / 0100 | batch 0007 / 0050 | loss 0.0263\n",
            "Train: epoch 0069 / 0100 | batch 0008 / 0050 | loss 0.0259\n",
            "Train: epoch 0069 / 0100 | batch 0009 / 0050 | loss 0.0275\n",
            "Train: epoch 0069 / 0100 | batch 0010 / 0050 | loss 0.0295\n",
            "Train: epoch 0069 / 0100 | batch 0011 / 0050 | loss 0.0290\n",
            "Train: epoch 0069 / 0100 | batch 0012 / 0050 | loss 0.0286\n",
            "Train: epoch 0069 / 0100 | batch 0013 / 0050 | loss 0.0282\n",
            "Train: epoch 0069 / 0100 | batch 0014 / 0050 | loss 0.0286\n",
            "Train: epoch 0069 / 0100 | batch 0015 / 0050 | loss 0.0287\n",
            "Train: epoch 0069 / 0100 | batch 0016 / 0050 | loss 0.0287\n",
            "Train: epoch 0069 / 0100 | batch 0017 / 0050 | loss 0.0298\n",
            "Train: epoch 0069 / 0100 | batch 0018 / 0050 | loss 0.0302\n",
            "Train: epoch 0069 / 0100 | batch 0019 / 0050 | loss 0.0298\n",
            "Train: epoch 0069 / 0100 | batch 0020 / 0050 | loss 0.0296\n",
            "Train: epoch 0069 / 0100 | batch 0021 / 0050 | loss 0.0298\n",
            "Train: epoch 0069 / 0100 | batch 0022 / 0050 | loss 0.0297\n",
            "Train: epoch 0069 / 0100 | batch 0023 / 0050 | loss 0.0297\n",
            "Train: epoch 0069 / 0100 | batch 0024 / 0050 | loss 0.0296\n",
            "Train: epoch 0069 / 0100 | batch 0025 / 0050 | loss 0.0294\n",
            "Train: epoch 0069 / 0100 | batch 0026 / 0050 | loss 0.0297\n",
            "Train: epoch 0069 / 0100 | batch 0027 / 0050 | loss 0.0293\n",
            "Train: epoch 0069 / 0100 | batch 0028 / 0050 | loss 0.0296\n",
            "Train: epoch 0069 / 0100 | batch 0029 / 0050 | loss 0.0299\n",
            "Train: epoch 0069 / 0100 | batch 0030 / 0050 | loss 0.0299\n",
            "Train: epoch 0069 / 0100 | batch 0031 / 0050 | loss 0.0303\n",
            "Train: epoch 0069 / 0100 | batch 0032 / 0050 | loss 0.0303\n",
            "Train: epoch 0069 / 0100 | batch 0033 / 0050 | loss 0.0301\n",
            "Train: epoch 0069 / 0100 | batch 0034 / 0050 | loss 0.0300\n",
            "Train: epoch 0069 / 0100 | batch 0035 / 0050 | loss 0.0297\n",
            "Train: epoch 0069 / 0100 | batch 0036 / 0050 | loss 0.0298\n",
            "Train: epoch 0069 / 0100 | batch 0037 / 0050 | loss 0.0296\n",
            "Train: epoch 0069 / 0100 | batch 0038 / 0050 | loss 0.0300\n",
            "Train: epoch 0069 / 0100 | batch 0039 / 0050 | loss 0.0297\n",
            "Train: epoch 0069 / 0100 | batch 0040 / 0050 | loss 0.0297\n",
            "Train: epoch 0069 / 0100 | batch 0041 / 0050 | loss 0.0299\n",
            "Train: epoch 0069 / 0100 | batch 0042 / 0050 | loss 0.0301\n",
            "Train: epoch 0069 / 0100 | batch 0043 / 0050 | loss 0.0300\n",
            "Train: epoch 0069 / 0100 | batch 0044 / 0050 | loss 0.0302\n",
            "Train: epoch 0069 / 0100 | batch 0045 / 0050 | loss 0.0302\n",
            "Train: epoch 0069 / 0100 | batch 0046 / 0050 | loss 0.0304\n",
            "Train: epoch 0069 / 0100 | batch 0047 / 0050 | loss 0.0303\n",
            "Train: epoch 0069 / 0100 | batch 0048 / 0050 | loss 0.0302\n",
            "Train: epoch 0069 / 0100 | batch 0049 / 0050 | loss 0.0302\n",
            "Val loss 0.0253\n",
            "Dice score : 0.050835542380809784\n",
            "Val loss 0.0267\n",
            "Dice score : 0.10211196541786194\n",
            "Val loss 0.0324\n",
            "Dice score : 0.11543271690607071\n",
            "Val loss 0.0302\n",
            "Dice score : 0.10460405051708221\n",
            "Val loss 0.0325\n",
            "Dice score : 0.043708451092243195\n",
            "Val loss 0.0411\n",
            "Dice score : 0.10286550968885422\n",
            "Val loss 0.0396\n",
            "Dice score : 0.06819489598274231\n",
            "Val loss 0.0395\n",
            "Dice score : 0.12531334161758423\n",
            "Val loss 0.0391\n",
            "Dice score : 0.07668045908212662\n",
            "Val loss 0.0376\n",
            "Dice score : 0.0873657613992691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 69/100 [1:23:49<37:31, 72.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0070 / 0100 | batch 0000 / 0050 | loss 0.0288\n",
            "Train: epoch 0070 / 0100 | batch 0001 / 0050 | loss 0.0300\n",
            "Train: epoch 0070 / 0100 | batch 0002 / 0050 | loss 0.0360\n",
            "Train: epoch 0070 / 0100 | batch 0003 / 0050 | loss 0.0329\n",
            "Train: epoch 0070 / 0100 | batch 0004 / 0050 | loss 0.0335\n",
            "Train: epoch 0070 / 0100 | batch 0005 / 0050 | loss 0.0374\n",
            "Train: epoch 0070 / 0100 | batch 0006 / 0050 | loss 0.0362\n",
            "Train: epoch 0070 / 0100 | batch 0007 / 0050 | loss 0.0344\n",
            "Train: epoch 0070 / 0100 | batch 0008 / 0050 | loss 0.0341\n",
            "Train: epoch 0070 / 0100 | batch 0009 / 0050 | loss 0.0346\n",
            "Train: epoch 0070 / 0100 | batch 0010 / 0050 | loss 0.0336\n",
            "Train: epoch 0070 / 0100 | batch 0011 / 0050 | loss 0.0338\n",
            "Train: epoch 0070 / 0100 | batch 0012 / 0050 | loss 0.0336\n",
            "Train: epoch 0070 / 0100 | batch 0013 / 0050 | loss 0.0327\n",
            "Train: epoch 0070 / 0100 | batch 0014 / 0050 | loss 0.0333\n",
            "Train: epoch 0070 / 0100 | batch 0015 / 0050 | loss 0.0329\n",
            "Train: epoch 0070 / 0100 | batch 0016 / 0050 | loss 0.0327\n",
            "Train: epoch 0070 / 0100 | batch 0017 / 0050 | loss 0.0323\n",
            "Train: epoch 0070 / 0100 | batch 0018 / 0050 | loss 0.0324\n",
            "Train: epoch 0070 / 0100 | batch 0019 / 0050 | loss 0.0329\n",
            "Train: epoch 0070 / 0100 | batch 0020 / 0050 | loss 0.0323\n",
            "Train: epoch 0070 / 0100 | batch 0021 / 0050 | loss 0.0320\n",
            "Train: epoch 0070 / 0100 | batch 0022 / 0050 | loss 0.0319\n",
            "Train: epoch 0070 / 0100 | batch 0023 / 0050 | loss 0.0315\n",
            "Train: epoch 0070 / 0100 | batch 0024 / 0050 | loss 0.0313\n",
            "Train: epoch 0070 / 0100 | batch 0025 / 0050 | loss 0.0310\n",
            "Train: epoch 0070 / 0100 | batch 0026 / 0050 | loss 0.0314\n",
            "Train: epoch 0070 / 0100 | batch 0027 / 0050 | loss 0.0320\n",
            "Train: epoch 0070 / 0100 | batch 0028 / 0050 | loss 0.0317\n",
            "Train: epoch 0070 / 0100 | batch 0029 / 0050 | loss 0.0320\n",
            "Train: epoch 0070 / 0100 | batch 0030 / 0050 | loss 0.0319\n",
            "Train: epoch 0070 / 0100 | batch 0031 / 0050 | loss 0.0321\n",
            "Train: epoch 0070 / 0100 | batch 0032 / 0050 | loss 0.0318\n",
            "Train: epoch 0070 / 0100 | batch 0033 / 0050 | loss 0.0316\n",
            "Train: epoch 0070 / 0100 | batch 0034 / 0050 | loss 0.0321\n",
            "Train: epoch 0070 / 0100 | batch 0035 / 0050 | loss 0.0321\n",
            "Train: epoch 0070 / 0100 | batch 0036 / 0050 | loss 0.0320\n",
            "Train: epoch 0070 / 0100 | batch 0037 / 0050 | loss 0.0319\n",
            "Train: epoch 0070 / 0100 | batch 0038 / 0050 | loss 0.0317\n",
            "Train: epoch 0070 / 0100 | batch 0039 / 0050 | loss 0.0316\n",
            "Train: epoch 0070 / 0100 | batch 0040 / 0050 | loss 0.0314\n",
            "Train: epoch 0070 / 0100 | batch 0041 / 0050 | loss 0.0312\n",
            "Train: epoch 0070 / 0100 | batch 0042 / 0050 | loss 0.0309\n",
            "Train: epoch 0070 / 0100 | batch 0043 / 0050 | loss 0.0308\n",
            "Train: epoch 0070 / 0100 | batch 0044 / 0050 | loss 0.0309\n",
            "Train: epoch 0070 / 0100 | batch 0045 / 0050 | loss 0.0308\n",
            "Train: epoch 0070 / 0100 | batch 0046 / 0050 | loss 0.0308\n",
            "Train: epoch 0070 / 0100 | batch 0047 / 0050 | loss 0.0309\n",
            "Train: epoch 0070 / 0100 | batch 0048 / 0050 | loss 0.0308\n",
            "Train: epoch 0070 / 0100 | batch 0049 / 0050 | loss 0.0310\n",
            "Val loss 0.0332\n",
            "Dice score : 0.10561738163232803\n",
            "Val loss 0.0393\n",
            "Dice score : 0.12913955748081207\n",
            "Val loss 0.0359\n",
            "Dice score : 0.07739070802927017\n",
            "Val loss 0.0431\n",
            "Dice score : 0.10383004695177078\n",
            "Val loss 0.0393\n",
            "Dice score : 0.10970417410135269\n",
            "Val loss 0.0369\n",
            "Dice score : 0.056652817875146866\n",
            "Val loss 0.0357\n",
            "Dice score : 0.07558000832796097\n",
            "Val loss 0.0347\n",
            "Dice score : 0.05691789090633392\n",
            "Val loss 0.0389\n",
            "Dice score : 0.14483633637428284\n",
            "Val loss 0.0374\n",
            "Dice score : 0.0963808000087738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 70/100 [1:25:03<36:32, 73.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0071 / 0100 | batch 0000 / 0050 | loss 0.0192\n",
            "Train: epoch 0071 / 0100 | batch 0001 / 0050 | loss 0.0209\n",
            "Train: epoch 0071 / 0100 | batch 0002 / 0050 | loss 0.0273\n",
            "Train: epoch 0071 / 0100 | batch 0003 / 0050 | loss 0.0267\n",
            "Train: epoch 0071 / 0100 | batch 0004 / 0050 | loss 0.0270\n",
            "Train: epoch 0071 / 0100 | batch 0005 / 0050 | loss 0.0280\n",
            "Train: epoch 0071 / 0100 | batch 0006 / 0050 | loss 0.0299\n",
            "Train: epoch 0071 / 0100 | batch 0007 / 0050 | loss 0.0326\n",
            "Train: epoch 0071 / 0100 | batch 0008 / 0050 | loss 0.0329\n",
            "Train: epoch 0071 / 0100 | batch 0009 / 0050 | loss 0.0324\n",
            "Train: epoch 0071 / 0100 | batch 0010 / 0050 | loss 0.0314\n",
            "Train: epoch 0071 / 0100 | batch 0011 / 0050 | loss 0.0312\n",
            "Train: epoch 0071 / 0100 | batch 0012 / 0050 | loss 0.0308\n",
            "Train: epoch 0071 / 0100 | batch 0013 / 0050 | loss 0.0306\n",
            "Train: epoch 0071 / 0100 | batch 0014 / 0050 | loss 0.0304\n",
            "Train: epoch 0071 / 0100 | batch 0015 / 0050 | loss 0.0299\n",
            "Train: epoch 0071 / 0100 | batch 0016 / 0050 | loss 0.0295\n",
            "Train: epoch 0071 / 0100 | batch 0017 / 0050 | loss 0.0292\n",
            "Train: epoch 0071 / 0100 | batch 0018 / 0050 | loss 0.0299\n",
            "Train: epoch 0071 / 0100 | batch 0019 / 0050 | loss 0.0299\n",
            "Train: epoch 0071 / 0100 | batch 0020 / 0050 | loss 0.0293\n",
            "Train: epoch 0071 / 0100 | batch 0021 / 0050 | loss 0.0300\n",
            "Train: epoch 0071 / 0100 | batch 0022 / 0050 | loss 0.0297\n",
            "Train: epoch 0071 / 0100 | batch 0023 / 0050 | loss 0.0298\n",
            "Train: epoch 0071 / 0100 | batch 0024 / 0050 | loss 0.0296\n",
            "Train: epoch 0071 / 0100 | batch 0025 / 0050 | loss 0.0301\n",
            "Train: epoch 0071 / 0100 | batch 0026 / 0050 | loss 0.0301\n",
            "Train: epoch 0071 / 0100 | batch 0027 / 0050 | loss 0.0303\n",
            "Train: epoch 0071 / 0100 | batch 0028 / 0050 | loss 0.0302\n",
            "Train: epoch 0071 / 0100 | batch 0029 / 0050 | loss 0.0306\n",
            "Train: epoch 0071 / 0100 | batch 0030 / 0050 | loss 0.0304\n",
            "Train: epoch 0071 / 0100 | batch 0031 / 0050 | loss 0.0302\n",
            "Train: epoch 0071 / 0100 | batch 0032 / 0050 | loss 0.0299\n",
            "Train: epoch 0071 / 0100 | batch 0033 / 0050 | loss 0.0298\n",
            "Train: epoch 0071 / 0100 | batch 0034 / 0050 | loss 0.0295\n",
            "Train: epoch 0071 / 0100 | batch 0035 / 0050 | loss 0.0295\n",
            "Train: epoch 0071 / 0100 | batch 0036 / 0050 | loss 0.0293\n",
            "Train: epoch 0071 / 0100 | batch 0037 / 0050 | loss 0.0295\n",
            "Train: epoch 0071 / 0100 | batch 0038 / 0050 | loss 0.0301\n",
            "Train: epoch 0071 / 0100 | batch 0039 / 0050 | loss 0.0304\n",
            "Train: epoch 0071 / 0100 | batch 0040 / 0050 | loss 0.0301\n",
            "Train: epoch 0071 / 0100 | batch 0041 / 0050 | loss 0.0303\n",
            "Train: epoch 0071 / 0100 | batch 0042 / 0050 | loss 0.0303\n",
            "Train: epoch 0071 / 0100 | batch 0043 / 0050 | loss 0.0302\n",
            "Train: epoch 0071 / 0100 | batch 0044 / 0050 | loss 0.0302\n",
            "Train: epoch 0071 / 0100 | batch 0045 / 0050 | loss 0.0300\n",
            "Train: epoch 0071 / 0100 | batch 0046 / 0050 | loss 0.0299\n",
            "Train: epoch 0071 / 0100 | batch 0047 / 0050 | loss 0.0302\n",
            "Train: epoch 0071 / 0100 | batch 0048 / 0050 | loss 0.0304\n",
            "Train: epoch 0071 / 0100 | batch 0049 / 0050 | loss 0.0302\n",
            "Val loss 0.0476\n",
            "Dice score : 0.09722205996513367\n",
            "Val loss 0.0512\n",
            "Dice score : 0.067790187895298\n",
            "Val loss 0.0473\n",
            "Dice score : 0.06854957342147827\n",
            "Val loss 0.0407\n",
            "Dice score : 0.10203604400157928\n",
            "Val loss 0.0382\n",
            "Dice score : 0.10825168341398239\n",
            "Val loss 0.0365\n",
            "Dice score : 0.07029611617326736\n",
            "Val loss 0.0378\n",
            "Dice score : 0.08402225375175476\n",
            "Val loss 0.0368\n",
            "Dice score : 0.06287778913974762\n",
            "Val loss 0.0376\n",
            "Dice score : 0.0854046419262886\n",
            "Val loss 0.0368\n",
            "Dice score : 0.10990070551633835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 71/100 [1:26:16<35:19, 73.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0072 / 0100 | batch 0000 / 0050 | loss 0.0292\n",
            "Train: epoch 0072 / 0100 | batch 0001 / 0050 | loss 0.0325\n",
            "Train: epoch 0072 / 0100 | batch 0002 / 0050 | loss 0.0331\n",
            "Train: epoch 0072 / 0100 | batch 0003 / 0050 | loss 0.0312\n",
            "Train: epoch 0072 / 0100 | batch 0004 / 0050 | loss 0.0293\n",
            "Train: epoch 0072 / 0100 | batch 0005 / 0050 | loss 0.0302\n",
            "Train: epoch 0072 / 0100 | batch 0006 / 0050 | loss 0.0314\n",
            "Train: epoch 0072 / 0100 | batch 0007 / 0050 | loss 0.0303\n",
            "Train: epoch 0072 / 0100 | batch 0008 / 0050 | loss 0.0313\n",
            "Train: epoch 0072 / 0100 | batch 0009 / 0050 | loss 0.0301\n",
            "Train: epoch 0072 / 0100 | batch 0010 / 0050 | loss 0.0308\n",
            "Train: epoch 0072 / 0100 | batch 0011 / 0050 | loss 0.0298\n",
            "Train: epoch 0072 / 0100 | batch 0012 / 0050 | loss 0.0306\n",
            "Train: epoch 0072 / 0100 | batch 0013 / 0050 | loss 0.0300\n",
            "Train: epoch 0072 / 0100 | batch 0014 / 0050 | loss 0.0293\n",
            "Train: epoch 0072 / 0100 | batch 0015 / 0050 | loss 0.0287\n",
            "Train: epoch 0072 / 0100 | batch 0016 / 0050 | loss 0.0283\n",
            "Train: epoch 0072 / 0100 | batch 0017 / 0050 | loss 0.0277\n",
            "Train: epoch 0072 / 0100 | batch 0018 / 0050 | loss 0.0281\n",
            "Train: epoch 0072 / 0100 | batch 0019 / 0050 | loss 0.0282\n",
            "Train: epoch 0072 / 0100 | batch 0020 / 0050 | loss 0.0293\n",
            "Train: epoch 0072 / 0100 | batch 0021 / 0050 | loss 0.0290\n",
            "Train: epoch 0072 / 0100 | batch 0022 / 0050 | loss 0.0287\n",
            "Train: epoch 0072 / 0100 | batch 0023 / 0050 | loss 0.0286\n",
            "Train: epoch 0072 / 0100 | batch 0024 / 0050 | loss 0.0286\n",
            "Train: epoch 0072 / 0100 | batch 0025 / 0050 | loss 0.0288\n",
            "Train: epoch 0072 / 0100 | batch 0026 / 0050 | loss 0.0288\n",
            "Train: epoch 0072 / 0100 | batch 0027 / 0050 | loss 0.0290\n",
            "Train: epoch 0072 / 0100 | batch 0028 / 0050 | loss 0.0287\n",
            "Train: epoch 0072 / 0100 | batch 0029 / 0050 | loss 0.0288\n",
            "Train: epoch 0072 / 0100 | batch 0030 / 0050 | loss 0.0292\n",
            "Train: epoch 0072 / 0100 | batch 0031 / 0050 | loss 0.0291\n",
            "Train: epoch 0072 / 0100 | batch 0032 / 0050 | loss 0.0298\n",
            "Train: epoch 0072 / 0100 | batch 0033 / 0050 | loss 0.0297\n",
            "Train: epoch 0072 / 0100 | batch 0034 / 0050 | loss 0.0298\n",
            "Train: epoch 0072 / 0100 | batch 0035 / 0050 | loss 0.0298\n",
            "Train: epoch 0072 / 0100 | batch 0036 / 0050 | loss 0.0299\n",
            "Train: epoch 0072 / 0100 | batch 0037 / 0050 | loss 0.0299\n",
            "Train: epoch 0072 / 0100 | batch 0038 / 0050 | loss 0.0299\n",
            "Train: epoch 0072 / 0100 | batch 0039 / 0050 | loss 0.0300\n",
            "Train: epoch 0072 / 0100 | batch 0040 / 0050 | loss 0.0301\n",
            "Train: epoch 0072 / 0100 | batch 0041 / 0050 | loss 0.0303\n",
            "Train: epoch 0072 / 0100 | batch 0042 / 0050 | loss 0.0302\n",
            "Train: epoch 0072 / 0100 | batch 0043 / 0050 | loss 0.0300\n",
            "Train: epoch 0072 / 0100 | batch 0044 / 0050 | loss 0.0297\n",
            "Train: epoch 0072 / 0100 | batch 0045 / 0050 | loss 0.0299\n",
            "Train: epoch 0072 / 0100 | batch 0046 / 0050 | loss 0.0298\n",
            "Train: epoch 0072 / 0100 | batch 0047 / 0050 | loss 0.0297\n",
            "Train: epoch 0072 / 0100 | batch 0048 / 0050 | loss 0.0298\n",
            "Train: epoch 0072 / 0100 | batch 0049 / 0050 | loss 0.0299\n",
            "Val loss 0.0279\n",
            "Dice score : 0.06867417693138123\n",
            "Val loss 0.0456\n",
            "Dice score : 0.11323443800210953\n",
            "Val loss 0.0394\n",
            "Dice score : 0.11633507162332535\n",
            "Val loss 0.0385\n",
            "Dice score : 0.05072754994034767\n",
            "Val loss 0.0390\n",
            "Dice score : 0.09921839088201523\n",
            "Val loss 0.0363\n",
            "Dice score : 0.08479198813438416\n",
            "Val loss 0.0390\n",
            "Dice score : 0.09835778176784515\n",
            "Val loss 0.0414\n",
            "Dice score : 0.059895917773246765\n",
            "Val loss 0.0410\n",
            "Dice score : 0.08425475656986237\n",
            "Val loss 0.0392\n",
            "Dice score : 0.046697188168764114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 72/100 [1:27:28<33:56, 72.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0073 / 0100 | batch 0000 / 0050 | loss 0.0285\n",
            "Train: epoch 0073 / 0100 | batch 0001 / 0050 | loss 0.0266\n",
            "Train: epoch 0073 / 0100 | batch 0002 / 0050 | loss 0.0276\n",
            "Train: epoch 0073 / 0100 | batch 0003 / 0050 | loss 0.0262\n",
            "Train: epoch 0073 / 0100 | batch 0004 / 0050 | loss 0.0258\n",
            "Train: epoch 0073 / 0100 | batch 0005 / 0050 | loss 0.0263\n",
            "Train: epoch 0073 / 0100 | batch 0006 / 0050 | loss 0.0263\n",
            "Train: epoch 0073 / 0100 | batch 0007 / 0050 | loss 0.0277\n",
            "Train: epoch 0073 / 0100 | batch 0008 / 0050 | loss 0.0281\n",
            "Train: epoch 0073 / 0100 | batch 0009 / 0050 | loss 0.0273\n",
            "Train: epoch 0073 / 0100 | batch 0010 / 0050 | loss 0.0267\n",
            "Train: epoch 0073 / 0100 | batch 0011 / 0050 | loss 0.0265\n",
            "Train: epoch 0073 / 0100 | batch 0012 / 0050 | loss 0.0260\n",
            "Train: epoch 0073 / 0100 | batch 0013 / 0050 | loss 0.0269\n",
            "Train: epoch 0073 / 0100 | batch 0014 / 0050 | loss 0.0264\n",
            "Train: epoch 0073 / 0100 | batch 0015 / 0050 | loss 0.0265\n",
            "Train: epoch 0073 / 0100 | batch 0016 / 0050 | loss 0.0265\n",
            "Train: epoch 0073 / 0100 | batch 0017 / 0050 | loss 0.0261\n",
            "Train: epoch 0073 / 0100 | batch 0018 / 0050 | loss 0.0262\n",
            "Train: epoch 0073 / 0100 | batch 0019 / 0050 | loss 0.0261\n",
            "Train: epoch 0073 / 0100 | batch 0020 / 0050 | loss 0.0263\n",
            "Train: epoch 0073 / 0100 | batch 0021 / 0050 | loss 0.0263\n",
            "Train: epoch 0073 / 0100 | batch 0022 / 0050 | loss 0.0277\n",
            "Train: epoch 0073 / 0100 | batch 0023 / 0050 | loss 0.0279\n",
            "Train: epoch 0073 / 0100 | batch 0024 / 0050 | loss 0.0278\n",
            "Train: epoch 0073 / 0100 | batch 0025 / 0050 | loss 0.0280\n",
            "Train: epoch 0073 / 0100 | batch 0026 / 0050 | loss 0.0283\n",
            "Train: epoch 0073 / 0100 | batch 0027 / 0050 | loss 0.0281\n",
            "Train: epoch 0073 / 0100 | batch 0028 / 0050 | loss 0.0283\n",
            "Train: epoch 0073 / 0100 | batch 0029 / 0050 | loss 0.0282\n",
            "Train: epoch 0073 / 0100 | batch 0030 / 0050 | loss 0.0281\n",
            "Train: epoch 0073 / 0100 | batch 0031 / 0050 | loss 0.0282\n",
            "Train: epoch 0073 / 0100 | batch 0032 / 0050 | loss 0.0283\n",
            "Train: epoch 0073 / 0100 | batch 0033 / 0050 | loss 0.0285\n",
            "Train: epoch 0073 / 0100 | batch 0034 / 0050 | loss 0.0287\n",
            "Train: epoch 0073 / 0100 | batch 0035 / 0050 | loss 0.0290\n",
            "Train: epoch 0073 / 0100 | batch 0036 / 0050 | loss 0.0290\n",
            "Train: epoch 0073 / 0100 | batch 0037 / 0050 | loss 0.0287\n",
            "Train: epoch 0073 / 0100 | batch 0038 / 0050 | loss 0.0293\n",
            "Train: epoch 0073 / 0100 | batch 0039 / 0050 | loss 0.0293\n",
            "Train: epoch 0073 / 0100 | batch 0040 / 0050 | loss 0.0292\n",
            "Train: epoch 0073 / 0100 | batch 0041 / 0050 | loss 0.0294\n",
            "Train: epoch 0073 / 0100 | batch 0042 / 0050 | loss 0.0294\n",
            "Train: epoch 0073 / 0100 | batch 0043 / 0050 | loss 0.0296\n",
            "Train: epoch 0073 / 0100 | batch 0044 / 0050 | loss 0.0298\n",
            "Train: epoch 0073 / 0100 | batch 0045 / 0050 | loss 0.0299\n",
            "Train: epoch 0073 / 0100 | batch 0046 / 0050 | loss 0.0299\n",
            "Train: epoch 0073 / 0100 | batch 0047 / 0050 | loss 0.0298\n",
            "Train: epoch 0073 / 0100 | batch 0048 / 0050 | loss 0.0299\n",
            "Train: epoch 0073 / 0100 | batch 0049 / 0050 | loss 0.0302\n",
            "Val loss 0.0277\n",
            "Dice score : 0.058446258306503296\n",
            "Val loss 0.0361\n",
            "Dice score : 0.07277680933475494\n",
            "Val loss 0.0390\n",
            "Dice score : 0.06893670558929443\n",
            "Val loss 0.0368\n",
            "Dice score : 0.05992601439356804\n",
            "Val loss 0.0352\n",
            "Dice score : 0.07782233506441116\n",
            "Val loss 0.0422\n",
            "Dice score : 0.03909452632069588\n",
            "Val loss 0.0403\n",
            "Dice score : 0.06050378084182739\n",
            "Val loss 0.0441\n",
            "Dice score : 0.10009820759296417\n",
            "Val loss 0.0442\n",
            "Dice score : 0.13453249633312225\n",
            "Val loss 0.0451\n",
            "Dice score : 0.08030670136213303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 73/100 [1:28:41<32:47, 72.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0074 / 0100 | batch 0000 / 0050 | loss 0.0391\n",
            "Train: epoch 0074 / 0100 | batch 0001 / 0050 | loss 0.0338\n",
            "Train: epoch 0074 / 0100 | batch 0002 / 0050 | loss 0.0305\n",
            "Train: epoch 0074 / 0100 | batch 0003 / 0050 | loss 0.0356\n",
            "Train: epoch 0074 / 0100 | batch 0004 / 0050 | loss 0.0394\n",
            "Train: epoch 0074 / 0100 | batch 0005 / 0050 | loss 0.0398\n",
            "Train: epoch 0074 / 0100 | batch 0006 / 0050 | loss 0.0375\n",
            "Train: epoch 0074 / 0100 | batch 0007 / 0050 | loss 0.0358\n",
            "Train: epoch 0074 / 0100 | batch 0008 / 0050 | loss 0.0342\n",
            "Train: epoch 0074 / 0100 | batch 0009 / 0050 | loss 0.0345\n",
            "Train: epoch 0074 / 0100 | batch 0010 / 0050 | loss 0.0339\n",
            "Train: epoch 0074 / 0100 | batch 0011 / 0050 | loss 0.0326\n",
            "Train: epoch 0074 / 0100 | batch 0012 / 0050 | loss 0.0320\n",
            "Train: epoch 0074 / 0100 | batch 0013 / 0050 | loss 0.0313\n",
            "Train: epoch 0074 / 0100 | batch 0014 / 0050 | loss 0.0304\n",
            "Train: epoch 0074 / 0100 | batch 0015 / 0050 | loss 0.0301\n",
            "Train: epoch 0074 / 0100 | batch 0016 / 0050 | loss 0.0297\n",
            "Train: epoch 0074 / 0100 | batch 0017 / 0050 | loss 0.0298\n",
            "Train: epoch 0074 / 0100 | batch 0018 / 0050 | loss 0.0294\n",
            "Train: epoch 0074 / 0100 | batch 0019 / 0050 | loss 0.0293\n",
            "Train: epoch 0074 / 0100 | batch 0020 / 0050 | loss 0.0296\n",
            "Train: epoch 0074 / 0100 | batch 0021 / 0050 | loss 0.0291\n",
            "Train: epoch 0074 / 0100 | batch 0022 / 0050 | loss 0.0289\n",
            "Train: epoch 0074 / 0100 | batch 0023 / 0050 | loss 0.0287\n",
            "Train: epoch 0074 / 0100 | batch 0024 / 0050 | loss 0.0289\n",
            "Train: epoch 0074 / 0100 | batch 0025 / 0050 | loss 0.0290\n",
            "Train: epoch 0074 / 0100 | batch 0026 / 0050 | loss 0.0290\n",
            "Train: epoch 0074 / 0100 | batch 0027 / 0050 | loss 0.0299\n",
            "Train: epoch 0074 / 0100 | batch 0028 / 0050 | loss 0.0302\n",
            "Train: epoch 0074 / 0100 | batch 0029 / 0050 | loss 0.0302\n",
            "Train: epoch 0074 / 0100 | batch 0030 / 0050 | loss 0.0302\n",
            "Train: epoch 0074 / 0100 | batch 0031 / 0050 | loss 0.0299\n",
            "Train: epoch 0074 / 0100 | batch 0032 / 0050 | loss 0.0298\n",
            "Train: epoch 0074 / 0100 | batch 0033 / 0050 | loss 0.0302\n",
            "Train: epoch 0074 / 0100 | batch 0034 / 0050 | loss 0.0299\n",
            "Train: epoch 0074 / 0100 | batch 0035 / 0050 | loss 0.0298\n",
            "Train: epoch 0074 / 0100 | batch 0036 / 0050 | loss 0.0296\n",
            "Train: epoch 0074 / 0100 | batch 0037 / 0050 | loss 0.0296\n",
            "Train: epoch 0074 / 0100 | batch 0038 / 0050 | loss 0.0294\n",
            "Train: epoch 0074 / 0100 | batch 0039 / 0050 | loss 0.0301\n",
            "Train: epoch 0074 / 0100 | batch 0040 / 0050 | loss 0.0301\n",
            "Train: epoch 0074 / 0100 | batch 0041 / 0050 | loss 0.0302\n",
            "Train: epoch 0074 / 0100 | batch 0042 / 0050 | loss 0.0302\n",
            "Train: epoch 0074 / 0100 | batch 0043 / 0050 | loss 0.0299\n",
            "Train: epoch 0074 / 0100 | batch 0044 / 0050 | loss 0.0299\n",
            "Train: epoch 0074 / 0100 | batch 0045 / 0050 | loss 0.0302\n",
            "Train: epoch 0074 / 0100 | batch 0046 / 0050 | loss 0.0302\n",
            "Train: epoch 0074 / 0100 | batch 0047 / 0050 | loss 0.0300\n",
            "Train: epoch 0074 / 0100 | batch 0048 / 0050 | loss 0.0299\n",
            "Train: epoch 0074 / 0100 | batch 0049 / 0050 | loss 0.0298\n",
            "Val loss 0.0342\n",
            "Dice score : 0.05631546303629875\n",
            "Val loss 0.0376\n",
            "Dice score : 0.06753817945718765\n",
            "Val loss 0.0434\n",
            "Dice score : 0.10667359828948975\n",
            "Val loss 0.0389\n",
            "Dice score : 0.0854223370552063\n",
            "Val loss 0.0352\n",
            "Dice score : 0.07071219384670258\n",
            "Val loss 0.0330\n",
            "Dice score : 0.07469365000724792\n",
            "Val loss 0.0327\n",
            "Dice score : 0.07746610790491104\n",
            "Val loss 0.0331\n",
            "Dice score : 0.11074453592300415\n",
            "Val loss 0.0322\n",
            "Dice score : 0.08085255324840546\n",
            "Val loss 0.0354\n",
            "Dice score : 0.11409853398799896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 74/100 [1:29:54<31:31, 72.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0075 / 0100 | batch 0000 / 0050 | loss 0.0257\n",
            "Train: epoch 0075 / 0100 | batch 0001 / 0050 | loss 0.0265\n",
            "Train: epoch 0075 / 0100 | batch 0002 / 0050 | loss 0.0236\n",
            "Train: epoch 0075 / 0100 | batch 0003 / 0050 | loss 0.0280\n",
            "Train: epoch 0075 / 0100 | batch 0004 / 0050 | loss 0.0265\n",
            "Train: epoch 0075 / 0100 | batch 0005 / 0050 | loss 0.0286\n",
            "Train: epoch 0075 / 0100 | batch 0006 / 0050 | loss 0.0311\n",
            "Train: epoch 0075 / 0100 | batch 0007 / 0050 | loss 0.0331\n",
            "Train: epoch 0075 / 0100 | batch 0008 / 0050 | loss 0.0316\n",
            "Train: epoch 0075 / 0100 | batch 0009 / 0050 | loss 0.0315\n",
            "Train: epoch 0075 / 0100 | batch 0010 / 0050 | loss 0.0311\n",
            "Train: epoch 0075 / 0100 | batch 0011 / 0050 | loss 0.0303\n",
            "Train: epoch 0075 / 0100 | batch 0012 / 0050 | loss 0.0303\n",
            "Train: epoch 0075 / 0100 | batch 0013 / 0050 | loss 0.0300\n",
            "Train: epoch 0075 / 0100 | batch 0014 / 0050 | loss 0.0291\n",
            "Train: epoch 0075 / 0100 | batch 0015 / 0050 | loss 0.0290\n",
            "Train: epoch 0075 / 0100 | batch 0016 / 0050 | loss 0.0286\n",
            "Train: epoch 0075 / 0100 | batch 0017 / 0050 | loss 0.0290\n",
            "Train: epoch 0075 / 0100 | batch 0018 / 0050 | loss 0.0291\n",
            "Train: epoch 0075 / 0100 | batch 0019 / 0050 | loss 0.0289\n",
            "Train: epoch 0075 / 0100 | batch 0020 / 0050 | loss 0.0291\n",
            "Train: epoch 0075 / 0100 | batch 0021 / 0050 | loss 0.0287\n",
            "Train: epoch 0075 / 0100 | batch 0022 / 0050 | loss 0.0290\n",
            "Train: epoch 0075 / 0100 | batch 0023 / 0050 | loss 0.0296\n",
            "Train: epoch 0075 / 0100 | batch 0024 / 0050 | loss 0.0301\n",
            "Train: epoch 0075 / 0100 | batch 0025 / 0050 | loss 0.0306\n",
            "Train: epoch 0075 / 0100 | batch 0026 / 0050 | loss 0.0308\n",
            "Train: epoch 0075 / 0100 | batch 0027 / 0050 | loss 0.0308\n",
            "Train: epoch 0075 / 0100 | batch 0028 / 0050 | loss 0.0316\n",
            "Train: epoch 0075 / 0100 | batch 0029 / 0050 | loss 0.0314\n",
            "Train: epoch 0075 / 0100 | batch 0030 / 0050 | loss 0.0310\n",
            "Train: epoch 0075 / 0100 | batch 0031 / 0050 | loss 0.0310\n",
            "Train: epoch 0075 / 0100 | batch 0032 / 0050 | loss 0.0308\n",
            "Train: epoch 0075 / 0100 | batch 0033 / 0050 | loss 0.0307\n",
            "Train: epoch 0075 / 0100 | batch 0034 / 0050 | loss 0.0307\n",
            "Train: epoch 0075 / 0100 | batch 0035 / 0050 | loss 0.0304\n",
            "Train: epoch 0075 / 0100 | batch 0036 / 0050 | loss 0.0302\n",
            "Train: epoch 0075 / 0100 | batch 0037 / 0050 | loss 0.0300\n",
            "Train: epoch 0075 / 0100 | batch 0038 / 0050 | loss 0.0298\n",
            "Train: epoch 0075 / 0100 | batch 0039 / 0050 | loss 0.0296\n",
            "Train: epoch 0075 / 0100 | batch 0040 / 0050 | loss 0.0297\n",
            "Train: epoch 0075 / 0100 | batch 0041 / 0050 | loss 0.0299\n",
            "Train: epoch 0075 / 0100 | batch 0042 / 0050 | loss 0.0297\n",
            "Train: epoch 0075 / 0100 | batch 0043 / 0050 | loss 0.0296\n",
            "Train: epoch 0075 / 0100 | batch 0044 / 0050 | loss 0.0296\n",
            "Train: epoch 0075 / 0100 | batch 0045 / 0050 | loss 0.0297\n",
            "Train: epoch 0075 / 0100 | batch 0046 / 0050 | loss 0.0296\n",
            "Train: epoch 0075 / 0100 | batch 0047 / 0050 | loss 0.0295\n",
            "Train: epoch 0075 / 0100 | batch 0048 / 0050 | loss 0.0292\n",
            "Train: epoch 0075 / 0100 | batch 0049 / 0050 | loss 0.0293\n",
            "Val loss 0.0459\n",
            "Dice score : 0.08504114300012589\n",
            "Val loss 0.0466\n",
            "Dice score : 0.11847255378961563\n",
            "Val loss 0.0460\n",
            "Dice score : 0.1230810284614563\n",
            "Val loss 0.0474\n",
            "Dice score : 0.07647018879652023\n",
            "Val loss 0.0442\n",
            "Dice score : 0.09136498719453812\n",
            "Val loss 0.0406\n",
            "Dice score : 0.08332585543394089\n",
            "Val loss 0.0375\n",
            "Dice score : 0.1077200323343277\n",
            "Val loss 0.0388\n",
            "Dice score : 0.08518007397651672\n",
            "Val loss 0.0377\n",
            "Dice score : 0.0500921905040741\n",
            "Val loss 0.0365\n",
            "Dice score : 0.11504514515399933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 75/100 [1:31:09<30:36, 73.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0076 / 0100 | batch 0000 / 0050 | loss 0.0356\n",
            "Train: epoch 0076 / 0100 | batch 0001 / 0050 | loss 0.0329\n",
            "Train: epoch 0076 / 0100 | batch 0002 / 0050 | loss 0.0309\n",
            "Train: epoch 0076 / 0100 | batch 0003 / 0050 | loss 0.0280\n",
            "Train: epoch 0076 / 0100 | batch 0004 / 0050 | loss 0.0270\n",
            "Train: epoch 0076 / 0100 | batch 0005 / 0050 | loss 0.0261\n",
            "Train: epoch 0076 / 0100 | batch 0006 / 0050 | loss 0.0272\n",
            "Train: epoch 0076 / 0100 | batch 0007 / 0050 | loss 0.0299\n",
            "Train: epoch 0076 / 0100 | batch 0008 / 0050 | loss 0.0288\n",
            "Train: epoch 0076 / 0100 | batch 0009 / 0050 | loss 0.0285\n",
            "Train: epoch 0076 / 0100 | batch 0010 / 0050 | loss 0.0280\n",
            "Train: epoch 0076 / 0100 | batch 0011 / 0050 | loss 0.0273\n",
            "Train: epoch 0076 / 0100 | batch 0012 / 0050 | loss 0.0273\n",
            "Train: epoch 0076 / 0100 | batch 0013 / 0050 | loss 0.0273\n",
            "Train: epoch 0076 / 0100 | batch 0014 / 0050 | loss 0.0276\n",
            "Train: epoch 0076 / 0100 | batch 0015 / 0050 | loss 0.0282\n",
            "Train: epoch 0076 / 0100 | batch 0016 / 0050 | loss 0.0296\n",
            "Train: epoch 0076 / 0100 | batch 0017 / 0050 | loss 0.0296\n",
            "Train: epoch 0076 / 0100 | batch 0018 / 0050 | loss 0.0301\n",
            "Train: epoch 0076 / 0100 | batch 0019 / 0050 | loss 0.0304\n",
            "Train: epoch 0076 / 0100 | batch 0020 / 0050 | loss 0.0301\n",
            "Train: epoch 0076 / 0100 | batch 0021 / 0050 | loss 0.0299\n",
            "Train: epoch 0076 / 0100 | batch 0022 / 0050 | loss 0.0301\n",
            "Train: epoch 0076 / 0100 | batch 0023 / 0050 | loss 0.0305\n",
            "Train: epoch 0076 / 0100 | batch 0024 / 0050 | loss 0.0304\n",
            "Train: epoch 0076 / 0100 | batch 0025 / 0050 | loss 0.0301\n",
            "Train: epoch 0076 / 0100 | batch 0026 / 0050 | loss 0.0301\n",
            "Train: epoch 0076 / 0100 | batch 0027 / 0050 | loss 0.0298\n",
            "Train: epoch 0076 / 0100 | batch 0028 / 0050 | loss 0.0295\n",
            "Train: epoch 0076 / 0100 | batch 0029 / 0050 | loss 0.0294\n",
            "Train: epoch 0076 / 0100 | batch 0030 / 0050 | loss 0.0296\n",
            "Train: epoch 0076 / 0100 | batch 0031 / 0050 | loss 0.0296\n",
            "Train: epoch 0076 / 0100 | batch 0032 / 0050 | loss 0.0299\n",
            "Train: epoch 0076 / 0100 | batch 0033 / 0050 | loss 0.0296\n",
            "Train: epoch 0076 / 0100 | batch 0034 / 0050 | loss 0.0295\n",
            "Train: epoch 0076 / 0100 | batch 0035 / 0050 | loss 0.0291\n",
            "Train: epoch 0076 / 0100 | batch 0036 / 0050 | loss 0.0288\n",
            "Train: epoch 0076 / 0100 | batch 0037 / 0050 | loss 0.0288\n",
            "Train: epoch 0076 / 0100 | batch 0038 / 0050 | loss 0.0292\n",
            "Train: epoch 0076 / 0100 | batch 0039 / 0050 | loss 0.0294\n",
            "Train: epoch 0076 / 0100 | batch 0040 / 0050 | loss 0.0293\n",
            "Train: epoch 0076 / 0100 | batch 0041 / 0050 | loss 0.0291\n",
            "Train: epoch 0076 / 0100 | batch 0042 / 0050 | loss 0.0290\n",
            "Train: epoch 0076 / 0100 | batch 0043 / 0050 | loss 0.0288\n",
            "Train: epoch 0076 / 0100 | batch 0044 / 0050 | loss 0.0286\n",
            "Train: epoch 0076 / 0100 | batch 0045 / 0050 | loss 0.0284\n",
            "Train: epoch 0076 / 0100 | batch 0046 / 0050 | loss 0.0284\n",
            "Train: epoch 0076 / 0100 | batch 0047 / 0050 | loss 0.0283\n",
            "Train: epoch 0076 / 0100 | batch 0048 / 0050 | loss 0.0283\n",
            "Train: epoch 0076 / 0100 | batch 0049 / 0050 | loss 0.0282\n",
            "Val loss 0.0489\n",
            "Dice score : 0.11720150709152222\n",
            "Val loss 0.0434\n",
            "Dice score : 0.0731973871588707\n",
            "Val loss 0.0378\n",
            "Dice score : 0.07485432177782059\n",
            "Val loss 0.0356\n",
            "Dice score : 0.10280073434114456\n",
            "Val loss 0.0346\n",
            "Dice score : 0.0728752464056015\n",
            "Val loss 0.0325\n",
            "Dice score : 0.1208077147603035\n",
            "Val loss 0.0353\n",
            "Dice score : 0.12186609208583832\n",
            "Val loss 0.0365\n",
            "Dice score : 0.08091428875923157\n",
            "Val loss 0.0359\n",
            "Dice score : 0.08298632502555847\n",
            "Val loss 0.0382\n",
            "Dice score : 0.0643782913684845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 76/100 [1:32:21<29:13, 73.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0077 / 0100 | batch 0000 / 0050 | loss 0.0217\n",
            "Train: epoch 0077 / 0100 | batch 0001 / 0050 | loss 0.0203\n",
            "Train: epoch 0077 / 0100 | batch 0002 / 0050 | loss 0.0221\n",
            "Train: epoch 0077 / 0100 | batch 0003 / 0050 | loss 0.0221\n",
            "Train: epoch 0077 / 0100 | batch 0004 / 0050 | loss 0.0253\n",
            "Train: epoch 0077 / 0100 | batch 0005 / 0050 | loss 0.0240\n",
            "Train: epoch 0077 / 0100 | batch 0006 / 0050 | loss 0.0235\n",
            "Train: epoch 0077 / 0100 | batch 0007 / 0050 | loss 0.0227\n",
            "Train: epoch 0077 / 0100 | batch 0008 / 0050 | loss 0.0224\n",
            "Train: epoch 0077 / 0100 | batch 0009 / 0050 | loss 0.0223\n",
            "Train: epoch 0077 / 0100 | batch 0010 / 0050 | loss 0.0244\n",
            "Train: epoch 0077 / 0100 | batch 0011 / 0050 | loss 0.0252\n",
            "Train: epoch 0077 / 0100 | batch 0012 / 0050 | loss 0.0269\n",
            "Train: epoch 0077 / 0100 | batch 0013 / 0050 | loss 0.0262\n",
            "Train: epoch 0077 / 0100 | batch 0014 / 0050 | loss 0.0261\n",
            "Train: epoch 0077 / 0100 | batch 0015 / 0050 | loss 0.0257\n",
            "Train: epoch 0077 / 0100 | batch 0016 / 0050 | loss 0.0262\n",
            "Train: epoch 0077 / 0100 | batch 0017 / 0050 | loss 0.0261\n",
            "Train: epoch 0077 / 0100 | batch 0018 / 0050 | loss 0.0267\n",
            "Train: epoch 0077 / 0100 | batch 0019 / 0050 | loss 0.0264\n",
            "Train: epoch 0077 / 0100 | batch 0020 / 0050 | loss 0.0261\n",
            "Train: epoch 0077 / 0100 | batch 0021 / 0050 | loss 0.0266\n",
            "Train: epoch 0077 / 0100 | batch 0022 / 0050 | loss 0.0272\n",
            "Train: epoch 0077 / 0100 | batch 0023 / 0050 | loss 0.0275\n",
            "Train: epoch 0077 / 0100 | batch 0024 / 0050 | loss 0.0275\n",
            "Train: epoch 0077 / 0100 | batch 0025 / 0050 | loss 0.0274\n",
            "Train: epoch 0077 / 0100 | batch 0026 / 0050 | loss 0.0273\n",
            "Train: epoch 0077 / 0100 | batch 0027 / 0050 | loss 0.0274\n",
            "Train: epoch 0077 / 0100 | batch 0028 / 0050 | loss 0.0274\n",
            "Train: epoch 0077 / 0100 | batch 0029 / 0050 | loss 0.0274\n",
            "Train: epoch 0077 / 0100 | batch 0030 / 0050 | loss 0.0276\n",
            "Train: epoch 0077 / 0100 | batch 0031 / 0050 | loss 0.0275\n",
            "Train: epoch 0077 / 0100 | batch 0032 / 0050 | loss 0.0272\n",
            "Train: epoch 0077 / 0100 | batch 0033 / 0050 | loss 0.0272\n",
            "Train: epoch 0077 / 0100 | batch 0034 / 0050 | loss 0.0278\n",
            "Train: epoch 0077 / 0100 | batch 0035 / 0050 | loss 0.0277\n",
            "Train: epoch 0077 / 0100 | batch 0036 / 0050 | loss 0.0276\n",
            "Train: epoch 0077 / 0100 | batch 0037 / 0050 | loss 0.0274\n",
            "Train: epoch 0077 / 0100 | batch 0038 / 0050 | loss 0.0276\n",
            "Train: epoch 0077 / 0100 | batch 0039 / 0050 | loss 0.0278\n",
            "Train: epoch 0077 / 0100 | batch 0040 / 0050 | loss 0.0278\n",
            "Train: epoch 0077 / 0100 | batch 0041 / 0050 | loss 0.0277\n",
            "Train: epoch 0077 / 0100 | batch 0042 / 0050 | loss 0.0277\n",
            "Train: epoch 0077 / 0100 | batch 0043 / 0050 | loss 0.0277\n",
            "Train: epoch 0077 / 0100 | batch 0044 / 0050 | loss 0.0279\n",
            "Train: epoch 0077 / 0100 | batch 0045 / 0050 | loss 0.0280\n",
            "Train: epoch 0077 / 0100 | batch 0046 / 0050 | loss 0.0278\n",
            "Train: epoch 0077 / 0100 | batch 0047 / 0050 | loss 0.0277\n",
            "Train: epoch 0077 / 0100 | batch 0048 / 0050 | loss 0.0277\n",
            "Train: epoch 0077 / 0100 | batch 0049 / 0050 | loss 0.0284\n",
            "Val loss 0.0556\n",
            "Dice score : 0.10626270622015\n",
            "Val loss 0.0381\n",
            "Dice score : 0.04670993983745575\n",
            "Val loss 0.0394\n",
            "Dice score : 0.0835680291056633\n",
            "Val loss 0.0356\n",
            "Dice score : 0.08950609713792801\n",
            "Val loss 0.0373\n",
            "Dice score : 0.08561845123767853\n",
            "Val loss 0.0349\n",
            "Dice score : 0.06479382514953613\n",
            "Val loss 0.0332\n",
            "Dice score : 0.08571602404117584\n",
            "Val loss 0.0349\n",
            "Dice score : 0.10321718454360962\n",
            "Val loss 0.0366\n",
            "Dice score : 0.07405178993940353\n",
            "Val loss 0.0362\n",
            "Dice score : 0.0626293271780014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 77/100 [1:33:35<28:05, 73.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0078 / 0100 | batch 0000 / 0050 | loss 0.0430\n",
            "Train: epoch 0078 / 0100 | batch 0001 / 0050 | loss 0.0387\n",
            "Train: epoch 0078 / 0100 | batch 0002 / 0050 | loss 0.0317\n",
            "Train: epoch 0078 / 0100 | batch 0003 / 0050 | loss 0.0298\n",
            "Train: epoch 0078 / 0100 | batch 0004 / 0050 | loss 0.0283\n",
            "Train: epoch 0078 / 0100 | batch 0005 / 0050 | loss 0.0271\n",
            "Train: epoch 0078 / 0100 | batch 0006 / 0050 | loss 0.0287\n",
            "Train: epoch 0078 / 0100 | batch 0007 / 0050 | loss 0.0285\n",
            "Train: epoch 0078 / 0100 | batch 0008 / 0050 | loss 0.0274\n",
            "Train: epoch 0078 / 0100 | batch 0009 / 0050 | loss 0.0267\n",
            "Train: epoch 0078 / 0100 | batch 0010 / 0050 | loss 0.0273\n",
            "Train: epoch 0078 / 0100 | batch 0011 / 0050 | loss 0.0270\n",
            "Train: epoch 0078 / 0100 | batch 0012 / 0050 | loss 0.0266\n",
            "Train: epoch 0078 / 0100 | batch 0013 / 0050 | loss 0.0276\n",
            "Train: epoch 0078 / 0100 | batch 0014 / 0050 | loss 0.0274\n",
            "Train: epoch 0078 / 0100 | batch 0015 / 0050 | loss 0.0284\n",
            "Train: epoch 0078 / 0100 | batch 0016 / 0050 | loss 0.0283\n",
            "Train: epoch 0078 / 0100 | batch 0017 / 0050 | loss 0.0279\n",
            "Train: epoch 0078 / 0100 | batch 0018 / 0050 | loss 0.0281\n",
            "Train: epoch 0078 / 0100 | batch 0019 / 0050 | loss 0.0284\n",
            "Train: epoch 0078 / 0100 | batch 0020 / 0050 | loss 0.0287\n",
            "Train: epoch 0078 / 0100 | batch 0021 / 0050 | loss 0.0286\n",
            "Train: epoch 0078 / 0100 | batch 0022 / 0050 | loss 0.0288\n",
            "Train: epoch 0078 / 0100 | batch 0023 / 0050 | loss 0.0284\n",
            "Train: epoch 0078 / 0100 | batch 0024 / 0050 | loss 0.0281\n",
            "Train: epoch 0078 / 0100 | batch 0025 / 0050 | loss 0.0279\n",
            "Train: epoch 0078 / 0100 | batch 0026 / 0050 | loss 0.0275\n",
            "Train: epoch 0078 / 0100 | batch 0027 / 0050 | loss 0.0272\n",
            "Train: epoch 0078 / 0100 | batch 0028 / 0050 | loss 0.0277\n",
            "Train: epoch 0078 / 0100 | batch 0029 / 0050 | loss 0.0279\n",
            "Train: epoch 0078 / 0100 | batch 0030 / 0050 | loss 0.0278\n",
            "Train: epoch 0078 / 0100 | batch 0031 / 0050 | loss 0.0276\n",
            "Train: epoch 0078 / 0100 | batch 0032 / 0050 | loss 0.0277\n",
            "Train: epoch 0078 / 0100 | batch 0033 / 0050 | loss 0.0275\n",
            "Train: epoch 0078 / 0100 | batch 0034 / 0050 | loss 0.0275\n",
            "Train: epoch 0078 / 0100 | batch 0035 / 0050 | loss 0.0276\n",
            "Train: epoch 0078 / 0100 | batch 0036 / 0050 | loss 0.0278\n",
            "Train: epoch 0078 / 0100 | batch 0037 / 0050 | loss 0.0277\n",
            "Train: epoch 0078 / 0100 | batch 0038 / 0050 | loss 0.0276\n",
            "Train: epoch 0078 / 0100 | batch 0039 / 0050 | loss 0.0276\n",
            "Train: epoch 0078 / 0100 | batch 0040 / 0050 | loss 0.0279\n",
            "Train: epoch 0078 / 0100 | batch 0041 / 0050 | loss 0.0284\n",
            "Train: epoch 0078 / 0100 | batch 0042 / 0050 | loss 0.0285\n",
            "Train: epoch 0078 / 0100 | batch 0043 / 0050 | loss 0.0284\n",
            "Train: epoch 0078 / 0100 | batch 0044 / 0050 | loss 0.0284\n",
            "Train: epoch 0078 / 0100 | batch 0045 / 0050 | loss 0.0285\n",
            "Train: epoch 0078 / 0100 | batch 0046 / 0050 | loss 0.0283\n",
            "Train: epoch 0078 / 0100 | batch 0047 / 0050 | loss 0.0285\n",
            "Train: epoch 0078 / 0100 | batch 0048 / 0050 | loss 0.0283\n",
            "Train: epoch 0078 / 0100 | batch 0049 / 0050 | loss 0.0283\n",
            "Val loss 0.0705\n",
            "Dice score : 0.12515677511692047\n",
            "Val loss 0.0486\n",
            "Dice score : 0.05235585197806358\n",
            "Val loss 0.0407\n",
            "Dice score : 0.09355351328849792\n",
            "Val loss 0.0356\n",
            "Dice score : 0.09112425893545151\n",
            "Val loss 0.0333\n",
            "Dice score : 0.05792875587940216\n",
            "Val loss 0.0348\n",
            "Dice score : 0.07715896517038345\n",
            "Val loss 0.0342\n",
            "Dice score : 0.08481565862894058\n",
            "Val loss 0.0353\n",
            "Dice score : 0.10549312084913254\n",
            "Val loss 0.0343\n",
            "Dice score : 0.1288626492023468\n",
            "Val loss 0.0357\n",
            "Dice score : 0.0939255952835083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 78/100 [1:34:47<26:45, 72.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0079 / 0100 | batch 0000 / 0050 | loss 0.0208\n",
            "Train: epoch 0079 / 0100 | batch 0001 / 0050 | loss 0.0272\n",
            "Train: epoch 0079 / 0100 | batch 0002 / 0050 | loss 0.0302\n",
            "Train: epoch 0079 / 0100 | batch 0003 / 0050 | loss 0.0295\n",
            "Train: epoch 0079 / 0100 | batch 0004 / 0050 | loss 0.0295\n",
            "Train: epoch 0079 / 0100 | batch 0005 / 0050 | loss 0.0313\n",
            "Train: epoch 0079 / 0100 | batch 0006 / 0050 | loss 0.0308\n",
            "Train: epoch 0079 / 0100 | batch 0007 / 0050 | loss 0.0313\n",
            "Train: epoch 0079 / 0100 | batch 0008 / 0050 | loss 0.0300\n",
            "Train: epoch 0079 / 0100 | batch 0009 / 0050 | loss 0.0304\n",
            "Train: epoch 0079 / 0100 | batch 0010 / 0050 | loss 0.0292\n",
            "Train: epoch 0079 / 0100 | batch 0011 / 0050 | loss 0.0289\n",
            "Train: epoch 0079 / 0100 | batch 0012 / 0050 | loss 0.0280\n",
            "Train: epoch 0079 / 0100 | batch 0013 / 0050 | loss 0.0271\n",
            "Train: epoch 0079 / 0100 | batch 0014 / 0050 | loss 0.0282\n",
            "Train: epoch 0079 / 0100 | batch 0015 / 0050 | loss 0.0281\n",
            "Train: epoch 0079 / 0100 | batch 0016 / 0050 | loss 0.0277\n",
            "Train: epoch 0079 / 0100 | batch 0017 / 0050 | loss 0.0286\n",
            "Train: epoch 0079 / 0100 | batch 0018 / 0050 | loss 0.0282\n",
            "Train: epoch 0079 / 0100 | batch 0019 / 0050 | loss 0.0278\n",
            "Train: epoch 0079 / 0100 | batch 0020 / 0050 | loss 0.0279\n",
            "Train: epoch 0079 / 0100 | batch 0021 / 0050 | loss 0.0282\n",
            "Train: epoch 0079 / 0100 | batch 0022 / 0050 | loss 0.0288\n",
            "Train: epoch 0079 / 0100 | batch 0023 / 0050 | loss 0.0284\n",
            "Train: epoch 0079 / 0100 | batch 0024 / 0050 | loss 0.0282\n",
            "Train: epoch 0079 / 0100 | batch 0025 / 0050 | loss 0.0282\n",
            "Train: epoch 0079 / 0100 | batch 0026 / 0050 | loss 0.0284\n",
            "Train: epoch 0079 / 0100 | batch 0027 / 0050 | loss 0.0283\n",
            "Train: epoch 0079 / 0100 | batch 0028 / 0050 | loss 0.0281\n",
            "Train: epoch 0079 / 0100 | batch 0029 / 0050 | loss 0.0279\n",
            "Train: epoch 0079 / 0100 | batch 0030 / 0050 | loss 0.0276\n",
            "Train: epoch 0079 / 0100 | batch 0031 / 0050 | loss 0.0274\n",
            "Train: epoch 0079 / 0100 | batch 0032 / 0050 | loss 0.0275\n",
            "Train: epoch 0079 / 0100 | batch 0033 / 0050 | loss 0.0276\n",
            "Train: epoch 0079 / 0100 | batch 0034 / 0050 | loss 0.0276\n",
            "Train: epoch 0079 / 0100 | batch 0035 / 0050 | loss 0.0273\n",
            "Train: epoch 0079 / 0100 | batch 0036 / 0050 | loss 0.0274\n",
            "Train: epoch 0079 / 0100 | batch 0037 / 0050 | loss 0.0275\n",
            "Train: epoch 0079 / 0100 | batch 0038 / 0050 | loss 0.0274\n",
            "Train: epoch 0079 / 0100 | batch 0039 / 0050 | loss 0.0273\n",
            "Train: epoch 0079 / 0100 | batch 0040 / 0050 | loss 0.0273\n",
            "Train: epoch 0079 / 0100 | batch 0041 / 0050 | loss 0.0274\n",
            "Train: epoch 0079 / 0100 | batch 0042 / 0050 | loss 0.0274\n",
            "Train: epoch 0079 / 0100 | batch 0043 / 0050 | loss 0.0276\n",
            "Train: epoch 0079 / 0100 | batch 0044 / 0050 | loss 0.0276\n",
            "Train: epoch 0079 / 0100 | batch 0045 / 0050 | loss 0.0277\n",
            "Train: epoch 0079 / 0100 | batch 0046 / 0050 | loss 0.0277\n",
            "Train: epoch 0079 / 0100 | batch 0047 / 0050 | loss 0.0275\n",
            "Train: epoch 0079 / 0100 | batch 0048 / 0050 | loss 0.0274\n",
            "Train: epoch 0079 / 0100 | batch 0049 / 0050 | loss 0.0275\n",
            "Val loss 0.0198\n",
            "Dice score : 0.135767862200737\n",
            "Val loss 0.0207\n",
            "Dice score : 0.10796840488910675\n",
            "Val loss 0.0309\n",
            "Dice score : 0.08359429985284805\n",
            "Val loss 0.0317\n",
            "Dice score : 0.1155015379190445\n",
            "Val loss 0.0327\n",
            "Dice score : 0.0760405883193016\n",
            "Val loss 0.0365\n",
            "Dice score : 0.09907208383083344\n",
            "Val loss 0.0387\n",
            "Dice score : 0.12418198585510254\n",
            "Val loss 0.0367\n",
            "Dice score : 0.08384363353252411\n",
            "Val loss 0.0367\n",
            "Dice score : 0.1295296549797058\n",
            "Val loss 0.0352\n",
            "Dice score : 0.10430872440338135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▉  | 79/100 [1:36:00<25:31, 72.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0080 / 0100 | batch 0000 / 0050 | loss 0.0254\n",
            "Train: epoch 0080 / 0100 | batch 0001 / 0050 | loss 0.0266\n",
            "Train: epoch 0080 / 0100 | batch 0002 / 0050 | loss 0.0261\n",
            "Train: epoch 0080 / 0100 | batch 0003 / 0050 | loss 0.0255\n",
            "Train: epoch 0080 / 0100 | batch 0004 / 0050 | loss 0.0269\n",
            "Train: epoch 0080 / 0100 | batch 0005 / 0050 | loss 0.0255\n",
            "Train: epoch 0080 / 0100 | batch 0006 / 0050 | loss 0.0245\n",
            "Train: epoch 0080 / 0100 | batch 0007 / 0050 | loss 0.0243\n",
            "Train: epoch 0080 / 0100 | batch 0008 / 0050 | loss 0.0241\n",
            "Train: epoch 0080 / 0100 | batch 0009 / 0050 | loss 0.0245\n",
            "Train: epoch 0080 / 0100 | batch 0010 / 0050 | loss 0.0279\n",
            "Train: epoch 0080 / 0100 | batch 0011 / 0050 | loss 0.0272\n",
            "Train: epoch 0080 / 0100 | batch 0012 / 0050 | loss 0.0263\n",
            "Train: epoch 0080 / 0100 | batch 0013 / 0050 | loss 0.0267\n",
            "Train: epoch 0080 / 0100 | batch 0014 / 0050 | loss 0.0262\n",
            "Train: epoch 0080 / 0100 | batch 0015 / 0050 | loss 0.0264\n",
            "Train: epoch 0080 / 0100 | batch 0016 / 0050 | loss 0.0264\n",
            "Train: epoch 0080 / 0100 | batch 0017 / 0050 | loss 0.0263\n",
            "Train: epoch 0080 / 0100 | batch 0018 / 0050 | loss 0.0266\n",
            "Train: epoch 0080 / 0100 | batch 0019 / 0050 | loss 0.0263\n",
            "Train: epoch 0080 / 0100 | batch 0020 / 0050 | loss 0.0265\n",
            "Train: epoch 0080 / 0100 | batch 0021 / 0050 | loss 0.0263\n",
            "Train: epoch 0080 / 0100 | batch 0022 / 0050 | loss 0.0268\n",
            "Train: epoch 0080 / 0100 | batch 0023 / 0050 | loss 0.0265\n",
            "Train: epoch 0080 / 0100 | batch 0024 / 0050 | loss 0.0266\n",
            "Train: epoch 0080 / 0100 | batch 0025 / 0050 | loss 0.0265\n",
            "Train: epoch 0080 / 0100 | batch 0026 / 0050 | loss 0.0263\n",
            "Train: epoch 0080 / 0100 | batch 0027 / 0050 | loss 0.0261\n",
            "Train: epoch 0080 / 0100 | batch 0028 / 0050 | loss 0.0258\n",
            "Train: epoch 0080 / 0100 | batch 0029 / 0050 | loss 0.0257\n",
            "Train: epoch 0080 / 0100 | batch 0030 / 0050 | loss 0.0258\n",
            "Train: epoch 0080 / 0100 | batch 0031 / 0050 | loss 0.0256\n",
            "Train: epoch 0080 / 0100 | batch 0032 / 0050 | loss 0.0255\n",
            "Train: epoch 0080 / 0100 | batch 0033 / 0050 | loss 0.0256\n",
            "Train: epoch 0080 / 0100 | batch 0034 / 0050 | loss 0.0257\n",
            "Train: epoch 0080 / 0100 | batch 0035 / 0050 | loss 0.0261\n",
            "Train: epoch 0080 / 0100 | batch 0036 / 0050 | loss 0.0259\n",
            "Train: epoch 0080 / 0100 | batch 0037 / 0050 | loss 0.0263\n",
            "Train: epoch 0080 / 0100 | batch 0038 / 0050 | loss 0.0262\n",
            "Train: epoch 0080 / 0100 | batch 0039 / 0050 | loss 0.0266\n",
            "Train: epoch 0080 / 0100 | batch 0040 / 0050 | loss 0.0266\n",
            "Train: epoch 0080 / 0100 | batch 0041 / 0050 | loss 0.0264\n",
            "Train: epoch 0080 / 0100 | batch 0042 / 0050 | loss 0.0263\n",
            "Train: epoch 0080 / 0100 | batch 0043 / 0050 | loss 0.0263\n",
            "Train: epoch 0080 / 0100 | batch 0044 / 0050 | loss 0.0262\n",
            "Train: epoch 0080 / 0100 | batch 0045 / 0050 | loss 0.0265\n",
            "Train: epoch 0080 / 0100 | batch 0046 / 0050 | loss 0.0269\n",
            "Train: epoch 0080 / 0100 | batch 0047 / 0050 | loss 0.0270\n",
            "Train: epoch 0080 / 0100 | batch 0048 / 0050 | loss 0.0271\n",
            "Train: epoch 0080 / 0100 | batch 0049 / 0050 | loss 0.0271\n",
            "Val loss 0.0419\n",
            "Dice score : 0.07405980676412582\n",
            "Val loss 0.0433\n",
            "Dice score : 0.06823525577783585\n",
            "Val loss 0.0395\n",
            "Dice score : 0.07053753733634949\n",
            "Val loss 0.0368\n",
            "Dice score : 0.05695958435535431\n",
            "Val loss 0.0390\n",
            "Dice score : 0.06771116703748703\n",
            "Val loss 0.0370\n",
            "Dice score : 0.07667935639619827\n",
            "Val loss 0.0350\n",
            "Dice score : 0.08788251876831055\n",
            "Val loss 0.0363\n",
            "Dice score : 0.09133291244506836\n",
            "Val loss 0.0385\n",
            "Dice score : 0.08646171540021896\n",
            "Val loss 0.0381\n",
            "Dice score : 0.04943380132317543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 80/100 [1:37:13<24:21, 73.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0081 / 0100 | batch 0000 / 0050 | loss 0.0247\n",
            "Train: epoch 0081 / 0100 | batch 0001 / 0050 | loss 0.0254\n",
            "Train: epoch 0081 / 0100 | batch 0002 / 0050 | loss 0.0257\n",
            "Train: epoch 0081 / 0100 | batch 0003 / 0050 | loss 0.0281\n",
            "Train: epoch 0081 / 0100 | batch 0004 / 0050 | loss 0.0300\n",
            "Train: epoch 0081 / 0100 | batch 0005 / 0050 | loss 0.0322\n",
            "Train: epoch 0081 / 0100 | batch 0006 / 0050 | loss 0.0325\n",
            "Train: epoch 0081 / 0100 | batch 0007 / 0050 | loss 0.0317\n",
            "Train: epoch 0081 / 0100 | batch 0008 / 0050 | loss 0.0300\n",
            "Train: epoch 0081 / 0100 | batch 0009 / 0050 | loss 0.0291\n",
            "Train: epoch 0081 / 0100 | batch 0010 / 0050 | loss 0.0282\n",
            "Train: epoch 0081 / 0100 | batch 0011 / 0050 | loss 0.0282\n",
            "Train: epoch 0081 / 0100 | batch 0012 / 0050 | loss 0.0278\n",
            "Train: epoch 0081 / 0100 | batch 0013 / 0050 | loss 0.0272\n",
            "Train: epoch 0081 / 0100 | batch 0014 / 0050 | loss 0.0273\n",
            "Train: epoch 0081 / 0100 | batch 0015 / 0050 | loss 0.0274\n",
            "Train: epoch 0081 / 0100 | batch 0016 / 0050 | loss 0.0278\n",
            "Train: epoch 0081 / 0100 | batch 0017 / 0050 | loss 0.0277\n",
            "Train: epoch 0081 / 0100 | batch 0018 / 0050 | loss 0.0286\n",
            "Train: epoch 0081 / 0100 | batch 0019 / 0050 | loss 0.0283\n",
            "Train: epoch 0081 / 0100 | batch 0020 / 0050 | loss 0.0279\n",
            "Train: epoch 0081 / 0100 | batch 0021 / 0050 | loss 0.0291\n",
            "Train: epoch 0081 / 0100 | batch 0022 / 0050 | loss 0.0287\n",
            "Train: epoch 0081 / 0100 | batch 0023 / 0050 | loss 0.0290\n",
            "Train: epoch 0081 / 0100 | batch 0024 / 0050 | loss 0.0287\n",
            "Train: epoch 0081 / 0100 | batch 0025 / 0050 | loss 0.0284\n",
            "Train: epoch 0081 / 0100 | batch 0026 / 0050 | loss 0.0283\n",
            "Train: epoch 0081 / 0100 | batch 0027 / 0050 | loss 0.0285\n",
            "Train: epoch 0081 / 0100 | batch 0028 / 0050 | loss 0.0281\n",
            "Train: epoch 0081 / 0100 | batch 0029 / 0050 | loss 0.0280\n",
            "Train: epoch 0081 / 0100 | batch 0030 / 0050 | loss 0.0278\n",
            "Train: epoch 0081 / 0100 | batch 0031 / 0050 | loss 0.0283\n",
            "Train: epoch 0081 / 0100 | batch 0032 / 0050 | loss 0.0279\n",
            "Train: epoch 0081 / 0100 | batch 0033 / 0050 | loss 0.0277\n",
            "Train: epoch 0081 / 0100 | batch 0034 / 0050 | loss 0.0278\n",
            "Train: epoch 0081 / 0100 | batch 0035 / 0050 | loss 0.0275\n",
            "Train: epoch 0081 / 0100 | batch 0036 / 0050 | loss 0.0279\n",
            "Train: epoch 0081 / 0100 | batch 0037 / 0050 | loss 0.0281\n",
            "Train: epoch 0081 / 0100 | batch 0038 / 0050 | loss 0.0280\n",
            "Train: epoch 0081 / 0100 | batch 0039 / 0050 | loss 0.0277\n",
            "Train: epoch 0081 / 0100 | batch 0040 / 0050 | loss 0.0278\n",
            "Train: epoch 0081 / 0100 | batch 0041 / 0050 | loss 0.0277\n",
            "Train: epoch 0081 / 0100 | batch 0042 / 0050 | loss 0.0280\n",
            "Train: epoch 0081 / 0100 | batch 0043 / 0050 | loss 0.0282\n",
            "Train: epoch 0081 / 0100 | batch 0044 / 0050 | loss 0.0280\n",
            "Train: epoch 0081 / 0100 | batch 0045 / 0050 | loss 0.0280\n",
            "Train: epoch 0081 / 0100 | batch 0046 / 0050 | loss 0.0279\n",
            "Train: epoch 0081 / 0100 | batch 0047 / 0050 | loss 0.0284\n",
            "Train: epoch 0081 / 0100 | batch 0048 / 0050 | loss 0.0284\n",
            "Train: epoch 0081 / 0100 | batch 0049 / 0050 | loss 0.0282\n",
            "Val loss 0.0380\n",
            "Dice score : 0.11738407611846924\n",
            "Val loss 0.0494\n",
            "Dice score : 0.07714993506669998\n",
            "Val loss 0.0453\n",
            "Dice score : 0.09344897419214249\n",
            "Val loss 0.0477\n",
            "Dice score : 0.1091630682349205\n",
            "Val loss 0.0422\n",
            "Dice score : 0.09520820528268814\n",
            "Val loss 0.0425\n",
            "Dice score : 0.09335041046142578\n",
            "Val loss 0.0393\n",
            "Dice score : 0.12060072273015976\n",
            "Val loss 0.0374\n",
            "Dice score : 0.09163669496774673\n",
            "Val loss 0.0358\n",
            "Dice score : 0.06082434952259064\n",
            "Val loss 0.0344\n",
            "Dice score : 0.07551867514848709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 81/100 [1:38:25<23:02, 72.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0082 / 0100 | batch 0000 / 0050 | loss 0.0342\n",
            "Train: epoch 0082 / 0100 | batch 0001 / 0050 | loss 0.0287\n",
            "Train: epoch 0082 / 0100 | batch 0002 / 0050 | loss 0.0247\n",
            "Train: epoch 0082 / 0100 | batch 0003 / 0050 | loss 0.0260\n",
            "Train: epoch 0082 / 0100 | batch 0004 / 0050 | loss 0.0243\n",
            "Train: epoch 0082 / 0100 | batch 0005 / 0050 | loss 0.0268\n",
            "Train: epoch 0082 / 0100 | batch 0006 / 0050 | loss 0.0264\n",
            "Train: epoch 0082 / 0100 | batch 0007 / 0050 | loss 0.0269\n",
            "Train: epoch 0082 / 0100 | batch 0008 / 0050 | loss 0.0276\n",
            "Train: epoch 0082 / 0100 | batch 0009 / 0050 | loss 0.0269\n",
            "Train: epoch 0082 / 0100 | batch 0010 / 0050 | loss 0.0271\n",
            "Train: epoch 0082 / 0100 | batch 0011 / 0050 | loss 0.0271\n",
            "Train: epoch 0082 / 0100 | batch 0012 / 0050 | loss 0.0269\n",
            "Train: epoch 0082 / 0100 | batch 0013 / 0050 | loss 0.0264\n",
            "Train: epoch 0082 / 0100 | batch 0014 / 0050 | loss 0.0262\n",
            "Train: epoch 0082 / 0100 | batch 0015 / 0050 | loss 0.0261\n",
            "Train: epoch 0082 / 0100 | batch 0016 / 0050 | loss 0.0271\n",
            "Train: epoch 0082 / 0100 | batch 0017 / 0050 | loss 0.0269\n",
            "Train: epoch 0082 / 0100 | batch 0018 / 0050 | loss 0.0271\n",
            "Train: epoch 0082 / 0100 | batch 0019 / 0050 | loss 0.0272\n",
            "Train: epoch 0082 / 0100 | batch 0020 / 0050 | loss 0.0272\n",
            "Train: epoch 0082 / 0100 | batch 0021 / 0050 | loss 0.0273\n",
            "Train: epoch 0082 / 0100 | batch 0022 / 0050 | loss 0.0269\n",
            "Train: epoch 0082 / 0100 | batch 0023 / 0050 | loss 0.0273\n",
            "Train: epoch 0082 / 0100 | batch 0024 / 0050 | loss 0.0269\n",
            "Train: epoch 0082 / 0100 | batch 0025 / 0050 | loss 0.0269\n",
            "Train: epoch 0082 / 0100 | batch 0026 / 0050 | loss 0.0271\n",
            "Train: epoch 0082 / 0100 | batch 0027 / 0050 | loss 0.0269\n",
            "Train: epoch 0082 / 0100 | batch 0028 / 0050 | loss 0.0265\n",
            "Train: epoch 0082 / 0100 | batch 0029 / 0050 | loss 0.0262\n",
            "Train: epoch 0082 / 0100 | batch 0030 / 0050 | loss 0.0260\n",
            "Train: epoch 0082 / 0100 | batch 0031 / 0050 | loss 0.0259\n",
            "Train: epoch 0082 / 0100 | batch 0032 / 0050 | loss 0.0258\n",
            "Train: epoch 0082 / 0100 | batch 0033 / 0050 | loss 0.0262\n",
            "Train: epoch 0082 / 0100 | batch 0034 / 0050 | loss 0.0265\n",
            "Train: epoch 0082 / 0100 | batch 0035 / 0050 | loss 0.0264\n",
            "Train: epoch 0082 / 0100 | batch 0036 / 0050 | loss 0.0262\n",
            "Train: epoch 0082 / 0100 | batch 0037 / 0050 | loss 0.0262\n",
            "Train: epoch 0082 / 0100 | batch 0038 / 0050 | loss 0.0262\n",
            "Train: epoch 0082 / 0100 | batch 0039 / 0050 | loss 0.0262\n",
            "Train: epoch 0082 / 0100 | batch 0040 / 0050 | loss 0.0264\n",
            "Train: epoch 0082 / 0100 | batch 0041 / 0050 | loss 0.0265\n",
            "Train: epoch 0082 / 0100 | batch 0042 / 0050 | loss 0.0264\n",
            "Train: epoch 0082 / 0100 | batch 0043 / 0050 | loss 0.0263\n",
            "Train: epoch 0082 / 0100 | batch 0044 / 0050 | loss 0.0269\n",
            "Train: epoch 0082 / 0100 | batch 0045 / 0050 | loss 0.0269\n",
            "Train: epoch 0082 / 0100 | batch 0046 / 0050 | loss 0.0270\n",
            "Train: epoch 0082 / 0100 | batch 0047 / 0050 | loss 0.0270\n",
            "Train: epoch 0082 / 0100 | batch 0048 / 0050 | loss 0.0268\n",
            "Train: epoch 0082 / 0100 | batch 0049 / 0050 | loss 0.0267\n",
            "Val loss 0.0314\n",
            "Dice score : 0.07200949639081955\n",
            "Val loss 0.0328\n",
            "Dice score : 0.06240740790963173\n",
            "Val loss 0.0300\n",
            "Dice score : 0.08769182115793228\n",
            "Val loss 0.0408\n",
            "Dice score : 0.09407001733779907\n",
            "Val loss 0.0434\n",
            "Dice score : 0.13138000667095184\n",
            "Val loss 0.0401\n",
            "Dice score : 0.07645297795534134\n",
            "Val loss 0.0382\n",
            "Dice score : 0.06538914144039154\n",
            "Val loss 0.0382\n",
            "Dice score : 0.07201982289552689\n",
            "Val loss 0.0363\n",
            "Dice score : 0.10176153481006622\n",
            "Val loss 0.0374\n",
            "Dice score : 0.15875032544136047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 82/100 [1:39:38<21:49, 72.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0083 / 0100 | batch 0000 / 0050 | loss 0.0292\n",
            "Train: epoch 0083 / 0100 | batch 0001 / 0050 | loss 0.0263\n",
            "Train: epoch 0083 / 0100 | batch 0002 / 0050 | loss 0.0351\n",
            "Train: epoch 0083 / 0100 | batch 0003 / 0050 | loss 0.0309\n",
            "Train: epoch 0083 / 0100 | batch 0004 / 0050 | loss 0.0287\n",
            "Train: epoch 0083 / 0100 | batch 0005 / 0050 | loss 0.0278\n",
            "Train: epoch 0083 / 0100 | batch 0006 / 0050 | loss 0.0291\n",
            "Train: epoch 0083 / 0100 | batch 0007 / 0050 | loss 0.0284\n",
            "Train: epoch 0083 / 0100 | batch 0008 / 0050 | loss 0.0270\n",
            "Train: epoch 0083 / 0100 | batch 0009 / 0050 | loss 0.0270\n",
            "Train: epoch 0083 / 0100 | batch 0010 / 0050 | loss 0.0287\n",
            "Train: epoch 0083 / 0100 | batch 0011 / 0050 | loss 0.0278\n",
            "Train: epoch 0083 / 0100 | batch 0012 / 0050 | loss 0.0271\n",
            "Train: epoch 0083 / 0100 | batch 0013 / 0050 | loss 0.0270\n",
            "Train: epoch 0083 / 0100 | batch 0014 / 0050 | loss 0.0270\n",
            "Train: epoch 0083 / 0100 | batch 0015 / 0050 | loss 0.0267\n",
            "Train: epoch 0083 / 0100 | batch 0016 / 0050 | loss 0.0273\n",
            "Train: epoch 0083 / 0100 | batch 0017 / 0050 | loss 0.0270\n",
            "Train: epoch 0083 / 0100 | batch 0018 / 0050 | loss 0.0275\n",
            "Train: epoch 0083 / 0100 | batch 0019 / 0050 | loss 0.0275\n",
            "Train: epoch 0083 / 0100 | batch 0020 / 0050 | loss 0.0277\n",
            "Train: epoch 0083 / 0100 | batch 0021 / 0050 | loss 0.0276\n",
            "Train: epoch 0083 / 0100 | batch 0022 / 0050 | loss 0.0274\n",
            "Train: epoch 0083 / 0100 | batch 0023 / 0050 | loss 0.0273\n",
            "Train: epoch 0083 / 0100 | batch 0024 / 0050 | loss 0.0269\n",
            "Train: epoch 0083 / 0100 | batch 0025 / 0050 | loss 0.0265\n",
            "Train: epoch 0083 / 0100 | batch 0026 / 0050 | loss 0.0266\n",
            "Train: epoch 0083 / 0100 | batch 0027 / 0050 | loss 0.0263\n",
            "Train: epoch 0083 / 0100 | batch 0028 / 0050 | loss 0.0261\n",
            "Train: epoch 0083 / 0100 | batch 0029 / 0050 | loss 0.0262\n",
            "Train: epoch 0083 / 0100 | batch 0030 / 0050 | loss 0.0262\n",
            "Train: epoch 0083 / 0100 | batch 0031 / 0050 | loss 0.0263\n",
            "Train: epoch 0083 / 0100 | batch 0032 / 0050 | loss 0.0262\n",
            "Train: epoch 0083 / 0100 | batch 0033 / 0050 | loss 0.0259\n",
            "Train: epoch 0083 / 0100 | batch 0034 / 0050 | loss 0.0256\n",
            "Train: epoch 0083 / 0100 | batch 0035 / 0050 | loss 0.0254\n",
            "Train: epoch 0083 / 0100 | batch 0036 / 0050 | loss 0.0254\n",
            "Train: epoch 0083 / 0100 | batch 0037 / 0050 | loss 0.0256\n",
            "Train: epoch 0083 / 0100 | batch 0038 / 0050 | loss 0.0258\n",
            "Train: epoch 0083 / 0100 | batch 0039 / 0050 | loss 0.0258\n",
            "Train: epoch 0083 / 0100 | batch 0040 / 0050 | loss 0.0258\n",
            "Train: epoch 0083 / 0100 | batch 0041 / 0050 | loss 0.0258\n",
            "Train: epoch 0083 / 0100 | batch 0042 / 0050 | loss 0.0258\n",
            "Train: epoch 0083 / 0100 | batch 0043 / 0050 | loss 0.0257\n",
            "Train: epoch 0083 / 0100 | batch 0044 / 0050 | loss 0.0255\n",
            "Train: epoch 0083 / 0100 | batch 0045 / 0050 | loss 0.0258\n",
            "Train: epoch 0083 / 0100 | batch 0046 / 0050 | loss 0.0263\n",
            "Train: epoch 0083 / 0100 | batch 0047 / 0050 | loss 0.0261\n",
            "Train: epoch 0083 / 0100 | batch 0048 / 0050 | loss 0.0263\n",
            "Train: epoch 0083 / 0100 | batch 0049 / 0050 | loss 0.0262\n",
            "Val loss 0.0363\n",
            "Dice score : 0.11640062183141708\n",
            "Val loss 0.0297\n",
            "Dice score : 0.10112404078245163\n",
            "Val loss 0.0362\n",
            "Dice score : 0.09531670808792114\n",
            "Val loss 0.0389\n",
            "Dice score : 0.10471101105213165\n",
            "Val loss 0.0398\n",
            "Dice score : 0.04483287036418915\n",
            "Val loss 0.0368\n",
            "Dice score : 0.12227258086204529\n",
            "Val loss 0.0346\n",
            "Dice score : 0.10988529026508331\n",
            "Val loss 0.0332\n",
            "Dice score : 0.06927771121263504\n",
            "Val loss 0.0356\n",
            "Dice score : 0.11725503951311111\n",
            "Val loss 0.0350\n",
            "Dice score : 0.1054660975933075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 83/100 [1:40:52<20:41, 73.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0084 / 0100 | batch 0000 / 0050 | loss 0.0210\n",
            "Train: epoch 0084 / 0100 | batch 0001 / 0050 | loss 0.0228\n",
            "Train: epoch 0084 / 0100 | batch 0002 / 0050 | loss 0.0225\n",
            "Train: epoch 0084 / 0100 | batch 0003 / 0050 | loss 0.0249\n",
            "Train: epoch 0084 / 0100 | batch 0004 / 0050 | loss 0.0266\n",
            "Train: epoch 0084 / 0100 | batch 0005 / 0050 | loss 0.0260\n",
            "Train: epoch 0084 / 0100 | batch 0006 / 0050 | loss 0.0251\n",
            "Train: epoch 0084 / 0100 | batch 0007 / 0050 | loss 0.0250\n",
            "Train: epoch 0084 / 0100 | batch 0008 / 0050 | loss 0.0249\n",
            "Train: epoch 0084 / 0100 | batch 0009 / 0050 | loss 0.0245\n",
            "Train: epoch 0084 / 0100 | batch 0010 / 0050 | loss 0.0247\n",
            "Train: epoch 0084 / 0100 | batch 0011 / 0050 | loss 0.0242\n",
            "Train: epoch 0084 / 0100 | batch 0012 / 0050 | loss 0.0249\n",
            "Train: epoch 0084 / 0100 | batch 0013 / 0050 | loss 0.0250\n",
            "Train: epoch 0084 / 0100 | batch 0014 / 0050 | loss 0.0246\n",
            "Train: epoch 0084 / 0100 | batch 0015 / 0050 | loss 0.0246\n",
            "Train: epoch 0084 / 0100 | batch 0016 / 0050 | loss 0.0244\n",
            "Train: epoch 0084 / 0100 | batch 0017 / 0050 | loss 0.0253\n",
            "Train: epoch 0084 / 0100 | batch 0018 / 0050 | loss 0.0251\n",
            "Train: epoch 0084 / 0100 | batch 0019 / 0050 | loss 0.0249\n",
            "Train: epoch 0084 / 0100 | batch 0020 / 0050 | loss 0.0246\n",
            "Train: epoch 0084 / 0100 | batch 0021 / 0050 | loss 0.0247\n",
            "Train: epoch 0084 / 0100 | batch 0022 / 0050 | loss 0.0250\n",
            "Train: epoch 0084 / 0100 | batch 0023 / 0050 | loss 0.0251\n",
            "Train: epoch 0084 / 0100 | batch 0024 / 0050 | loss 0.0249\n",
            "Train: epoch 0084 / 0100 | batch 0025 / 0050 | loss 0.0248\n",
            "Train: epoch 0084 / 0100 | batch 0026 / 0050 | loss 0.0256\n",
            "Train: epoch 0084 / 0100 | batch 0027 / 0050 | loss 0.0252\n",
            "Train: epoch 0084 / 0100 | batch 0028 / 0050 | loss 0.0250\n",
            "Train: epoch 0084 / 0100 | batch 0029 / 0050 | loss 0.0251\n",
            "Train: epoch 0084 / 0100 | batch 0030 / 0050 | loss 0.0251\n",
            "Train: epoch 0084 / 0100 | batch 0031 / 0050 | loss 0.0252\n",
            "Train: epoch 0084 / 0100 | batch 0032 / 0050 | loss 0.0251\n",
            "Train: epoch 0084 / 0100 | batch 0033 / 0050 | loss 0.0250\n",
            "Train: epoch 0084 / 0100 | batch 0034 / 0050 | loss 0.0252\n",
            "Train: epoch 0084 / 0100 | batch 0035 / 0050 | loss 0.0251\n",
            "Train: epoch 0084 / 0100 | batch 0036 / 0050 | loss 0.0249\n",
            "Train: epoch 0084 / 0100 | batch 0037 / 0050 | loss 0.0249\n",
            "Train: epoch 0084 / 0100 | batch 0038 / 0050 | loss 0.0249\n",
            "Train: epoch 0084 / 0100 | batch 0039 / 0050 | loss 0.0250\n",
            "Train: epoch 0084 / 0100 | batch 0040 / 0050 | loss 0.0248\n",
            "Train: epoch 0084 / 0100 | batch 0041 / 0050 | loss 0.0253\n",
            "Train: epoch 0084 / 0100 | batch 0042 / 0050 | loss 0.0253\n",
            "Train: epoch 0084 / 0100 | batch 0043 / 0050 | loss 0.0253\n",
            "Train: epoch 0084 / 0100 | batch 0044 / 0050 | loss 0.0254\n",
            "Train: epoch 0084 / 0100 | batch 0045 / 0050 | loss 0.0252\n",
            "Train: epoch 0084 / 0100 | batch 0046 / 0050 | loss 0.0254\n",
            "Train: epoch 0084 / 0100 | batch 0047 / 0050 | loss 0.0254\n",
            "Train: epoch 0084 / 0100 | batch 0048 / 0050 | loss 0.0256\n",
            "Train: epoch 0084 / 0100 | batch 0049 / 0050 | loss 0.0257\n",
            "Val loss 0.0337\n",
            "Dice score : 0.08935069292783737\n",
            "Val loss 0.0400\n",
            "Dice score : 0.0921681672334671\n",
            "Val loss 0.0392\n",
            "Dice score : 0.058636248111724854\n",
            "Val loss 0.0355\n",
            "Dice score : 0.07284609228372574\n",
            "Val loss 0.0358\n",
            "Dice score : 0.09253285080194473\n",
            "Val loss 0.0389\n",
            "Dice score : 0.11555738747119904\n",
            "Val loss 0.0370\n",
            "Dice score : 0.06312666088342667\n",
            "Val loss 0.0390\n",
            "Dice score : 0.06250295042991638\n",
            "Val loss 0.0372\n",
            "Dice score : 0.0641806498169899\n",
            "Val loss 0.0359\n",
            "Dice score : 0.09627328813076019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 84/100 [1:42:04<19:28, 73.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0085 / 0100 | batch 0000 / 0050 | loss 0.0200\n",
            "Train: epoch 0085 / 0100 | batch 0001 / 0050 | loss 0.0281\n",
            "Train: epoch 0085 / 0100 | batch 0002 / 0050 | loss 0.0254\n",
            "Train: epoch 0085 / 0100 | batch 0003 / 0050 | loss 0.0261\n",
            "Train: epoch 0085 / 0100 | batch 0004 / 0050 | loss 0.0265\n",
            "Train: epoch 0085 / 0100 | batch 0005 / 0050 | loss 0.0250\n",
            "Train: epoch 0085 / 0100 | batch 0006 / 0050 | loss 0.0254\n",
            "Train: epoch 0085 / 0100 | batch 0007 / 0050 | loss 0.0246\n",
            "Train: epoch 0085 / 0100 | batch 0008 / 0050 | loss 0.0236\n",
            "Train: epoch 0085 / 0100 | batch 0009 / 0050 | loss 0.0249\n",
            "Train: epoch 0085 / 0100 | batch 0010 / 0050 | loss 0.0244\n",
            "Train: epoch 0085 / 0100 | batch 0011 / 0050 | loss 0.0252\n",
            "Train: epoch 0085 / 0100 | batch 0012 / 0050 | loss 0.0260\n",
            "Train: epoch 0085 / 0100 | batch 0013 / 0050 | loss 0.0256\n",
            "Train: epoch 0085 / 0100 | batch 0014 / 0050 | loss 0.0255\n",
            "Train: epoch 0085 / 0100 | batch 0015 / 0050 | loss 0.0256\n",
            "Train: epoch 0085 / 0100 | batch 0016 / 0050 | loss 0.0253\n",
            "Train: epoch 0085 / 0100 | batch 0017 / 0050 | loss 0.0253\n",
            "Train: epoch 0085 / 0100 | batch 0018 / 0050 | loss 0.0255\n",
            "Train: epoch 0085 / 0100 | batch 0019 / 0050 | loss 0.0258\n",
            "Train: epoch 0085 / 0100 | batch 0020 / 0050 | loss 0.0257\n",
            "Train: epoch 0085 / 0100 | batch 0021 / 0050 | loss 0.0258\n",
            "Train: epoch 0085 / 0100 | batch 0022 / 0050 | loss 0.0259\n",
            "Train: epoch 0085 / 0100 | batch 0023 / 0050 | loss 0.0261\n",
            "Train: epoch 0085 / 0100 | batch 0024 / 0050 | loss 0.0259\n",
            "Train: epoch 0085 / 0100 | batch 0025 / 0050 | loss 0.0256\n",
            "Train: epoch 0085 / 0100 | batch 0026 / 0050 | loss 0.0254\n",
            "Train: epoch 0085 / 0100 | batch 0027 / 0050 | loss 0.0252\n",
            "Train: epoch 0085 / 0100 | batch 0028 / 0050 | loss 0.0260\n",
            "Train: epoch 0085 / 0100 | batch 0029 / 0050 | loss 0.0259\n",
            "Train: epoch 0085 / 0100 | batch 0030 / 0050 | loss 0.0256\n",
            "Train: epoch 0085 / 0100 | batch 0031 / 0050 | loss 0.0256\n",
            "Train: epoch 0085 / 0100 | batch 0032 / 0050 | loss 0.0255\n",
            "Train: epoch 0085 / 0100 | batch 0033 / 0050 | loss 0.0252\n",
            "Train: epoch 0085 / 0100 | batch 0034 / 0050 | loss 0.0253\n",
            "Train: epoch 0085 / 0100 | batch 0035 / 0050 | loss 0.0257\n",
            "Train: epoch 0085 / 0100 | batch 0036 / 0050 | loss 0.0255\n",
            "Train: epoch 0085 / 0100 | batch 0037 / 0050 | loss 0.0254\n",
            "Train: epoch 0085 / 0100 | batch 0038 / 0050 | loss 0.0254\n",
            "Train: epoch 0085 / 0100 | batch 0039 / 0050 | loss 0.0256\n",
            "Train: epoch 0085 / 0100 | batch 0040 / 0050 | loss 0.0261\n",
            "Train: epoch 0085 / 0100 | batch 0041 / 0050 | loss 0.0260\n",
            "Train: epoch 0085 / 0100 | batch 0042 / 0050 | loss 0.0261\n",
            "Train: epoch 0085 / 0100 | batch 0043 / 0050 | loss 0.0259\n",
            "Train: epoch 0085 / 0100 | batch 0044 / 0050 | loss 0.0257\n",
            "Train: epoch 0085 / 0100 | batch 0045 / 0050 | loss 0.0255\n",
            "Train: epoch 0085 / 0100 | batch 0046 / 0050 | loss 0.0254\n",
            "Train: epoch 0085 / 0100 | batch 0047 / 0050 | loss 0.0254\n",
            "Train: epoch 0085 / 0100 | batch 0048 / 0050 | loss 0.0256\n",
            "Train: epoch 0085 / 0100 | batch 0049 / 0050 | loss 0.0254\n",
            "Val loss 0.1141\n",
            "Dice score : 0.08605054020881653\n",
            "Val loss 0.0713\n",
            "Dice score : 0.08760808408260345\n",
            "Val loss 0.0559\n",
            "Dice score : 0.0855952724814415\n",
            "Val loss 0.0479\n",
            "Dice score : 0.09734299778938293\n",
            "Val loss 0.0430\n",
            "Dice score : 0.05892706662416458\n",
            "Val loss 0.0453\n",
            "Dice score : 0.1761057823896408\n",
            "Val loss 0.0445\n",
            "Dice score : 0.0951349139213562\n",
            "Val loss 0.0419\n",
            "Dice score : 0.12780629098415375\n",
            "Val loss 0.0438\n",
            "Dice score : 0.10975471138954163\n",
            "Val loss 0.0436\n",
            "Dice score : 0.1279233694076538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 85/100 [1:43:19<18:21, 73.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0086 / 0100 | batch 0000 / 0050 | loss 0.0468\n",
            "Train: epoch 0086 / 0100 | batch 0001 / 0050 | loss 0.0324\n",
            "Train: epoch 0086 / 0100 | batch 0002 / 0050 | loss 0.0316\n",
            "Train: epoch 0086 / 0100 | batch 0003 / 0050 | loss 0.0306\n",
            "Train: epoch 0086 / 0100 | batch 0004 / 0050 | loss 0.0290\n",
            "Train: epoch 0086 / 0100 | batch 0005 / 0050 | loss 0.0307\n",
            "Train: epoch 0086 / 0100 | batch 0006 / 0050 | loss 0.0295\n",
            "Train: epoch 0086 / 0100 | batch 0007 / 0050 | loss 0.0277\n",
            "Train: epoch 0086 / 0100 | batch 0008 / 0050 | loss 0.0288\n",
            "Train: epoch 0086 / 0100 | batch 0009 / 0050 | loss 0.0278\n",
            "Train: epoch 0086 / 0100 | batch 0010 / 0050 | loss 0.0273\n",
            "Train: epoch 0086 / 0100 | batch 0011 / 0050 | loss 0.0269\n",
            "Train: epoch 0086 / 0100 | batch 0012 / 0050 | loss 0.0271\n",
            "Train: epoch 0086 / 0100 | batch 0013 / 0050 | loss 0.0272\n",
            "Train: epoch 0086 / 0100 | batch 0014 / 0050 | loss 0.0266\n",
            "Train: epoch 0086 / 0100 | batch 0015 / 0050 | loss 0.0266\n",
            "Train: epoch 0086 / 0100 | batch 0016 / 0050 | loss 0.0267\n",
            "Train: epoch 0086 / 0100 | batch 0017 / 0050 | loss 0.0264\n",
            "Train: epoch 0086 / 0100 | batch 0018 / 0050 | loss 0.0263\n",
            "Train: epoch 0086 / 0100 | batch 0019 / 0050 | loss 0.0266\n",
            "Train: epoch 0086 / 0100 | batch 0020 / 0050 | loss 0.0268\n",
            "Train: epoch 0086 / 0100 | batch 0021 / 0050 | loss 0.0274\n",
            "Train: epoch 0086 / 0100 | batch 0022 / 0050 | loss 0.0272\n",
            "Train: epoch 0086 / 0100 | batch 0023 / 0050 | loss 0.0271\n",
            "Train: epoch 0086 / 0100 | batch 0024 / 0050 | loss 0.0271\n",
            "Train: epoch 0086 / 0100 | batch 0025 / 0050 | loss 0.0272\n",
            "Train: epoch 0086 / 0100 | batch 0026 / 0050 | loss 0.0270\n",
            "Train: epoch 0086 / 0100 | batch 0027 / 0050 | loss 0.0268\n",
            "Train: epoch 0086 / 0100 | batch 0028 / 0050 | loss 0.0266\n",
            "Train: epoch 0086 / 0100 | batch 0029 / 0050 | loss 0.0264\n",
            "Train: epoch 0086 / 0100 | batch 0030 / 0050 | loss 0.0264\n",
            "Train: epoch 0086 / 0100 | batch 0031 / 0050 | loss 0.0262\n",
            "Train: epoch 0086 / 0100 | batch 0032 / 0050 | loss 0.0262\n",
            "Train: epoch 0086 / 0100 | batch 0033 / 0050 | loss 0.0262\n",
            "Train: epoch 0086 / 0100 | batch 0034 / 0050 | loss 0.0260\n",
            "Train: epoch 0086 / 0100 | batch 0035 / 0050 | loss 0.0259\n",
            "Train: epoch 0086 / 0100 | batch 0036 / 0050 | loss 0.0260\n",
            "Train: epoch 0086 / 0100 | batch 0037 / 0050 | loss 0.0258\n",
            "Train: epoch 0086 / 0100 | batch 0038 / 0050 | loss 0.0258\n",
            "Train: epoch 0086 / 0100 | batch 0039 / 0050 | loss 0.0258\n",
            "Train: epoch 0086 / 0100 | batch 0040 / 0050 | loss 0.0256\n",
            "Train: epoch 0086 / 0100 | batch 0041 / 0050 | loss 0.0254\n",
            "Train: epoch 0086 / 0100 | batch 0042 / 0050 | loss 0.0252\n",
            "Train: epoch 0086 / 0100 | batch 0043 / 0050 | loss 0.0253\n",
            "Train: epoch 0086 / 0100 | batch 0044 / 0050 | loss 0.0252\n",
            "Train: epoch 0086 / 0100 | batch 0045 / 0050 | loss 0.0252\n",
            "Train: epoch 0086 / 0100 | batch 0046 / 0050 | loss 0.0250\n",
            "Train: epoch 0086 / 0100 | batch 0047 / 0050 | loss 0.0251\n",
            "Train: epoch 0086 / 0100 | batch 0048 / 0050 | loss 0.0251\n",
            "Train: epoch 0086 / 0100 | batch 0049 / 0050 | loss 0.0251\n",
            "Val loss 0.0394\n",
            "Dice score : 0.11339983344078064\n",
            "Val loss 0.0363\n",
            "Dice score : 0.12051306664943695\n",
            "Val loss 0.0322\n",
            "Dice score : 0.05947062745690346\n",
            "Val loss 0.0294\n",
            "Dice score : 0.0906495675444603\n",
            "Val loss 0.0321\n",
            "Dice score : 0.09761710464954376\n",
            "Val loss 0.0364\n",
            "Dice score : 0.1316608339548111\n",
            "Val loss 0.0369\n",
            "Dice score : 0.1094294935464859\n",
            "Val loss 0.0404\n",
            "Dice score : 0.09509772062301636\n",
            "Val loss 0.0387\n",
            "Dice score : 0.0651593804359436\n",
            "Val loss 0.0381\n",
            "Dice score : 0.0659678727388382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 86/100 [1:44:31<17:04, 73.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0087 / 0100 | batch 0000 / 0050 | loss 0.0378\n",
            "Train: epoch 0087 / 0100 | batch 0001 / 0050 | loss 0.0274\n",
            "Train: epoch 0087 / 0100 | batch 0002 / 0050 | loss 0.0236\n",
            "Train: epoch 0087 / 0100 | batch 0003 / 0050 | loss 0.0234\n",
            "Train: epoch 0087 / 0100 | batch 0004 / 0050 | loss 0.0250\n",
            "Train: epoch 0087 / 0100 | batch 0005 / 0050 | loss 0.0250\n",
            "Train: epoch 0087 / 0100 | batch 0006 / 0050 | loss 0.0260\n",
            "Train: epoch 0087 / 0100 | batch 0007 / 0050 | loss 0.0254\n",
            "Train: epoch 0087 / 0100 | batch 0008 / 0050 | loss 0.0246\n",
            "Train: epoch 0087 / 0100 | batch 0009 / 0050 | loss 0.0241\n",
            "Train: epoch 0087 / 0100 | batch 0010 / 0050 | loss 0.0236\n",
            "Train: epoch 0087 / 0100 | batch 0011 / 0050 | loss 0.0237\n",
            "Train: epoch 0087 / 0100 | batch 0012 / 0050 | loss 0.0231\n",
            "Train: epoch 0087 / 0100 | batch 0013 / 0050 | loss 0.0235\n",
            "Train: epoch 0087 / 0100 | batch 0014 / 0050 | loss 0.0234\n",
            "Train: epoch 0087 / 0100 | batch 0015 / 0050 | loss 0.0237\n",
            "Train: epoch 0087 / 0100 | batch 0016 / 0050 | loss 0.0235\n",
            "Train: epoch 0087 / 0100 | batch 0017 / 0050 | loss 0.0230\n",
            "Train: epoch 0087 / 0100 | batch 0018 / 0050 | loss 0.0240\n",
            "Train: epoch 0087 / 0100 | batch 0019 / 0050 | loss 0.0246\n",
            "Train: epoch 0087 / 0100 | batch 0020 / 0050 | loss 0.0247\n",
            "Train: epoch 0087 / 0100 | batch 0021 / 0050 | loss 0.0243\n",
            "Train: epoch 0087 / 0100 | batch 0022 / 0050 | loss 0.0248\n",
            "Train: epoch 0087 / 0100 | batch 0023 / 0050 | loss 0.0245\n",
            "Train: epoch 0087 / 0100 | batch 0024 / 0050 | loss 0.0246\n",
            "Train: epoch 0087 / 0100 | batch 0025 / 0050 | loss 0.0246\n",
            "Train: epoch 0087 / 0100 | batch 0026 / 0050 | loss 0.0243\n",
            "Train: epoch 0087 / 0100 | batch 0027 / 0050 | loss 0.0244\n",
            "Train: epoch 0087 / 0100 | batch 0028 / 0050 | loss 0.0244\n",
            "Train: epoch 0087 / 0100 | batch 0029 / 0050 | loss 0.0246\n",
            "Train: epoch 0087 / 0100 | batch 0030 / 0050 | loss 0.0246\n",
            "Train: epoch 0087 / 0100 | batch 0031 / 0050 | loss 0.0245\n",
            "Train: epoch 0087 / 0100 | batch 0032 / 0050 | loss 0.0244\n",
            "Train: epoch 0087 / 0100 | batch 0033 / 0050 | loss 0.0244\n",
            "Train: epoch 0087 / 0100 | batch 0034 / 0050 | loss 0.0245\n",
            "Train: epoch 0087 / 0100 | batch 0035 / 0050 | loss 0.0245\n",
            "Train: epoch 0087 / 0100 | batch 0036 / 0050 | loss 0.0244\n",
            "Train: epoch 0087 / 0100 | batch 0037 / 0050 | loss 0.0243\n",
            "Train: epoch 0087 / 0100 | batch 0038 / 0050 | loss 0.0248\n",
            "Train: epoch 0087 / 0100 | batch 0039 / 0050 | loss 0.0245\n",
            "Train: epoch 0087 / 0100 | batch 0040 / 0050 | loss 0.0247\n",
            "Train: epoch 0087 / 0100 | batch 0041 / 0050 | loss 0.0248\n",
            "Train: epoch 0087 / 0100 | batch 0042 / 0050 | loss 0.0248\n",
            "Train: epoch 0087 / 0100 | batch 0043 / 0050 | loss 0.0247\n",
            "Train: epoch 0087 / 0100 | batch 0044 / 0050 | loss 0.0249\n",
            "Train: epoch 0087 / 0100 | batch 0045 / 0050 | loss 0.0248\n",
            "Train: epoch 0087 / 0100 | batch 0046 / 0050 | loss 0.0248\n",
            "Train: epoch 0087 / 0100 | batch 0047 / 0050 | loss 0.0252\n",
            "Train: epoch 0087 / 0100 | batch 0048 / 0050 | loss 0.0250\n",
            "Train: epoch 0087 / 0100 | batch 0049 / 0050 | loss 0.0249\n",
            "Val loss 0.0241\n",
            "Dice score : 0.10486532002687454\n",
            "Val loss 0.0433\n",
            "Dice score : 0.12606191635131836\n",
            "Val loss 0.0358\n",
            "Dice score : 0.09888141602277756\n",
            "Val loss 0.0353\n",
            "Dice score : 0.08601023256778717\n",
            "Val loss 0.0320\n",
            "Dice score : 0.13513118028640747\n",
            "Val loss 0.0307\n",
            "Dice score : 0.07552430033683777\n",
            "Val loss 0.0314\n",
            "Dice score : 0.09371081739664078\n",
            "Val loss 0.0336\n",
            "Dice score : 0.08641025424003601\n",
            "Val loss 0.0348\n",
            "Dice score : 0.11712660640478134\n",
            "Val loss 0.0349\n",
            "Dice score : 0.11374841630458832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 87/100 [1:45:45<15:51, 73.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0088 / 0100 | batch 0000 / 0050 | loss 0.0193\n",
            "Train: epoch 0088 / 0100 | batch 0001 / 0050 | loss 0.0262\n",
            "Train: epoch 0088 / 0100 | batch 0002 / 0050 | loss 0.0234\n",
            "Train: epoch 0088 / 0100 | batch 0003 / 0050 | loss 0.0219\n",
            "Train: epoch 0088 / 0100 | batch 0004 / 0050 | loss 0.0215\n",
            "Train: epoch 0088 / 0100 | batch 0005 / 0050 | loss 0.0207\n",
            "Train: epoch 0088 / 0100 | batch 0006 / 0050 | loss 0.0213\n",
            "Train: epoch 0088 / 0100 | batch 0007 / 0050 | loss 0.0214\n",
            "Train: epoch 0088 / 0100 | batch 0008 / 0050 | loss 0.0224\n",
            "Train: epoch 0088 / 0100 | batch 0009 / 0050 | loss 0.0222\n",
            "Train: epoch 0088 / 0100 | batch 0010 / 0050 | loss 0.0232\n",
            "Train: epoch 0088 / 0100 | batch 0011 / 0050 | loss 0.0232\n",
            "Train: epoch 0088 / 0100 | batch 0012 / 0050 | loss 0.0229\n",
            "Train: epoch 0088 / 0100 | batch 0013 / 0050 | loss 0.0234\n",
            "Train: epoch 0088 / 0100 | batch 0014 / 0050 | loss 0.0231\n",
            "Train: epoch 0088 / 0100 | batch 0015 / 0050 | loss 0.0235\n",
            "Train: epoch 0088 / 0100 | batch 0016 / 0050 | loss 0.0236\n",
            "Train: epoch 0088 / 0100 | batch 0017 / 0050 | loss 0.0233\n",
            "Train: epoch 0088 / 0100 | batch 0018 / 0050 | loss 0.0233\n",
            "Train: epoch 0088 / 0100 | batch 0019 / 0050 | loss 0.0232\n",
            "Train: epoch 0088 / 0100 | batch 0020 / 0050 | loss 0.0231\n",
            "Train: epoch 0088 / 0100 | batch 0021 / 0050 | loss 0.0236\n",
            "Train: epoch 0088 / 0100 | batch 0022 / 0050 | loss 0.0235\n",
            "Train: epoch 0088 / 0100 | batch 0023 / 0050 | loss 0.0235\n",
            "Train: epoch 0088 / 0100 | batch 0024 / 0050 | loss 0.0232\n",
            "Train: epoch 0088 / 0100 | batch 0025 / 0050 | loss 0.0232\n",
            "Train: epoch 0088 / 0100 | batch 0026 / 0050 | loss 0.0232\n",
            "Train: epoch 0088 / 0100 | batch 0027 / 0050 | loss 0.0230\n",
            "Train: epoch 0088 / 0100 | batch 0028 / 0050 | loss 0.0234\n",
            "Train: epoch 0088 / 0100 | batch 0029 / 0050 | loss 0.0235\n",
            "Train: epoch 0088 / 0100 | batch 0030 / 0050 | loss 0.0232\n",
            "Train: epoch 0088 / 0100 | batch 0031 / 0050 | loss 0.0235\n",
            "Train: epoch 0088 / 0100 | batch 0032 / 0050 | loss 0.0233\n",
            "Train: epoch 0088 / 0100 | batch 0033 / 0050 | loss 0.0240\n",
            "Train: epoch 0088 / 0100 | batch 0034 / 0050 | loss 0.0239\n",
            "Train: epoch 0088 / 0100 | batch 0035 / 0050 | loss 0.0236\n",
            "Train: epoch 0088 / 0100 | batch 0036 / 0050 | loss 0.0236\n",
            "Train: epoch 0088 / 0100 | batch 0037 / 0050 | loss 0.0242\n",
            "Train: epoch 0088 / 0100 | batch 0038 / 0050 | loss 0.0242\n",
            "Train: epoch 0088 / 0100 | batch 0039 / 0050 | loss 0.0241\n",
            "Train: epoch 0088 / 0100 | batch 0040 / 0050 | loss 0.0241\n",
            "Train: epoch 0088 / 0100 | batch 0041 / 0050 | loss 0.0239\n",
            "Train: epoch 0088 / 0100 | batch 0042 / 0050 | loss 0.0238\n",
            "Train: epoch 0088 / 0100 | batch 0043 / 0050 | loss 0.0236\n",
            "Train: epoch 0088 / 0100 | batch 0044 / 0050 | loss 0.0235\n",
            "Train: epoch 0088 / 0100 | batch 0045 / 0050 | loss 0.0234\n",
            "Train: epoch 0088 / 0100 | batch 0046 / 0050 | loss 0.0235\n",
            "Train: epoch 0088 / 0100 | batch 0047 / 0050 | loss 0.0234\n",
            "Train: epoch 0088 / 0100 | batch 0048 / 0050 | loss 0.0238\n",
            "Train: epoch 0088 / 0100 | batch 0049 / 0050 | loss 0.0236\n",
            "Val loss 0.0574\n",
            "Dice score : 0.11010567098855972\n",
            "Val loss 0.0412\n",
            "Dice score : 0.07337908446788788\n",
            "Val loss 0.0398\n",
            "Dice score : 0.11564908176660538\n",
            "Val loss 0.0384\n",
            "Dice score : 0.13411466777324677\n",
            "Val loss 0.0368\n",
            "Dice score : 0.05282958596944809\n",
            "Val loss 0.0448\n",
            "Dice score : 0.16242283582687378\n",
            "Val loss 0.0420\n",
            "Dice score : 0.1193302720785141\n",
            "Val loss 0.0400\n",
            "Dice score : 0.08181396871805191\n",
            "Val loss 0.0396\n",
            "Dice score : 0.0768933817744255\n",
            "Val loss 0.0385\n",
            "Dice score : 0.09460961818695068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 88/100 [1:46:59<14:41, 73.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0089 / 0100 | batch 0000 / 0050 | loss 0.0267\n",
            "Train: epoch 0089 / 0100 | batch 0001 / 0050 | loss 0.0333\n",
            "Train: epoch 0089 / 0100 | batch 0002 / 0050 | loss 0.0348\n",
            "Train: epoch 0089 / 0100 | batch 0003 / 0050 | loss 0.0327\n",
            "Train: epoch 0089 / 0100 | batch 0004 / 0050 | loss 0.0302\n",
            "Train: epoch 0089 / 0100 | batch 0005 / 0050 | loss 0.0277\n",
            "Train: epoch 0089 / 0100 | batch 0006 / 0050 | loss 0.0279\n",
            "Train: epoch 0089 / 0100 | batch 0007 / 0050 | loss 0.0285\n",
            "Train: epoch 0089 / 0100 | batch 0008 / 0050 | loss 0.0275\n",
            "Train: epoch 0089 / 0100 | batch 0009 / 0050 | loss 0.0267\n",
            "Train: epoch 0089 / 0100 | batch 0010 / 0050 | loss 0.0261\n",
            "Train: epoch 0089 / 0100 | batch 0011 / 0050 | loss 0.0265\n",
            "Train: epoch 0089 / 0100 | batch 0012 / 0050 | loss 0.0256\n",
            "Train: epoch 0089 / 0100 | batch 0013 / 0050 | loss 0.0258\n",
            "Train: epoch 0089 / 0100 | batch 0014 / 0050 | loss 0.0254\n",
            "Train: epoch 0089 / 0100 | batch 0015 / 0050 | loss 0.0249\n",
            "Train: epoch 0089 / 0100 | batch 0016 / 0050 | loss 0.0258\n",
            "Train: epoch 0089 / 0100 | batch 0017 / 0050 | loss 0.0259\n",
            "Train: epoch 0089 / 0100 | batch 0018 / 0050 | loss 0.0254\n",
            "Train: epoch 0089 / 0100 | batch 0019 / 0050 | loss 0.0253\n",
            "Train: epoch 0089 / 0100 | batch 0020 / 0050 | loss 0.0256\n",
            "Train: epoch 0089 / 0100 | batch 0021 / 0050 | loss 0.0257\n",
            "Train: epoch 0089 / 0100 | batch 0022 / 0050 | loss 0.0254\n",
            "Train: epoch 0089 / 0100 | batch 0023 / 0050 | loss 0.0256\n",
            "Train: epoch 0089 / 0100 | batch 0024 / 0050 | loss 0.0253\n",
            "Train: epoch 0089 / 0100 | batch 0025 / 0050 | loss 0.0250\n",
            "Train: epoch 0089 / 0100 | batch 0026 / 0050 | loss 0.0252\n",
            "Train: epoch 0089 / 0100 | batch 0027 / 0050 | loss 0.0249\n",
            "Train: epoch 0089 / 0100 | batch 0028 / 0050 | loss 0.0249\n",
            "Train: epoch 0089 / 0100 | batch 0029 / 0050 | loss 0.0250\n",
            "Train: epoch 0089 / 0100 | batch 0030 / 0050 | loss 0.0248\n",
            "Train: epoch 0089 / 0100 | batch 0031 / 0050 | loss 0.0247\n",
            "Train: epoch 0089 / 0100 | batch 0032 / 0050 | loss 0.0245\n",
            "Train: epoch 0089 / 0100 | batch 0033 / 0050 | loss 0.0245\n",
            "Train: epoch 0089 / 0100 | batch 0034 / 0050 | loss 0.0243\n",
            "Train: epoch 0089 / 0100 | batch 0035 / 0050 | loss 0.0243\n",
            "Train: epoch 0089 / 0100 | batch 0036 / 0050 | loss 0.0241\n",
            "Train: epoch 0089 / 0100 | batch 0037 / 0050 | loss 0.0246\n",
            "Train: epoch 0089 / 0100 | batch 0038 / 0050 | loss 0.0247\n",
            "Train: epoch 0089 / 0100 | batch 0039 / 0050 | loss 0.0246\n",
            "Train: epoch 0089 / 0100 | batch 0040 / 0050 | loss 0.0244\n",
            "Train: epoch 0089 / 0100 | batch 0041 / 0050 | loss 0.0243\n",
            "Train: epoch 0089 / 0100 | batch 0042 / 0050 | loss 0.0245\n",
            "Train: epoch 0089 / 0100 | batch 0043 / 0050 | loss 0.0244\n",
            "Train: epoch 0089 / 0100 | batch 0044 / 0050 | loss 0.0244\n",
            "Train: epoch 0089 / 0100 | batch 0045 / 0050 | loss 0.0244\n",
            "Train: epoch 0089 / 0100 | batch 0046 / 0050 | loss 0.0246\n",
            "Train: epoch 0089 / 0100 | batch 0047 / 0050 | loss 0.0245\n",
            "Train: epoch 0089 / 0100 | batch 0048 / 0050 | loss 0.0244\n",
            "Train: epoch 0089 / 0100 | batch 0049 / 0050 | loss 0.0244\n",
            "Val loss 0.0250\n",
            "Dice score : 0.1061844751238823\n",
            "Val loss 0.0228\n",
            "Dice score : 0.0649649128317833\n",
            "Val loss 0.0351\n",
            "Dice score : 0.06936406344175339\n",
            "Val loss 0.0363\n",
            "Dice score : 0.13285890221595764\n",
            "Val loss 0.0343\n",
            "Dice score : 0.054666902869939804\n",
            "Val loss 0.0372\n",
            "Dice score : 0.11752446740865707\n",
            "Val loss 0.0371\n",
            "Dice score : 0.10789909958839417\n",
            "Val loss 0.0354\n",
            "Dice score : 0.11533995717763901\n",
            "Val loss 0.0377\n",
            "Dice score : 0.09871084988117218\n",
            "Val loss 0.0375\n",
            "Dice score : 0.05021729692816734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 89/100 [1:48:10<13:19, 72.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0090 / 0100 | batch 0000 / 0050 | loss 0.0234\n",
            "Train: epoch 0090 / 0100 | batch 0001 / 0050 | loss 0.0207\n",
            "Train: epoch 0090 / 0100 | batch 0002 / 0050 | loss 0.0201\n",
            "Train: epoch 0090 / 0100 | batch 0003 / 0050 | loss 0.0199\n",
            "Train: epoch 0090 / 0100 | batch 0004 / 0050 | loss 0.0202\n",
            "Train: epoch 0090 / 0100 | batch 0005 / 0050 | loss 0.0204\n",
            "Train: epoch 0090 / 0100 | batch 0006 / 0050 | loss 0.0201\n",
            "Train: epoch 0090 / 0100 | batch 0007 / 0050 | loss 0.0197\n",
            "Train: epoch 0090 / 0100 | batch 0008 / 0050 | loss 0.0199\n",
            "Train: epoch 0090 / 0100 | batch 0009 / 0050 | loss 0.0204\n",
            "Train: epoch 0090 / 0100 | batch 0010 / 0050 | loss 0.0201\n",
            "Train: epoch 0090 / 0100 | batch 0011 / 0050 | loss 0.0198\n",
            "Train: epoch 0090 / 0100 | batch 0012 / 0050 | loss 0.0198\n",
            "Train: epoch 0090 / 0100 | batch 0013 / 0050 | loss 0.0206\n",
            "Train: epoch 0090 / 0100 | batch 0014 / 0050 | loss 0.0205\n",
            "Train: epoch 0090 / 0100 | batch 0015 / 0050 | loss 0.0209\n",
            "Train: epoch 0090 / 0100 | batch 0016 / 0050 | loss 0.0207\n",
            "Train: epoch 0090 / 0100 | batch 0017 / 0050 | loss 0.0214\n",
            "Train: epoch 0090 / 0100 | batch 0018 / 0050 | loss 0.0213\n",
            "Train: epoch 0090 / 0100 | batch 0019 / 0050 | loss 0.0211\n",
            "Train: epoch 0090 / 0100 | batch 0020 / 0050 | loss 0.0219\n",
            "Train: epoch 0090 / 0100 | batch 0021 / 0050 | loss 0.0222\n",
            "Train: epoch 0090 / 0100 | batch 0022 / 0050 | loss 0.0225\n",
            "Train: epoch 0090 / 0100 | batch 0023 / 0050 | loss 0.0222\n",
            "Train: epoch 0090 / 0100 | batch 0024 / 0050 | loss 0.0227\n",
            "Train: epoch 0090 / 0100 | batch 0025 / 0050 | loss 0.0225\n",
            "Train: epoch 0090 / 0100 | batch 0026 / 0050 | loss 0.0227\n",
            "Train: epoch 0090 / 0100 | batch 0027 / 0050 | loss 0.0232\n",
            "Train: epoch 0090 / 0100 | batch 0028 / 0050 | loss 0.0234\n",
            "Train: epoch 0090 / 0100 | batch 0029 / 0050 | loss 0.0232\n",
            "Train: epoch 0090 / 0100 | batch 0030 / 0050 | loss 0.0231\n",
            "Train: epoch 0090 / 0100 | batch 0031 / 0050 | loss 0.0231\n",
            "Train: epoch 0090 / 0100 | batch 0032 / 0050 | loss 0.0232\n",
            "Train: epoch 0090 / 0100 | batch 0033 / 0050 | loss 0.0231\n",
            "Train: epoch 0090 / 0100 | batch 0034 / 0050 | loss 0.0231\n",
            "Train: epoch 0090 / 0100 | batch 0035 / 0050 | loss 0.0232\n",
            "Train: epoch 0090 / 0100 | batch 0036 / 0050 | loss 0.0231\n",
            "Train: epoch 0090 / 0100 | batch 0037 / 0050 | loss 0.0236\n",
            "Train: epoch 0090 / 0100 | batch 0038 / 0050 | loss 0.0235\n",
            "Train: epoch 0090 / 0100 | batch 0039 / 0050 | loss 0.0236\n",
            "Train: epoch 0090 / 0100 | batch 0040 / 0050 | loss 0.0236\n",
            "Train: epoch 0090 / 0100 | batch 0041 / 0050 | loss 0.0235\n",
            "Train: epoch 0090 / 0100 | batch 0042 / 0050 | loss 0.0235\n",
            "Train: epoch 0090 / 0100 | batch 0043 / 0050 | loss 0.0234\n",
            "Train: epoch 0090 / 0100 | batch 0044 / 0050 | loss 0.0233\n",
            "Train: epoch 0090 / 0100 | batch 0045 / 0050 | loss 0.0233\n",
            "Train: epoch 0090 / 0100 | batch 0046 / 0050 | loss 0.0235\n",
            "Train: epoch 0090 / 0100 | batch 0047 / 0050 | loss 0.0236\n",
            "Train: epoch 0090 / 0100 | batch 0048 / 0050 | loss 0.0234\n",
            "Train: epoch 0090 / 0100 | batch 0049 / 0050 | loss 0.0234\n",
            "Val loss 0.0476\n",
            "Dice score : 0.09577437490224838\n",
            "Val loss 0.0513\n",
            "Dice score : 0.10309315472841263\n",
            "Val loss 0.0554\n",
            "Dice score : 0.06760799139738083\n",
            "Val loss 0.0589\n",
            "Dice score : 0.11050579696893692\n",
            "Val loss 0.0544\n",
            "Dice score : 0.06547610461711884\n",
            "Val loss 0.0509\n",
            "Dice score : 0.08970639854669571\n",
            "Val loss 0.0471\n",
            "Dice score : 0.05828682705760002\n",
            "Val loss 0.0451\n",
            "Dice score : 0.09189409017562866\n",
            "Val loss 0.0428\n",
            "Dice score : 0.06760148704051971\n",
            "Val loss 0.0415\n",
            "Dice score : 0.07999404519796371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 90/100 [1:49:24<12:11, 73.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0091 / 0100 | batch 0000 / 0050 | loss 0.0238\n",
            "Train: epoch 0091 / 0100 | batch 0001 / 0050 | loss 0.0204\n",
            "Train: epoch 0091 / 0100 | batch 0002 / 0050 | loss 0.0192\n",
            "Train: epoch 0091 / 0100 | batch 0003 / 0050 | loss 0.0207\n",
            "Train: epoch 0091 / 0100 | batch 0004 / 0050 | loss 0.0214\n",
            "Train: epoch 0091 / 0100 | batch 0005 / 0050 | loss 0.0210\n",
            "Train: epoch 0091 / 0100 | batch 0006 / 0050 | loss 0.0222\n",
            "Train: epoch 0091 / 0100 | batch 0007 / 0050 | loss 0.0226\n",
            "Train: epoch 0091 / 0100 | batch 0008 / 0050 | loss 0.0222\n",
            "Train: epoch 0091 / 0100 | batch 0009 / 0050 | loss 0.0216\n",
            "Train: epoch 0091 / 0100 | batch 0010 / 0050 | loss 0.0221\n",
            "Train: epoch 0091 / 0100 | batch 0011 / 0050 | loss 0.0213\n",
            "Train: epoch 0091 / 0100 | batch 0012 / 0050 | loss 0.0214\n",
            "Train: epoch 0091 / 0100 | batch 0013 / 0050 | loss 0.0213\n",
            "Train: epoch 0091 / 0100 | batch 0014 / 0050 | loss 0.0216\n",
            "Train: epoch 0091 / 0100 | batch 0015 / 0050 | loss 0.0214\n",
            "Train: epoch 0091 / 0100 | batch 0016 / 0050 | loss 0.0214\n",
            "Train: epoch 0091 / 0100 | batch 0017 / 0050 | loss 0.0224\n",
            "Train: epoch 0091 / 0100 | batch 0018 / 0050 | loss 0.0222\n",
            "Train: epoch 0091 / 0100 | batch 0019 / 0050 | loss 0.0218\n",
            "Train: epoch 0091 / 0100 | batch 0020 / 0050 | loss 0.0216\n",
            "Train: epoch 0091 / 0100 | batch 0021 / 0050 | loss 0.0216\n",
            "Train: epoch 0091 / 0100 | batch 0022 / 0050 | loss 0.0217\n",
            "Train: epoch 0091 / 0100 | batch 0023 / 0050 | loss 0.0222\n",
            "Train: epoch 0091 / 0100 | batch 0024 / 0050 | loss 0.0219\n",
            "Train: epoch 0091 / 0100 | batch 0025 / 0050 | loss 0.0219\n",
            "Train: epoch 0091 / 0100 | batch 0026 / 0050 | loss 0.0218\n",
            "Train: epoch 0091 / 0100 | batch 0027 / 0050 | loss 0.0216\n",
            "Train: epoch 0091 / 0100 | batch 0028 / 0050 | loss 0.0215\n",
            "Train: epoch 0091 / 0100 | batch 0029 / 0050 | loss 0.0214\n",
            "Train: epoch 0091 / 0100 | batch 0030 / 0050 | loss 0.0213\n",
            "Train: epoch 0091 / 0100 | batch 0031 / 0050 | loss 0.0217\n",
            "Train: epoch 0091 / 0100 | batch 0032 / 0050 | loss 0.0222\n",
            "Train: epoch 0091 / 0100 | batch 0033 / 0050 | loss 0.0222\n",
            "Train: epoch 0091 / 0100 | batch 0034 / 0050 | loss 0.0221\n",
            "Train: epoch 0091 / 0100 | batch 0035 / 0050 | loss 0.0219\n",
            "Train: epoch 0091 / 0100 | batch 0036 / 0050 | loss 0.0219\n",
            "Train: epoch 0091 / 0100 | batch 0037 / 0050 | loss 0.0222\n",
            "Train: epoch 0091 / 0100 | batch 0038 / 0050 | loss 0.0221\n",
            "Train: epoch 0091 / 0100 | batch 0039 / 0050 | loss 0.0223\n",
            "Train: epoch 0091 / 0100 | batch 0040 / 0050 | loss 0.0223\n",
            "Train: epoch 0091 / 0100 | batch 0041 / 0050 | loss 0.0225\n",
            "Train: epoch 0091 / 0100 | batch 0042 / 0050 | loss 0.0223\n",
            "Train: epoch 0091 / 0100 | batch 0043 / 0050 | loss 0.0223\n",
            "Train: epoch 0091 / 0100 | batch 0044 / 0050 | loss 0.0224\n",
            "Train: epoch 0091 / 0100 | batch 0045 / 0050 | loss 0.0224\n",
            "Train: epoch 0091 / 0100 | batch 0046 / 0050 | loss 0.0225\n",
            "Train: epoch 0091 / 0100 | batch 0047 / 0050 | loss 0.0226\n",
            "Train: epoch 0091 / 0100 | batch 0048 / 0050 | loss 0.0226\n",
            "Train: epoch 0091 / 0100 | batch 0049 / 0050 | loss 0.0226\n",
            "Val loss 0.0769\n",
            "Dice score : 0.11778328567743301\n",
            "Val loss 0.0707\n",
            "Dice score : 0.09223498404026031\n",
            "Val loss 0.0636\n",
            "Dice score : 0.08996071666479111\n",
            "Val loss 0.0552\n",
            "Dice score : 0.07587540149688721\n",
            "Val loss 0.0494\n",
            "Dice score : 0.08310380578041077\n",
            "Val loss 0.0443\n",
            "Dice score : 0.07167808711528778\n",
            "Val loss 0.0418\n",
            "Dice score : 0.10095919668674469\n",
            "Val loss 0.0414\n",
            "Dice score : 0.050227463245391846\n",
            "Val loss 0.0396\n",
            "Dice score : 0.07124773412942886\n",
            "Val loss 0.0396\n",
            "Dice score : 0.08164439350366592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 91/100 [1:50:37<10:58, 73.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0092 / 0100 | batch 0000 / 0050 | loss 0.0196\n",
            "Train: epoch 0092 / 0100 | batch 0001 / 0050 | loss 0.0209\n",
            "Train: epoch 0092 / 0100 | batch 0002 / 0050 | loss 0.0239\n",
            "Train: epoch 0092 / 0100 | batch 0003 / 0050 | loss 0.0253\n",
            "Train: epoch 0092 / 0100 | batch 0004 / 0050 | loss 0.0259\n",
            "Train: epoch 0092 / 0100 | batch 0005 / 0050 | loss 0.0268\n",
            "Train: epoch 0092 / 0100 | batch 0006 / 0050 | loss 0.0260\n",
            "Train: epoch 0092 / 0100 | batch 0007 / 0050 | loss 0.0250\n",
            "Train: epoch 0092 / 0100 | batch 0008 / 0050 | loss 0.0237\n",
            "Train: epoch 0092 / 0100 | batch 0009 / 0050 | loss 0.0261\n",
            "Train: epoch 0092 / 0100 | batch 0010 / 0050 | loss 0.0261\n",
            "Train: epoch 0092 / 0100 | batch 0011 / 0050 | loss 0.0264\n",
            "Train: epoch 0092 / 0100 | batch 0012 / 0050 | loss 0.0264\n",
            "Train: epoch 0092 / 0100 | batch 0013 / 0050 | loss 0.0260\n",
            "Train: epoch 0092 / 0100 | batch 0014 / 0050 | loss 0.0253\n",
            "Train: epoch 0092 / 0100 | batch 0015 / 0050 | loss 0.0255\n",
            "Train: epoch 0092 / 0100 | batch 0016 / 0050 | loss 0.0250\n",
            "Train: epoch 0092 / 0100 | batch 0017 / 0050 | loss 0.0246\n",
            "Train: epoch 0092 / 0100 | batch 0018 / 0050 | loss 0.0245\n",
            "Train: epoch 0092 / 0100 | batch 0019 / 0050 | loss 0.0246\n",
            "Train: epoch 0092 / 0100 | batch 0020 / 0050 | loss 0.0244\n",
            "Train: epoch 0092 / 0100 | batch 0021 / 0050 | loss 0.0242\n",
            "Train: epoch 0092 / 0100 | batch 0022 / 0050 | loss 0.0238\n",
            "Train: epoch 0092 / 0100 | batch 0023 / 0050 | loss 0.0236\n",
            "Train: epoch 0092 / 0100 | batch 0024 / 0050 | loss 0.0235\n",
            "Train: epoch 0092 / 0100 | batch 0025 / 0050 | loss 0.0232\n",
            "Train: epoch 0092 / 0100 | batch 0026 / 0050 | loss 0.0229\n",
            "Train: epoch 0092 / 0100 | batch 0027 / 0050 | loss 0.0226\n",
            "Train: epoch 0092 / 0100 | batch 0028 / 0050 | loss 0.0228\n",
            "Train: epoch 0092 / 0100 | batch 0029 / 0050 | loss 0.0228\n",
            "Train: epoch 0092 / 0100 | batch 0030 / 0050 | loss 0.0226\n",
            "Train: epoch 0092 / 0100 | batch 0031 / 0050 | loss 0.0229\n",
            "Train: epoch 0092 / 0100 | batch 0032 / 0050 | loss 0.0228\n",
            "Train: epoch 0092 / 0100 | batch 0033 / 0050 | loss 0.0228\n",
            "Train: epoch 0092 / 0100 | batch 0034 / 0050 | loss 0.0228\n",
            "Train: epoch 0092 / 0100 | batch 0035 / 0050 | loss 0.0227\n",
            "Train: epoch 0092 / 0100 | batch 0036 / 0050 | loss 0.0225\n",
            "Train: epoch 0092 / 0100 | batch 0037 / 0050 | loss 0.0227\n",
            "Train: epoch 0092 / 0100 | batch 0038 / 0050 | loss 0.0227\n",
            "Train: epoch 0092 / 0100 | batch 0039 / 0050 | loss 0.0225\n",
            "Train: epoch 0092 / 0100 | batch 0040 / 0050 | loss 0.0223\n",
            "Train: epoch 0092 / 0100 | batch 0041 / 0050 | loss 0.0222\n",
            "Train: epoch 0092 / 0100 | batch 0042 / 0050 | loss 0.0222\n",
            "Train: epoch 0092 / 0100 | batch 0043 / 0050 | loss 0.0222\n",
            "Train: epoch 0092 / 0100 | batch 0044 / 0050 | loss 0.0220\n",
            "Train: epoch 0092 / 0100 | batch 0045 / 0050 | loss 0.0221\n",
            "Train: epoch 0092 / 0100 | batch 0046 / 0050 | loss 0.0221\n",
            "Train: epoch 0092 / 0100 | batch 0047 / 0050 | loss 0.0221\n",
            "Train: epoch 0092 / 0100 | batch 0048 / 0050 | loss 0.0221\n",
            "Train: epoch 0092 / 0100 | batch 0049 / 0050 | loss 0.0223\n",
            "Val loss 0.0470\n",
            "Dice score : 0.1828857958316803\n",
            "Val loss 0.0363\n",
            "Dice score : 0.11783427000045776\n",
            "Val loss 0.0428\n",
            "Dice score : 0.11324683576822281\n",
            "Val loss 0.0372\n",
            "Dice score : 0.06618154793977737\n",
            "Val loss 0.0339\n",
            "Dice score : 0.15531738102436066\n",
            "Val loss 0.0387\n",
            "Dice score : 0.10576385259628296\n",
            "Val loss 0.0372\n",
            "Dice score : 0.09042845666408539\n",
            "Val loss 0.0384\n",
            "Dice score : 0.143409863114357\n",
            "Val loss 0.0366\n",
            "Dice score : 0.09788589924573898\n",
            "Val loss 0.0372\n",
            "Dice score : 0.05517512932419777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 92/100 [1:51:50<09:44, 73.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0093 / 0100 | batch 0000 / 0050 | loss 0.0298\n",
            "Train: epoch 0093 / 0100 | batch 0001 / 0050 | loss 0.0268\n",
            "Train: epoch 0093 / 0100 | batch 0002 / 0050 | loss 0.0247\n",
            "Train: epoch 0093 / 0100 | batch 0003 / 0050 | loss 0.0254\n",
            "Train: epoch 0093 / 0100 | batch 0004 / 0050 | loss 0.0244\n",
            "Train: epoch 0093 / 0100 | batch 0005 / 0050 | loss 0.0227\n",
            "Train: epoch 0093 / 0100 | batch 0006 / 0050 | loss 0.0237\n",
            "Train: epoch 0093 / 0100 | batch 0007 / 0050 | loss 0.0229\n",
            "Train: epoch 0093 / 0100 | batch 0008 / 0050 | loss 0.0231\n",
            "Train: epoch 0093 / 0100 | batch 0009 / 0050 | loss 0.0226\n",
            "Train: epoch 0093 / 0100 | batch 0010 / 0050 | loss 0.0219\n",
            "Train: epoch 0093 / 0100 | batch 0011 / 0050 | loss 0.0221\n",
            "Train: epoch 0093 / 0100 | batch 0012 / 0050 | loss 0.0217\n",
            "Train: epoch 0093 / 0100 | batch 0013 / 0050 | loss 0.0217\n",
            "Train: epoch 0093 / 0100 | batch 0014 / 0050 | loss 0.0213\n",
            "Train: epoch 0093 / 0100 | batch 0015 / 0050 | loss 0.0210\n",
            "Train: epoch 0093 / 0100 | batch 0016 / 0050 | loss 0.0217\n",
            "Train: epoch 0093 / 0100 | batch 0017 / 0050 | loss 0.0217\n",
            "Train: epoch 0093 / 0100 | batch 0018 / 0050 | loss 0.0224\n",
            "Train: epoch 0093 / 0100 | batch 0019 / 0050 | loss 0.0228\n",
            "Train: epoch 0093 / 0100 | batch 0020 / 0050 | loss 0.0226\n",
            "Train: epoch 0093 / 0100 | batch 0021 / 0050 | loss 0.0224\n",
            "Train: epoch 0093 / 0100 | batch 0022 / 0050 | loss 0.0225\n",
            "Train: epoch 0093 / 0100 | batch 0023 / 0050 | loss 0.0227\n",
            "Train: epoch 0093 / 0100 | batch 0024 / 0050 | loss 0.0226\n",
            "Train: epoch 0093 / 0100 | batch 0025 / 0050 | loss 0.0227\n",
            "Train: epoch 0093 / 0100 | batch 0026 / 0050 | loss 0.0229\n",
            "Train: epoch 0093 / 0100 | batch 0027 / 0050 | loss 0.0229\n",
            "Train: epoch 0093 / 0100 | batch 0028 / 0050 | loss 0.0226\n",
            "Train: epoch 0093 / 0100 | batch 0029 / 0050 | loss 0.0225\n",
            "Train: epoch 0093 / 0100 | batch 0030 / 0050 | loss 0.0225\n",
            "Train: epoch 0093 / 0100 | batch 0031 / 0050 | loss 0.0223\n",
            "Train: epoch 0093 / 0100 | batch 0032 / 0050 | loss 0.0224\n",
            "Train: epoch 0093 / 0100 | batch 0033 / 0050 | loss 0.0224\n",
            "Train: epoch 0093 / 0100 | batch 0034 / 0050 | loss 0.0227\n",
            "Train: epoch 0093 / 0100 | batch 0035 / 0050 | loss 0.0225\n",
            "Train: epoch 0093 / 0100 | batch 0036 / 0050 | loss 0.0224\n",
            "Train: epoch 0093 / 0100 | batch 0037 / 0050 | loss 0.0223\n",
            "Train: epoch 0093 / 0100 | batch 0038 / 0050 | loss 0.0221\n",
            "Train: epoch 0093 / 0100 | batch 0039 / 0050 | loss 0.0222\n",
            "Train: epoch 0093 / 0100 | batch 0040 / 0050 | loss 0.0223\n",
            "Train: epoch 0093 / 0100 | batch 0041 / 0050 | loss 0.0223\n",
            "Train: epoch 0093 / 0100 | batch 0042 / 0050 | loss 0.0221\n",
            "Train: epoch 0093 / 0100 | batch 0043 / 0050 | loss 0.0220\n",
            "Train: epoch 0093 / 0100 | batch 0044 / 0050 | loss 0.0219\n",
            "Train: epoch 0093 / 0100 | batch 0045 / 0050 | loss 0.0218\n",
            "Train: epoch 0093 / 0100 | batch 0046 / 0050 | loss 0.0217\n",
            "Train: epoch 0093 / 0100 | batch 0047 / 0050 | loss 0.0216\n",
            "Train: epoch 0093 / 0100 | batch 0048 / 0050 | loss 0.0218\n",
            "Train: epoch 0093 / 0100 | batch 0049 / 0050 | loss 0.0217\n",
            "Val loss 0.0799\n",
            "Dice score : 0.07535729557275772\n",
            "Val loss 0.0525\n",
            "Dice score : 0.09059202671051025\n",
            "Val loss 0.0418\n",
            "Dice score : 0.10662171244621277\n",
            "Val loss 0.0417\n",
            "Dice score : 0.10987500846385956\n",
            "Val loss 0.0457\n",
            "Dice score : 0.1028805747628212\n",
            "Val loss 0.0412\n",
            "Dice score : 0.058828890323638916\n",
            "Val loss 0.0402\n",
            "Dice score : 0.10133711248636246\n",
            "Val loss 0.0435\n",
            "Dice score : 0.14416570961475372\n",
            "Val loss 0.0412\n",
            "Dice score : 0.09020771831274033\n",
            "Val loss 0.0392\n",
            "Dice score : 0.11091089248657227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 93/100 [1:53:03<08:31, 73.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0094 / 0100 | batch 0000 / 0050 | loss 0.0138\n",
            "Train: epoch 0094 / 0100 | batch 0001 / 0050 | loss 0.0254\n",
            "Train: epoch 0094 / 0100 | batch 0002 / 0050 | loss 0.0262\n",
            "Train: epoch 0094 / 0100 | batch 0003 / 0050 | loss 0.0239\n",
            "Train: epoch 0094 / 0100 | batch 0004 / 0050 | loss 0.0220\n",
            "Train: epoch 0094 / 0100 | batch 0005 / 0050 | loss 0.0209\n",
            "Train: epoch 0094 / 0100 | batch 0006 / 0050 | loss 0.0203\n",
            "Train: epoch 0094 / 0100 | batch 0007 / 0050 | loss 0.0196\n",
            "Train: epoch 0094 / 0100 | batch 0008 / 0050 | loss 0.0198\n",
            "Train: epoch 0094 / 0100 | batch 0009 / 0050 | loss 0.0209\n",
            "Train: epoch 0094 / 0100 | batch 0010 / 0050 | loss 0.0211\n",
            "Train: epoch 0094 / 0100 | batch 0011 / 0050 | loss 0.0215\n",
            "Train: epoch 0094 / 0100 | batch 0012 / 0050 | loss 0.0214\n",
            "Train: epoch 0094 / 0100 | batch 0013 / 0050 | loss 0.0215\n",
            "Train: epoch 0094 / 0100 | batch 0014 / 0050 | loss 0.0216\n",
            "Train: epoch 0094 / 0100 | batch 0015 / 0050 | loss 0.0211\n",
            "Train: epoch 0094 / 0100 | batch 0016 / 0050 | loss 0.0212\n",
            "Train: epoch 0094 / 0100 | batch 0017 / 0050 | loss 0.0210\n",
            "Train: epoch 0094 / 0100 | batch 0018 / 0050 | loss 0.0217\n",
            "Train: epoch 0094 / 0100 | batch 0019 / 0050 | loss 0.0218\n",
            "Train: epoch 0094 / 0100 | batch 0020 / 0050 | loss 0.0216\n",
            "Train: epoch 0094 / 0100 | batch 0021 / 0050 | loss 0.0214\n",
            "Train: epoch 0094 / 0100 | batch 0022 / 0050 | loss 0.0213\n",
            "Train: epoch 0094 / 0100 | batch 0023 / 0050 | loss 0.0213\n",
            "Train: epoch 0094 / 0100 | batch 0024 / 0050 | loss 0.0212\n",
            "Train: epoch 0094 / 0100 | batch 0025 / 0050 | loss 0.0209\n",
            "Train: epoch 0094 / 0100 | batch 0026 / 0050 | loss 0.0210\n",
            "Train: epoch 0094 / 0100 | batch 0027 / 0050 | loss 0.0213\n",
            "Train: epoch 0094 / 0100 | batch 0028 / 0050 | loss 0.0213\n",
            "Train: epoch 0094 / 0100 | batch 0029 / 0050 | loss 0.0212\n",
            "Train: epoch 0094 / 0100 | batch 0030 / 0050 | loss 0.0216\n",
            "Train: epoch 0094 / 0100 | batch 0031 / 0050 | loss 0.0215\n",
            "Train: epoch 0094 / 0100 | batch 0032 / 0050 | loss 0.0214\n",
            "Train: epoch 0094 / 0100 | batch 0033 / 0050 | loss 0.0215\n",
            "Train: epoch 0094 / 0100 | batch 0034 / 0050 | loss 0.0213\n",
            "Train: epoch 0094 / 0100 | batch 0035 / 0050 | loss 0.0213\n",
            "Train: epoch 0094 / 0100 | batch 0036 / 0050 | loss 0.0212\n",
            "Train: epoch 0094 / 0100 | batch 0037 / 0050 | loss 0.0212\n",
            "Train: epoch 0094 / 0100 | batch 0038 / 0050 | loss 0.0214\n",
            "Train: epoch 0094 / 0100 | batch 0039 / 0050 | loss 0.0214\n",
            "Train: epoch 0094 / 0100 | batch 0040 / 0050 | loss 0.0217\n",
            "Train: epoch 0094 / 0100 | batch 0041 / 0050 | loss 0.0215\n",
            "Train: epoch 0094 / 0100 | batch 0042 / 0050 | loss 0.0216\n",
            "Train: epoch 0094 / 0100 | batch 0043 / 0050 | loss 0.0215\n",
            "Train: epoch 0094 / 0100 | batch 0044 / 0050 | loss 0.0214\n",
            "Train: epoch 0094 / 0100 | batch 0045 / 0050 | loss 0.0216\n",
            "Train: epoch 0094 / 0100 | batch 0046 / 0050 | loss 0.0216\n",
            "Train: epoch 0094 / 0100 | batch 0047 / 0050 | loss 0.0218\n",
            "Train: epoch 0094 / 0100 | batch 0048 / 0050 | loss 0.0218\n",
            "Train: epoch 0094 / 0100 | batch 0049 / 0050 | loss 0.0216\n",
            "Val loss 0.0288\n",
            "Dice score : 0.1338655799627304\n",
            "Val loss 0.0276\n",
            "Dice score : 0.08795265108346939\n",
            "Val loss 0.0389\n",
            "Dice score : 0.07019493728876114\n",
            "Val loss 0.0366\n",
            "Dice score : 0.06308186799287796\n",
            "Val loss 0.0449\n",
            "Dice score : 0.13329777121543884\n",
            "Val loss 0.0452\n",
            "Dice score : 0.1256304681301117\n",
            "Val loss 0.0425\n",
            "Dice score : 0.09989890456199646\n",
            "Val loss 0.0419\n",
            "Dice score : 0.09551065415143967\n",
            "Val loss 0.0437\n",
            "Dice score : 0.0936691090464592\n",
            "Val loss 0.0422\n",
            "Dice score : 0.1235511302947998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 94/100 [1:54:16<07:18, 73.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0095 / 0100 | batch 0000 / 0050 | loss 0.0143\n",
            "Train: epoch 0095 / 0100 | batch 0001 / 0050 | loss 0.0153\n",
            "Train: epoch 0095 / 0100 | batch 0002 / 0050 | loss 0.0168\n",
            "Train: epoch 0095 / 0100 | batch 0003 / 0050 | loss 0.0171\n",
            "Train: epoch 0095 / 0100 | batch 0004 / 0050 | loss 0.0179\n",
            "Train: epoch 0095 / 0100 | batch 0005 / 0050 | loss 0.0187\n",
            "Train: epoch 0095 / 0100 | batch 0006 / 0050 | loss 0.0185\n",
            "Train: epoch 0095 / 0100 | batch 0007 / 0050 | loss 0.0192\n",
            "Train: epoch 0095 / 0100 | batch 0008 / 0050 | loss 0.0198\n",
            "Train: epoch 0095 / 0100 | batch 0009 / 0050 | loss 0.0199\n",
            "Train: epoch 0095 / 0100 | batch 0010 / 0050 | loss 0.0205\n",
            "Train: epoch 0095 / 0100 | batch 0011 / 0050 | loss 0.0202\n",
            "Train: epoch 0095 / 0100 | batch 0012 / 0050 | loss 0.0200\n",
            "Train: epoch 0095 / 0100 | batch 0013 / 0050 | loss 0.0208\n",
            "Train: epoch 0095 / 0100 | batch 0014 / 0050 | loss 0.0205\n",
            "Train: epoch 0095 / 0100 | batch 0015 / 0050 | loss 0.0204\n",
            "Train: epoch 0095 / 0100 | batch 0016 / 0050 | loss 0.0207\n",
            "Train: epoch 0095 / 0100 | batch 0017 / 0050 | loss 0.0210\n",
            "Train: epoch 0095 / 0100 | batch 0018 / 0050 | loss 0.0210\n",
            "Train: epoch 0095 / 0100 | batch 0019 / 0050 | loss 0.0218\n",
            "Train: epoch 0095 / 0100 | batch 0020 / 0050 | loss 0.0215\n",
            "Train: epoch 0095 / 0100 | batch 0021 / 0050 | loss 0.0215\n",
            "Train: epoch 0095 / 0100 | batch 0022 / 0050 | loss 0.0216\n",
            "Train: epoch 0095 / 0100 | batch 0023 / 0050 | loss 0.0223\n",
            "Train: epoch 0095 / 0100 | batch 0024 / 0050 | loss 0.0225\n",
            "Train: epoch 0095 / 0100 | batch 0025 / 0050 | loss 0.0230\n",
            "Train: epoch 0095 / 0100 | batch 0026 / 0050 | loss 0.0235\n",
            "Train: epoch 0095 / 0100 | batch 0027 / 0050 | loss 0.0233\n",
            "Train: epoch 0095 / 0100 | batch 0028 / 0050 | loss 0.0231\n",
            "Train: epoch 0095 / 0100 | batch 0029 / 0050 | loss 0.0229\n",
            "Train: epoch 0095 / 0100 | batch 0030 / 0050 | loss 0.0227\n",
            "Train: epoch 0095 / 0100 | batch 0031 / 0050 | loss 0.0228\n",
            "Train: epoch 0095 / 0100 | batch 0032 / 0050 | loss 0.0230\n",
            "Train: epoch 0095 / 0100 | batch 0033 / 0050 | loss 0.0229\n",
            "Train: epoch 0095 / 0100 | batch 0034 / 0050 | loss 0.0229\n",
            "Train: epoch 0095 / 0100 | batch 0035 / 0050 | loss 0.0227\n",
            "Train: epoch 0095 / 0100 | batch 0036 / 0050 | loss 0.0226\n",
            "Train: epoch 0095 / 0100 | batch 0037 / 0050 | loss 0.0224\n",
            "Train: epoch 0095 / 0100 | batch 0038 / 0050 | loss 0.0224\n",
            "Train: epoch 0095 / 0100 | batch 0039 / 0050 | loss 0.0224\n",
            "Train: epoch 0095 / 0100 | batch 0040 / 0050 | loss 0.0224\n",
            "Train: epoch 0095 / 0100 | batch 0041 / 0050 | loss 0.0224\n",
            "Train: epoch 0095 / 0100 | batch 0042 / 0050 | loss 0.0223\n",
            "Train: epoch 0095 / 0100 | batch 0043 / 0050 | loss 0.0223\n",
            "Train: epoch 0095 / 0100 | batch 0044 / 0050 | loss 0.0222\n",
            "Train: epoch 0095 / 0100 | batch 0045 / 0050 | loss 0.0222\n",
            "Train: epoch 0095 / 0100 | batch 0046 / 0050 | loss 0.0222\n",
            "Train: epoch 0095 / 0100 | batch 0047 / 0050 | loss 0.0222\n",
            "Train: epoch 0095 / 0100 | batch 0048 / 0050 | loss 0.0222\n",
            "Train: epoch 0095 / 0100 | batch 0049 / 0050 | loss 0.0223\n",
            "Val loss 0.0556\n",
            "Dice score : 0.11926763504743576\n",
            "Val loss 0.0385\n",
            "Dice score : 0.14546898007392883\n",
            "Val loss 0.0513\n",
            "Dice score : 0.17378422617912292\n",
            "Val loss 0.0443\n",
            "Dice score : 0.08943828195333481\n",
            "Val loss 0.0407\n",
            "Dice score : 0.08966126292943954\n",
            "Val loss 0.0384\n",
            "Dice score : 0.1109858974814415\n",
            "Val loss 0.0434\n",
            "Dice score : 0.08092023432254791\n",
            "Val loss 0.0418\n",
            "Dice score : 0.0791284516453743\n",
            "Val loss 0.0408\n",
            "Dice score : 0.1011112779378891\n",
            "Val loss 0.0395\n",
            "Dice score : 0.07677837461233139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 95/100 [1:55:30<06:07, 73.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0096 / 0100 | batch 0000 / 0050 | loss 0.0219\n",
            "Train: epoch 0096 / 0100 | batch 0001 / 0050 | loss 0.0257\n",
            "Train: epoch 0096 / 0100 | batch 0002 / 0050 | loss 0.0259\n",
            "Train: epoch 0096 / 0100 | batch 0003 / 0050 | loss 0.0254\n",
            "Train: epoch 0096 / 0100 | batch 0004 / 0050 | loss 0.0235\n",
            "Train: epoch 0096 / 0100 | batch 0005 / 0050 | loss 0.0223\n",
            "Train: epoch 0096 / 0100 | batch 0006 / 0050 | loss 0.0220\n",
            "Train: epoch 0096 / 0100 | batch 0007 / 0050 | loss 0.0211\n",
            "Train: epoch 0096 / 0100 | batch 0008 / 0050 | loss 0.0210\n",
            "Train: epoch 0096 / 0100 | batch 0009 / 0050 | loss 0.0221\n",
            "Train: epoch 0096 / 0100 | batch 0010 / 0050 | loss 0.0224\n",
            "Train: epoch 0096 / 0100 | batch 0011 / 0050 | loss 0.0221\n",
            "Train: epoch 0096 / 0100 | batch 0012 / 0050 | loss 0.0223\n",
            "Train: epoch 0096 / 0100 | batch 0013 / 0050 | loss 0.0217\n",
            "Train: epoch 0096 / 0100 | batch 0014 / 0050 | loss 0.0213\n",
            "Train: epoch 0096 / 0100 | batch 0015 / 0050 | loss 0.0216\n",
            "Train: epoch 0096 / 0100 | batch 0016 / 0050 | loss 0.0213\n",
            "Train: epoch 0096 / 0100 | batch 0017 / 0050 | loss 0.0210\n",
            "Train: epoch 0096 / 0100 | batch 0018 / 0050 | loss 0.0211\n",
            "Train: epoch 0096 / 0100 | batch 0019 / 0050 | loss 0.0208\n",
            "Train: epoch 0096 / 0100 | batch 0020 / 0050 | loss 0.0209\n",
            "Train: epoch 0096 / 0100 | batch 0021 / 0050 | loss 0.0214\n",
            "Train: epoch 0096 / 0100 | batch 0022 / 0050 | loss 0.0214\n",
            "Train: epoch 0096 / 0100 | batch 0023 / 0050 | loss 0.0218\n",
            "Train: epoch 0096 / 0100 | batch 0024 / 0050 | loss 0.0218\n",
            "Train: epoch 0096 / 0100 | batch 0025 / 0050 | loss 0.0217\n",
            "Train: epoch 0096 / 0100 | batch 0026 / 0050 | loss 0.0217\n",
            "Train: epoch 0096 / 0100 | batch 0027 / 0050 | loss 0.0216\n",
            "Train: epoch 0096 / 0100 | batch 0028 / 0050 | loss 0.0216\n",
            "Train: epoch 0096 / 0100 | batch 0029 / 0050 | loss 0.0217\n",
            "Train: epoch 0096 / 0100 | batch 0030 / 0050 | loss 0.0215\n",
            "Train: epoch 0096 / 0100 | batch 0031 / 0050 | loss 0.0216\n",
            "Train: epoch 0096 / 0100 | batch 0032 / 0050 | loss 0.0216\n",
            "Train: epoch 0096 / 0100 | batch 0033 / 0050 | loss 0.0214\n",
            "Train: epoch 0096 / 0100 | batch 0034 / 0050 | loss 0.0213\n",
            "Train: epoch 0096 / 0100 | batch 0035 / 0050 | loss 0.0212\n",
            "Train: epoch 0096 / 0100 | batch 0036 / 0050 | loss 0.0210\n",
            "Train: epoch 0096 / 0100 | batch 0037 / 0050 | loss 0.0215\n",
            "Train: epoch 0096 / 0100 | batch 0038 / 0050 | loss 0.0213\n",
            "Train: epoch 0096 / 0100 | batch 0039 / 0050 | loss 0.0212\n",
            "Train: epoch 0096 / 0100 | batch 0040 / 0050 | loss 0.0211\n",
            "Train: epoch 0096 / 0100 | batch 0041 / 0050 | loss 0.0211\n",
            "Train: epoch 0096 / 0100 | batch 0042 / 0050 | loss 0.0215\n",
            "Train: epoch 0096 / 0100 | batch 0043 / 0050 | loss 0.0216\n",
            "Train: epoch 0096 / 0100 | batch 0044 / 0050 | loss 0.0215\n",
            "Train: epoch 0096 / 0100 | batch 0045 / 0050 | loss 0.0213\n",
            "Train: epoch 0096 / 0100 | batch 0046 / 0050 | loss 0.0214\n",
            "Train: epoch 0096 / 0100 | batch 0047 / 0050 | loss 0.0214\n",
            "Train: epoch 0096 / 0100 | batch 0048 / 0050 | loss 0.0212\n",
            "Train: epoch 0096 / 0100 | batch 0049 / 0050 | loss 0.0212\n",
            "Val loss 0.0226\n",
            "Dice score : 0.08768431097269058\n",
            "Val loss 0.0332\n",
            "Dice score : 0.05998905375599861\n",
            "Val loss 0.0337\n",
            "Dice score : 0.08356378227472305\n",
            "Val loss 0.0316\n",
            "Dice score : 0.09102107584476471\n",
            "Val loss 0.0310\n",
            "Dice score : 0.05088619515299797\n",
            "Val loss 0.0301\n",
            "Dice score : 0.0636608749628067\n",
            "Val loss 0.0329\n",
            "Dice score : 0.0672144666314125\n",
            "Val loss 0.0349\n",
            "Dice score : 0.09205519407987595\n",
            "Val loss 0.0352\n",
            "Dice score : 0.12595099210739136\n",
            "Val loss 0.0367\n",
            "Dice score : 0.10443347692489624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 96/100 [1:56:43<04:52, 73.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0097 / 0100 | batch 0000 / 0050 | loss 0.0174\n",
            "Train: epoch 0097 / 0100 | batch 0001 / 0050 | loss 0.0216\n",
            "Train: epoch 0097 / 0100 | batch 0002 / 0050 | loss 0.0206\n",
            "Train: epoch 0097 / 0100 | batch 0003 / 0050 | loss 0.0193\n",
            "Train: epoch 0097 / 0100 | batch 0004 / 0050 | loss 0.0204\n",
            "Train: epoch 0097 / 0100 | batch 0005 / 0050 | loss 0.0244\n",
            "Train: epoch 0097 / 0100 | batch 0006 / 0050 | loss 0.0243\n",
            "Train: epoch 0097 / 0100 | batch 0007 / 0050 | loss 0.0242\n",
            "Train: epoch 0097 / 0100 | batch 0008 / 0050 | loss 0.0234\n",
            "Train: epoch 0097 / 0100 | batch 0009 / 0050 | loss 0.0230\n",
            "Train: epoch 0097 / 0100 | batch 0010 / 0050 | loss 0.0232\n",
            "Train: epoch 0097 / 0100 | batch 0011 / 0050 | loss 0.0230\n",
            "Train: epoch 0097 / 0100 | batch 0012 / 0050 | loss 0.0231\n",
            "Train: epoch 0097 / 0100 | batch 0013 / 0050 | loss 0.0228\n",
            "Train: epoch 0097 / 0100 | batch 0014 / 0050 | loss 0.0222\n",
            "Train: epoch 0097 / 0100 | batch 0015 / 0050 | loss 0.0223\n",
            "Train: epoch 0097 / 0100 | batch 0016 / 0050 | loss 0.0225\n",
            "Train: epoch 0097 / 0100 | batch 0017 / 0050 | loss 0.0226\n",
            "Train: epoch 0097 / 0100 | batch 0018 / 0050 | loss 0.0222\n",
            "Train: epoch 0097 / 0100 | batch 0019 / 0050 | loss 0.0217\n",
            "Train: epoch 0097 / 0100 | batch 0020 / 0050 | loss 0.0216\n",
            "Train: epoch 0097 / 0100 | batch 0021 / 0050 | loss 0.0218\n",
            "Train: epoch 0097 / 0100 | batch 0022 / 0050 | loss 0.0225\n",
            "Train: epoch 0097 / 0100 | batch 0023 / 0050 | loss 0.0225\n",
            "Train: epoch 0097 / 0100 | batch 0024 / 0050 | loss 0.0225\n",
            "Train: epoch 0097 / 0100 | batch 0025 / 0050 | loss 0.0222\n",
            "Train: epoch 0097 / 0100 | batch 0026 / 0050 | loss 0.0225\n",
            "Train: epoch 0097 / 0100 | batch 0027 / 0050 | loss 0.0222\n",
            "Train: epoch 0097 / 0100 | batch 0028 / 0050 | loss 0.0220\n",
            "Train: epoch 0097 / 0100 | batch 0029 / 0050 | loss 0.0220\n",
            "Train: epoch 0097 / 0100 | batch 0030 / 0050 | loss 0.0218\n",
            "Train: epoch 0097 / 0100 | batch 0031 / 0050 | loss 0.0217\n",
            "Train: epoch 0097 / 0100 | batch 0032 / 0050 | loss 0.0217\n",
            "Train: epoch 0097 / 0100 | batch 0033 / 0050 | loss 0.0217\n",
            "Train: epoch 0097 / 0100 | batch 0034 / 0050 | loss 0.0220\n",
            "Train: epoch 0097 / 0100 | batch 0035 / 0050 | loss 0.0220\n",
            "Train: epoch 0097 / 0100 | batch 0036 / 0050 | loss 0.0218\n",
            "Train: epoch 0097 / 0100 | batch 0037 / 0050 | loss 0.0217\n",
            "Train: epoch 0097 / 0100 | batch 0038 / 0050 | loss 0.0216\n",
            "Train: epoch 0097 / 0100 | batch 0039 / 0050 | loss 0.0215\n",
            "Train: epoch 0097 / 0100 | batch 0040 / 0050 | loss 0.0217\n",
            "Train: epoch 0097 / 0100 | batch 0041 / 0050 | loss 0.0216\n",
            "Train: epoch 0097 / 0100 | batch 0042 / 0050 | loss 0.0216\n",
            "Train: epoch 0097 / 0100 | batch 0043 / 0050 | loss 0.0215\n",
            "Train: epoch 0097 / 0100 | batch 0044 / 0050 | loss 0.0214\n",
            "Train: epoch 0097 / 0100 | batch 0045 / 0050 | loss 0.0215\n",
            "Train: epoch 0097 / 0100 | batch 0046 / 0050 | loss 0.0214\n",
            "Train: epoch 0097 / 0100 | batch 0047 / 0050 | loss 0.0212\n",
            "Train: epoch 0097 / 0100 | batch 0048 / 0050 | loss 0.0212\n",
            "Train: epoch 0097 / 0100 | batch 0049 / 0050 | loss 0.0210\n",
            "Val loss 0.0263\n",
            "Dice score : 0.15447798371315002\n",
            "Val loss 0.0268\n",
            "Dice score : 0.1277523636817932\n",
            "Val loss 0.0272\n",
            "Dice score : 0.08485613763332367\n",
            "Val loss 0.0265\n",
            "Dice score : 0.08169318735599518\n",
            "Val loss 0.0356\n",
            "Dice score : 0.07814498990774155\n",
            "Val loss 0.0461\n",
            "Dice score : 0.13850100338459015\n",
            "Val loss 0.0433\n",
            "Dice score : 0.06931716203689575\n",
            "Val loss 0.0407\n",
            "Dice score : 0.14781081676483154\n",
            "Val loss 0.0406\n",
            "Dice score : 0.15408360958099365\n",
            "Val loss 0.0423\n",
            "Dice score : 0.15446096658706665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 97/100 [1:57:58<03:40, 73.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0098 / 0100 | batch 0000 / 0050 | loss 0.0154\n",
            "Train: epoch 0098 / 0100 | batch 0001 / 0050 | loss 0.0150\n",
            "Train: epoch 0098 / 0100 | batch 0002 / 0050 | loss 0.0153\n",
            "Train: epoch 0098 / 0100 | batch 0003 / 0050 | loss 0.0156\n",
            "Train: epoch 0098 / 0100 | batch 0004 / 0050 | loss 0.0188\n",
            "Train: epoch 0098 / 0100 | batch 0005 / 0050 | loss 0.0187\n",
            "Train: epoch 0098 / 0100 | batch 0006 / 0050 | loss 0.0187\n",
            "Train: epoch 0098 / 0100 | batch 0007 / 0050 | loss 0.0195\n",
            "Train: epoch 0098 / 0100 | batch 0008 / 0050 | loss 0.0198\n",
            "Train: epoch 0098 / 0100 | batch 0009 / 0050 | loss 0.0194\n",
            "Train: epoch 0098 / 0100 | batch 0010 / 0050 | loss 0.0203\n",
            "Train: epoch 0098 / 0100 | batch 0011 / 0050 | loss 0.0208\n",
            "Train: epoch 0098 / 0100 | batch 0012 / 0050 | loss 0.0204\n",
            "Train: epoch 0098 / 0100 | batch 0013 / 0050 | loss 0.0205\n",
            "Train: epoch 0098 / 0100 | batch 0014 / 0050 | loss 0.0210\n",
            "Train: epoch 0098 / 0100 | batch 0015 / 0050 | loss 0.0208\n",
            "Train: epoch 0098 / 0100 | batch 0016 / 0050 | loss 0.0205\n",
            "Train: epoch 0098 / 0100 | batch 0017 / 0050 | loss 0.0212\n",
            "Train: epoch 0098 / 0100 | batch 0018 / 0050 | loss 0.0217\n",
            "Train: epoch 0098 / 0100 | batch 0019 / 0050 | loss 0.0215\n",
            "Train: epoch 0098 / 0100 | batch 0020 / 0050 | loss 0.0220\n",
            "Train: epoch 0098 / 0100 | batch 0021 / 0050 | loss 0.0219\n",
            "Train: epoch 0098 / 0100 | batch 0022 / 0050 | loss 0.0216\n",
            "Train: epoch 0098 / 0100 | batch 0023 / 0050 | loss 0.0215\n",
            "Train: epoch 0098 / 0100 | batch 0024 / 0050 | loss 0.0214\n",
            "Train: epoch 0098 / 0100 | batch 0025 / 0050 | loss 0.0213\n",
            "Train: epoch 0098 / 0100 | batch 0026 / 0050 | loss 0.0212\n",
            "Train: epoch 0098 / 0100 | batch 0027 / 0050 | loss 0.0213\n",
            "Train: epoch 0098 / 0100 | batch 0028 / 0050 | loss 0.0213\n",
            "Train: epoch 0098 / 0100 | batch 0029 / 0050 | loss 0.0214\n",
            "Train: epoch 0098 / 0100 | batch 0030 / 0050 | loss 0.0212\n",
            "Train: epoch 0098 / 0100 | batch 0031 / 0050 | loss 0.0209\n",
            "Train: epoch 0098 / 0100 | batch 0032 / 0050 | loss 0.0209\n",
            "Train: epoch 0098 / 0100 | batch 0033 / 0050 | loss 0.0208\n",
            "Train: epoch 0098 / 0100 | batch 0034 / 0050 | loss 0.0206\n",
            "Train: epoch 0098 / 0100 | batch 0035 / 0050 | loss 0.0207\n",
            "Train: epoch 0098 / 0100 | batch 0036 / 0050 | loss 0.0210\n",
            "Train: epoch 0098 / 0100 | batch 0037 / 0050 | loss 0.0209\n",
            "Train: epoch 0098 / 0100 | batch 0038 / 0050 | loss 0.0208\n",
            "Train: epoch 0098 / 0100 | batch 0039 / 0050 | loss 0.0206\n",
            "Train: epoch 0098 / 0100 | batch 0040 / 0050 | loss 0.0207\n",
            "Train: epoch 0098 / 0100 | batch 0041 / 0050 | loss 0.0207\n",
            "Train: epoch 0098 / 0100 | batch 0042 / 0050 | loss 0.0206\n",
            "Train: epoch 0098 / 0100 | batch 0043 / 0050 | loss 0.0205\n",
            "Train: epoch 0098 / 0100 | batch 0044 / 0050 | loss 0.0208\n",
            "Train: epoch 0098 / 0100 | batch 0045 / 0050 | loss 0.0210\n",
            "Train: epoch 0098 / 0100 | batch 0046 / 0050 | loss 0.0209\n",
            "Train: epoch 0098 / 0100 | batch 0047 / 0050 | loss 0.0207\n",
            "Train: epoch 0098 / 0100 | batch 0048 / 0050 | loss 0.0208\n",
            "Train: epoch 0098 / 0100 | batch 0049 / 0050 | loss 0.0209\n",
            "Val loss 0.0320\n",
            "Dice score : 0.11455287039279938\n",
            "Val loss 0.0311\n",
            "Dice score : 0.08607137203216553\n",
            "Val loss 0.0372\n",
            "Dice score : 0.10956715047359467\n",
            "Val loss 0.0466\n",
            "Dice score : 0.06144896522164345\n",
            "Val loss 0.0521\n",
            "Dice score : 0.12683072686195374\n",
            "Val loss 0.0476\n",
            "Dice score : 0.09794840961694717\n",
            "Val loss 0.0444\n",
            "Dice score : 0.0998246893286705\n",
            "Val loss 0.0427\n",
            "Dice score : 0.09086589515209198\n",
            "Val loss 0.0405\n",
            "Dice score : 0.07452251017093658\n",
            "Val loss 0.0399\n",
            "Dice score : 0.1475297510623932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 98/100 [1:59:13<02:28, 74.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0099 / 0100 | batch 0000 / 0050 | loss 0.0335\n",
            "Train: epoch 0099 / 0100 | batch 0001 / 0050 | loss 0.0278\n",
            "Train: epoch 0099 / 0100 | batch 0002 / 0050 | loss 0.0305\n",
            "Train: epoch 0099 / 0100 | batch 0003 / 0050 | loss 0.0271\n",
            "Train: epoch 0099 / 0100 | batch 0004 / 0050 | loss 0.0247\n",
            "Train: epoch 0099 / 0100 | batch 0005 / 0050 | loss 0.0229\n",
            "Train: epoch 0099 / 0100 | batch 0006 / 0050 | loss 0.0224\n",
            "Train: epoch 0099 / 0100 | batch 0007 / 0050 | loss 0.0226\n",
            "Train: epoch 0099 / 0100 | batch 0008 / 0050 | loss 0.0218\n",
            "Train: epoch 0099 / 0100 | batch 0009 / 0050 | loss 0.0228\n",
            "Train: epoch 0099 / 0100 | batch 0010 / 0050 | loss 0.0221\n",
            "Train: epoch 0099 / 0100 | batch 0011 / 0050 | loss 0.0218\n",
            "Train: epoch 0099 / 0100 | batch 0012 / 0050 | loss 0.0221\n",
            "Train: epoch 0099 / 0100 | batch 0013 / 0050 | loss 0.0223\n",
            "Train: epoch 0099 / 0100 | batch 0014 / 0050 | loss 0.0219\n",
            "Train: epoch 0099 / 0100 | batch 0015 / 0050 | loss 0.0216\n",
            "Train: epoch 0099 / 0100 | batch 0016 / 0050 | loss 0.0211\n",
            "Train: epoch 0099 / 0100 | batch 0017 / 0050 | loss 0.0213\n",
            "Train: epoch 0099 / 0100 | batch 0018 / 0050 | loss 0.0211\n",
            "Train: epoch 0099 / 0100 | batch 0019 / 0050 | loss 0.0209\n",
            "Train: epoch 0099 / 0100 | batch 0020 / 0050 | loss 0.0208\n",
            "Train: epoch 0099 / 0100 | batch 0021 / 0050 | loss 0.0207\n",
            "Train: epoch 0099 / 0100 | batch 0022 / 0050 | loss 0.0204\n",
            "Train: epoch 0099 / 0100 | batch 0023 / 0050 | loss 0.0202\n",
            "Train: epoch 0099 / 0100 | batch 0024 / 0050 | loss 0.0202\n",
            "Train: epoch 0099 / 0100 | batch 0025 / 0050 | loss 0.0199\n",
            "Train: epoch 0099 / 0100 | batch 0026 / 0050 | loss 0.0198\n",
            "Train: epoch 0099 / 0100 | batch 0027 / 0050 | loss 0.0200\n",
            "Train: epoch 0099 / 0100 | batch 0028 / 0050 | loss 0.0201\n",
            "Train: epoch 0099 / 0100 | batch 0029 / 0050 | loss 0.0202\n",
            "Train: epoch 0099 / 0100 | batch 0030 / 0050 | loss 0.0200\n",
            "Train: epoch 0099 / 0100 | batch 0031 / 0050 | loss 0.0199\n",
            "Train: epoch 0099 / 0100 | batch 0032 / 0050 | loss 0.0199\n",
            "Train: epoch 0099 / 0100 | batch 0033 / 0050 | loss 0.0200\n",
            "Train: epoch 0099 / 0100 | batch 0034 / 0050 | loss 0.0201\n",
            "Train: epoch 0099 / 0100 | batch 0035 / 0050 | loss 0.0200\n",
            "Train: epoch 0099 / 0100 | batch 0036 / 0050 | loss 0.0199\n",
            "Train: epoch 0099 / 0100 | batch 0037 / 0050 | loss 0.0198\n",
            "Train: epoch 0099 / 0100 | batch 0038 / 0050 | loss 0.0196\n",
            "Train: epoch 0099 / 0100 | batch 0039 / 0050 | loss 0.0196\n",
            "Train: epoch 0099 / 0100 | batch 0040 / 0050 | loss 0.0196\n",
            "Train: epoch 0099 / 0100 | batch 0041 / 0050 | loss 0.0197\n",
            "Train: epoch 0099 / 0100 | batch 0042 / 0050 | loss 0.0199\n",
            "Train: epoch 0099 / 0100 | batch 0043 / 0050 | loss 0.0203\n",
            "Train: epoch 0099 / 0100 | batch 0044 / 0050 | loss 0.0203\n",
            "Train: epoch 0099 / 0100 | batch 0045 / 0050 | loss 0.0204\n",
            "Train: epoch 0099 / 0100 | batch 0046 / 0050 | loss 0.0204\n",
            "Train: epoch 0099 / 0100 | batch 0047 / 0050 | loss 0.0203\n",
            "Train: epoch 0099 / 0100 | batch 0048 / 0050 | loss 0.0203\n",
            "Train: epoch 0099 / 0100 | batch 0049 / 0050 | loss 0.0204\n",
            "Val loss 0.0499\n",
            "Dice score : 0.07296497374773026\n",
            "Val loss 0.0347\n",
            "Dice score : 0.06322336196899414\n",
            "Val loss 0.0458\n",
            "Dice score : 0.0836661085486412\n",
            "Val loss 0.0428\n",
            "Dice score : 0.11370044201612473\n",
            "Val loss 0.0391\n",
            "Dice score : 0.06210872158408165\n",
            "Val loss 0.0433\n",
            "Dice score : 0.11235813051462173\n",
            "Val loss 0.0432\n",
            "Dice score : 0.07469318807125092\n",
            "Val loss 0.0418\n",
            "Dice score : 0.08110389113426208\n",
            "Val loss 0.0405\n",
            "Dice score : 0.08538778871297836\n",
            "Val loss 0.0407\n",
            "Dice score : 0.037894878536462784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 99/100 [2:00:24<01:13, 73.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch 0100 / 0100 | batch 0000 / 0050 | loss 0.0276\n",
            "Train: epoch 0100 / 0100 | batch 0001 / 0050 | loss 0.0286\n",
            "Train: epoch 0100 / 0100 | batch 0002 / 0050 | loss 0.0303\n",
            "Train: epoch 0100 / 0100 | batch 0003 / 0050 | loss 0.0287\n",
            "Train: epoch 0100 / 0100 | batch 0004 / 0050 | loss 0.0260\n",
            "Train: epoch 0100 / 0100 | batch 0005 / 0050 | loss 0.0256\n",
            "Train: epoch 0100 / 0100 | batch 0006 / 0050 | loss 0.0244\n",
            "Train: epoch 0100 / 0100 | batch 0007 / 0050 | loss 0.0235\n",
            "Train: epoch 0100 / 0100 | batch 0008 / 0050 | loss 0.0229\n",
            "Train: epoch 0100 / 0100 | batch 0009 / 0050 | loss 0.0230\n",
            "Train: epoch 0100 / 0100 | batch 0010 / 0050 | loss 0.0231\n",
            "Train: epoch 0100 / 0100 | batch 0011 / 0050 | loss 0.0239\n",
            "Train: epoch 0100 / 0100 | batch 0012 / 0050 | loss 0.0233\n",
            "Train: epoch 0100 / 0100 | batch 0013 / 0050 | loss 0.0237\n",
            "Train: epoch 0100 / 0100 | batch 0014 / 0050 | loss 0.0231\n",
            "Train: epoch 0100 / 0100 | batch 0015 / 0050 | loss 0.0229\n",
            "Train: epoch 0100 / 0100 | batch 0016 / 0050 | loss 0.0228\n",
            "Train: epoch 0100 / 0100 | batch 0017 / 0050 | loss 0.0223\n",
            "Train: epoch 0100 / 0100 | batch 0018 / 0050 | loss 0.0219\n",
            "Train: epoch 0100 / 0100 | batch 0019 / 0050 | loss 0.0219\n",
            "Train: epoch 0100 / 0100 | batch 0020 / 0050 | loss 0.0220\n",
            "Train: epoch 0100 / 0100 | batch 0021 / 0050 | loss 0.0219\n",
            "Train: epoch 0100 / 0100 | batch 0022 / 0050 | loss 0.0216\n",
            "Train: epoch 0100 / 0100 | batch 0023 / 0050 | loss 0.0213\n",
            "Train: epoch 0100 / 0100 | batch 0024 / 0050 | loss 0.0211\n",
            "Train: epoch 0100 / 0100 | batch 0025 / 0050 | loss 0.0209\n",
            "Train: epoch 0100 / 0100 | batch 0026 / 0050 | loss 0.0210\n",
            "Train: epoch 0100 / 0100 | batch 0027 / 0050 | loss 0.0208\n",
            "Train: epoch 0100 / 0100 | batch 0028 / 0050 | loss 0.0208\n",
            "Train: epoch 0100 / 0100 | batch 0029 / 0050 | loss 0.0207\n",
            "Train: epoch 0100 / 0100 | batch 0030 / 0050 | loss 0.0206\n",
            "Train: epoch 0100 / 0100 | batch 0031 / 0050 | loss 0.0205\n",
            "Train: epoch 0100 / 0100 | batch 0032 / 0050 | loss 0.0207\n",
            "Train: epoch 0100 / 0100 | batch 0033 / 0050 | loss 0.0208\n",
            "Train: epoch 0100 / 0100 | batch 0034 / 0050 | loss 0.0205\n",
            "Train: epoch 0100 / 0100 | batch 0035 / 0050 | loss 0.0207\n",
            "Train: epoch 0100 / 0100 | batch 0036 / 0050 | loss 0.0209\n",
            "Train: epoch 0100 / 0100 | batch 0037 / 0050 | loss 0.0208\n",
            "Train: epoch 0100 / 0100 | batch 0038 / 0050 | loss 0.0209\n",
            "Train: epoch 0100 / 0100 | batch 0039 / 0050 | loss 0.0209\n",
            "Train: epoch 0100 / 0100 | batch 0040 / 0050 | loss 0.0208\n",
            "Train: epoch 0100 / 0100 | batch 0041 / 0050 | loss 0.0207\n",
            "Train: epoch 0100 / 0100 | batch 0042 / 0050 | loss 0.0205\n",
            "Train: epoch 0100 / 0100 | batch 0043 / 0050 | loss 0.0204\n",
            "Train: epoch 0100 / 0100 | batch 0044 / 0050 | loss 0.0203\n",
            "Train: epoch 0100 / 0100 | batch 0045 / 0050 | loss 0.0202\n",
            "Train: epoch 0100 / 0100 | batch 0046 / 0050 | loss 0.0201\n",
            "Train: epoch 0100 / 0100 | batch 0047 / 0050 | loss 0.0201\n",
            "Train: epoch 0100 / 0100 | batch 0048 / 0050 | loss 0.0200\n",
            "Train: epoch 0100 / 0100 | batch 0049 / 0050 | loss 0.0200\n",
            "Val loss 0.0271\n",
            "Dice score : 0.0733482614159584\n",
            "Val loss 0.0537\n",
            "Dice score : 0.10625586658716202\n",
            "Val loss 0.0581\n",
            "Dice score : 0.1619064211845398\n",
            "Val loss 0.0512\n",
            "Dice score : 0.1460069864988327\n",
            "Val loss 0.0473\n",
            "Dice score : 0.12224197387695312\n",
            "Val loss 0.0442\n",
            "Dice score : 0.07629700750112534\n",
            "Val loss 0.0442\n",
            "Dice score : 0.11355209350585938\n",
            "Val loss 0.0420\n",
            "Dice score : 0.10264353454113007\n",
            "Val loss 0.0403\n",
            "Dice score : 0.14147961139678955\n",
            "Val loss 0.0432\n",
            "Dice score : 0.08696700632572174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [2:01:36<00:00, 72.96s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = Dataset(data_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/test' , transform = transform_testset)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = 8)\n",
        "\n",
        "trainer_Spine_segment.test(test_loader)"
      ],
      "metadata": {
        "id": "XHIR53bc3-Q3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7137ab99-a42f-4f7c-eb0f-c9746791649c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['121.dcm', '122.dcm', '123.dcm', '124.dcm', '125.dcm', '126.dcm', '127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3240, 2144)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3240, 2144, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['122.dcm', '123.dcm', '124.dcm', '125.dcm', '126.dcm', '127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3188, 1868)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3188, 1868, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['123.dcm', '124.dcm', '125.dcm', '126.dcm', '127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3240, 2112)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3240, 2112, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['124.dcm', '125.dcm', '126.dcm', '127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3232, 1964)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3232, 1964, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['125.dcm', '126.dcm', '127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3240, 2280)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3240, 2280, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['126.dcm', '127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3240, 1896)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3240, 1896, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1796)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1796, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1808)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1808, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1940)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1940, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1628)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1628, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1880)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1880, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1676)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1676, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 2064)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 2064, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1692)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1692, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 2056)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 2056, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1620)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1620, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1972)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1972, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1664)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1664, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 2756)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 2756, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 2756)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 2756, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 2756)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 2756, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 2756)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 2756, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 1628)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 1628, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2568, 1068)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2568, 1068, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3152, 1524)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3152, 1524, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2932, 1616)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2932, 1616, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 2756)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 2756, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2776, 1176)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2776, 1176, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3260, 1892)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3260, 1892, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 1496)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 1496, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 2036)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 2036, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1980)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1980, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1996)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1996, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1704)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1704, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 2168)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 2168, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 2004)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 2004, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 2208)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 2208, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 2484)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 2484, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1692)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1692, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 2092)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 2092, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1856)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1856, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1860)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1860, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 2196)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 2196, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1828)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1828, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1704)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1704, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1964)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1964, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1636)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1636, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3064, 1364)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3064, 1364, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1924)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1924, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3232, 2640)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3232, 2640, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2352, 1984)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2352, 1984, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2748, 2220)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2748, 2220, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2748, 2200)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2748, 2200, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2692, 1532)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2692, 1532, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2572, 1448)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2572, 1448, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2680, 1424)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2680, 1424, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2712, 1548)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2712, 1548, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['198.dcm', '199.dcm']\n",
            "(1, 2704, 1628)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2704, 1628, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['199.dcm']\n",
            "(1, 3232, 2440)\n",
            "torch.Size([1, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3232, 2440, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_net = UNET()\n",
        "train_net.cuda()\n",
        "\n",
        "optim = torch.optim.Adam(train_net.parameters(), lr = 0.0001)\n",
        "train_net, optim, start_epoch = load(ckpt_dir = ckpt_dir, net = train_net, optim = optim) # 저장된 네트워크 불러오기\n",
        "\n",
        "train_trainer_Spine_segment = trainer(train_net, train_loader,\"Adam\", epoch_size=30, learning_rate=0.0001)\n",
        "\n",
        "train_trainer_Spine_segment.train(val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "tkqyl-khi2OI",
        "outputId": "d6f6cab7-2349-4a19-8bf1-b9a413d2a662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/30 [00:00<?, ?it/s]Exception ignored in: <function _releaseLock at 0x7f248bbac8c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 221, in _releaseLock\n",
            "    def _releaseLock():\n",
            "KeyboardInterrupt\n",
            "  0%|          | 0/30 [00:12<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEmpty\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5935cc34df3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_trainer_Spine_segment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_trainer_Spine_segment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-d5b143b6aaeb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, validation_loader)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mloss_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;31m#forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 53216, 53220, 53224) exited unexpectedly"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = Dataset(data_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/test' , transform = transform_testset)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = 8)\n",
        "\n",
        "train_trainer_Spine_segment.test(test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VxLcxWhtU7o",
        "outputId": "e6be96cb-4149-4aff-d7d0-17894ca5167a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['151.dcm', '152.dcm', '153.dcm', '154.dcm', '155.dcm', '156.dcm', '157.dcm', '158.dcm', '159.dcm', '160.dcm', '181.dcm', '182.dcm', '183.dcm', '184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 3052, 2140)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(3052, 2140, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['152.dcm', '153.dcm', '154.dcm', '155.dcm', '156.dcm', '157.dcm', '158.dcm', '159.dcm', '160.dcm', '181.dcm', '182.dcm', '183.dcm', '184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 3264, 1852)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(3264, 1852, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['153.dcm', '154.dcm', '155.dcm', '156.dcm', '157.dcm', '158.dcm', '159.dcm', '160.dcm', '181.dcm', '182.dcm', '183.dcm', '184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 3228, 1616)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(3228, 1616, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['154.dcm', '155.dcm', '156.dcm', '157.dcm', '158.dcm', '159.dcm', '160.dcm', '181.dcm', '182.dcm', '183.dcm', '184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 3228, 1816)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(3228, 1816, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['155.dcm', '156.dcm', '157.dcm', '158.dcm', '159.dcm', '160.dcm', '181.dcm', '182.dcm', '183.dcm', '184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 3228, 1800)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(3228, 1800, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['156.dcm', '157.dcm', '158.dcm', '159.dcm', '160.dcm', '181.dcm', '182.dcm', '183.dcm', '184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 3264, 1672)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(3264, 1672, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['157.dcm', '158.dcm', '159.dcm', '160.dcm', '181.dcm', '182.dcm', '183.dcm', '184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 3264, 1964)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(3264, 1964, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['158.dcm', '159.dcm', '160.dcm', '181.dcm', '182.dcm', '183.dcm', '184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 3036, 3076)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(3036, 3076, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['159.dcm', '160.dcm', '181.dcm', '182.dcm', '183.dcm', '184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 3112, 2108)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(3112, 2108, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['160.dcm', '181.dcm', '182.dcm', '183.dcm', '184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 3264, 2140)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(3264, 2140, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['181.dcm', '182.dcm', '183.dcm', '184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 2680, 1460)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(2680, 1460, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['182.dcm', '183.dcm', '184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 2716, 1628)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(2716, 1628, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['183.dcm', '184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 2712, 1992)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(2712, 1992, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['184.dcm', '185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 2736, 1324)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(2736, 1324, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['185.dcm', '186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 2832, 2520)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(2832, 2520, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['186.dcm', '187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 2852, 1800)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(2852, 1800, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['187.dcm', '188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 2496, 1172)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(2496, 1172, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['188.dcm', '189.dcm', '190.dcm']\n",
            "(1, 2748, 1340)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(2748, 1340, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['189.dcm', '190.dcm']\n",
            "(1, 2560, 1280)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(2560, 1280, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['190.dcm']\n",
            "(1, 2760, 1972)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "(2760, 1972, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "eFrTXt-0qsTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = Dataset(data_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/test' , transform = transform_testset)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = 8)"
      ],
      "metadata": {
        "id": "f81Uv1nEqu1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_Spine_segment.test(test_loader)"
      ],
      "metadata": {
        "id": "EzoVOllkql8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# checkpoint load to model"
      ],
      "metadata": {
        "id": "VNXG42c7jlt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = Dataset(data_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/test')\n",
        "\n",
        "# print(example.lst_input)\n",
        "# for name in example.lst_input:\n",
        "#     print(name[:-3])\n",
        "#     break\n",
        "# print(example.lst_input.pop(0)[:-3])\n",
        "# for i in range(20):\n",
        "#   data = example.__getitem__(i)\n",
        "#   input = data['input']\n",
        "#   print(input.shape)\n",
        "\n",
        "a =np.load('/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/leaderboard/155.npy')\n",
        "print(a.shape[0]*a.shape[1])\n",
        "print(a[:,:,6].sum())\n",
        "print(type(a[0,0,0]))\n",
        "plt.subplot(111)\n",
        "plt.imshow(a[:,:,6], cmap='gray')\n",
        "plt.title('output')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "AxneApNA3mFw",
        "outputId": "d6cd36f6-80d9-4a92-8dcd-dd5ba2c593f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5810400\n",
            "782353\n",
            "<class 'numpy.uint8'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'output')"
            ]
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 이 셀 실행"
      ],
      "metadata": {
        "id": "0IMr_Q5cYsa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_net = UNET()\n",
        "test_net.cuda()\n",
        "\n",
        "optim = torch.optim.Adagrad(test_net.parameters(), lr = 0.01)\n",
        "test_net, optim, start_epoch = load(ckpt_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/record2', net = test_net, optim = optim) # 저장된 네트워크 불러오기\n",
        "\n",
        "test_trainer_Spine_segment = trainer(test_net, train_loader,\"Adagrad\", epoch_size=20, learning_rate=0.0001)\n",
        "\n",
        "test_trainer_Spine_segment.test(test_loader)"
      ],
      "metadata": {
        "id": "5dnQvvTPcaaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78aafd18-87e7-4210-a500-21fce35d20e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['121.dcm', '122.dcm', '123.dcm', '124.dcm', '125.dcm', '126.dcm', '127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3240, 2144)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3240, 2144, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['122.dcm', '123.dcm', '124.dcm', '125.dcm', '126.dcm', '127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3188, 1868)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3188, 1868, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['123.dcm', '124.dcm', '125.dcm', '126.dcm', '127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3240, 2112)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3240, 2112, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['124.dcm', '125.dcm', '126.dcm', '127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3232, 1964)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3232, 1964, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['125.dcm', '126.dcm', '127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3240, 2280)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3240, 2280, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['126.dcm', '127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3240, 1896)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3240, 1896, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['127.dcm', '128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1796)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1796, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['128.dcm', '129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1808)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1808, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['129.dcm', '130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1940)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1940, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['130.dcm', '131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1628)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1628, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['131.dcm', '132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1880)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1880, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['132.dcm', '133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1676)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1676, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['133.dcm', '134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 2064)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 2064, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['134.dcm', '135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1692)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1692, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['135.dcm', '136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 2056)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 2056, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['136.dcm', '137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1620)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1620, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['137.dcm', '138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1972)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1972, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['138.dcm', '139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1664)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1664, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['139.dcm', '140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 2756)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 2756, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['140.dcm', '141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 2756)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 2756, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['141.dcm', '142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 2756)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 2756, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['142.dcm', '143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 2756)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 2756, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['143.dcm', '144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 1628)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 1628, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['144.dcm', '145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2568, 1068)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2568, 1068, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['145.dcm', '146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3152, 1524)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3152, 1524, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['146.dcm', '147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2932, 1616)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2932, 1616, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['147.dcm', '148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 2756)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 2756, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['148.dcm', '149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2776, 1176)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2776, 1176, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['149.dcm', '150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3260, 1892)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3260, 1892, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['150.dcm', '161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3268, 1496)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3268, 1496, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['161.dcm', '162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 2036)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 2036, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['162.dcm', '163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1980)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1980, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['163.dcm', '164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1996)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1996, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['164.dcm', '165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1704)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1704, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['165.dcm', '166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 2168)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 2168, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['166.dcm', '167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 2004)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 2004, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['167.dcm', '168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 2208)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 2208, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['168.dcm', '169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 2484)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 2484, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['169.dcm', '170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1692)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1692, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['170.dcm', '171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 2092)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 2092, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['171.dcm', '172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1856)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1856, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['172.dcm', '173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1860)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1860, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['173.dcm', '174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 2196)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 2196, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['174.dcm', '175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1828)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1828, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['175.dcm', '176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1704)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1704, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['176.dcm', '177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1964)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1964, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['177.dcm', '178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3228, 1636)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3228, 1636, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['178.dcm', '179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3064, 1364)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3064, 1364, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['179.dcm', '180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3264, 1924)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3264, 1924, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['180.dcm', '191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 3232, 2640)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3232, 2640, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['191.dcm', '192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2352, 1984)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2352, 1984, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['192.dcm', '193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2748, 2220)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2748, 2220, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['193.dcm', '194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2748, 2200)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2748, 2200, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['194.dcm', '195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2692, 1532)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2692, 1532, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['195.dcm', '196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2572, 1448)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2572, 1448, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['196.dcm', '197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2680, 1424)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2680, 1424, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['197.dcm', '198.dcm', '199.dcm']\n",
            "(1, 2712, 1548)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2712, 1548, 7)\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['198.dcm', '199.dcm']\n",
            "(1, 2704, 1628)\n",
            "torch.Size([2, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(2704, 1628, 7)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "['199.dcm']\n",
            "(1, 3232, 2440)\n",
            "torch.Size([1, 6, 1024, 1024])\n",
            "torch.Size([6, 1024, 1024])\n",
            "1.0\n",
            "(3232, 2440, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After Train & Test : (Input & label & output) image file save and visualization"
      ],
      "metadata": {
        "id": "HbWS_gYCw4KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = np.load('/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/result/numpy/output_0011.npy')\n",
        "\n",
        "# aaaaa = output[4]\n",
        "# print(aaaaa)\n",
        "# classifier = lambda x :  1.0 * (x > 0.5)\n",
        "# print(np.amin(aaaaa))\n",
        "# print(np.amax(aaaaa))\n",
        "\n",
        "# dn = lambda x, min : x - min\n",
        "# positive = dn(aaaaa, min=np.amin(aaaaa))\n",
        "# dnn =dn(output[3], max=np.amax(output[3]), min=np.amin(output[3]))\n",
        "\n",
        "\n",
        "# preds = (output-np.mean(aaaaa))/np.std(aaaaa)\n",
        "# preds = torch.sigmoid(preds)\n",
        "# preds = classifier(preds)\n",
        "\n",
        "\n",
        "print(output.shape)\n",
        "\n",
        "print(output)\n",
        "\n",
        "\n",
        "plt.subplot(161)\n",
        "plt.imshow(output[0], cmap='gray')\n",
        "plt.title('output')\n",
        "\n",
        "plt.subplot(162)\n",
        "plt.imshow(output[1], cmap='gray')\n",
        "plt.title('output')\n",
        "\n",
        "plt.subplot(163)\n",
        "plt.imshow(output[2], cmap='gray')\n",
        "plt.title('output')\n",
        "\n",
        "plt.subplot(164)\n",
        "plt.imshow(output[3], cmap='gray')\n",
        "plt.title('output')\n",
        "\n",
        "plt.subplot(165)\n",
        "plt.imshow(output[4], cmap='gray')\n",
        "plt.title('output')\n",
        "\n",
        "plt.subplot(166)\n",
        "plt.imshow(output[5], cmap='gray')\n",
        "plt.title('output')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BlfF46XpF0hp",
        "outputId": "5a118242-4711-4efc-e1af-a46bd5f157c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 1024, 1024)\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 0. ... 0. 1. 1.]\n",
            "  [1. 1. 0. ... 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 1. 0. ... 1. 1. 1.]\n",
            "  [1. 1. 0. ... 1. 1. 1.]\n",
            "  [0. 1. 1. ... 1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 0. 1. 1.]\n",
            "  ...\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 0. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]\n",
            "  [1. 1. 0. ... 0. 0. 1.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 1. 1. 1.]\n",
            "  [1. 0. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 0.]\n",
            "  [0. 0. 0. ... 1. 1. 0.]\n",
            "  [1. 0. 0. ... 1. 1. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [1. 1. 0. ... 1. 1. 0.]\n",
            "  [1. 1. 0. ... 1. 1. 1.]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'output')"
            ]
          },
          "metadata": {},
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = np.load('/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/result/numpy/input_0010.npy')\n",
        "output = np.load('/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/result/numpy/output_0010.npy')\n",
        "\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.imshow(input, cmap='gray')\n",
        "plt.title('input')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(output[0],cmap='gray')\n",
        "plt.title('output')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "vjZSU0U7hct8",
        "outputId": "ec62d2d6-087e-4ae7-8339-a870a154756b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'output')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_dir = '/content/drive/MyDrive/21-2/basic_ai/challenge/challenge_dataset/data/result'\n",
        "\n",
        "lst_data = os.listdir(result_dir)\n",
        "\n",
        "lst_input = [f for f in lst_data if f.startswith('input')]\n",
        "lst_label = [f for f in lst_data if f.startswith('label')]\n",
        "lst_output = [f for f in lst_data if f.startswith('output')]\n",
        "\n",
        "lst_input.sort()\n",
        "lst_label.sort()\n",
        "lst_output.sort()\n",
        "\n",
        "id=0\n",
        "\n",
        "input = np.load(os.path.join(result_dir, lst_input[id]))\n",
        "label = np.load(os.path.join(result_dir, lst_label[id]))\n",
        "output = np.load(os.path.join(result_dir, lst_output[id]))\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.imshow(input, cmap='gray')\n",
        "plt.title('input')\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.imshow(label,cmap='gray')\n",
        "plt.title('label')\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.imshow(result,cmap='gray')\n",
        "plt.title('result')"
      ],
      "metadata": {
        "id": "PVGsBUWlw-lF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "a6835569-2436-492f-cb9e-df7a6b00d127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-dd5931fca8ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlst_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlst_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlst_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2cHP5couS9iU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}